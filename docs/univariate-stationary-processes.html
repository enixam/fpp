<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Univariate stationary processes | Notes for “Forecasting: Principles and Practice, 3rd edition”</title>
  <meta name="description" content="“Reproducing”Forecasting: Principles and Practice, 3rd edition"" />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Univariate stationary processes | Notes for “Forecasting: Principles and Practice, 3rd edition”" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="“Reproducing”Forecasting: Principles and Practice, 3rd edition"" />
  <meta name="github-repo" content="enixam/fpp" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Univariate stationary processes | Notes for “Forecasting: Principles and Practice, 3rd edition”" />
  
  <meta name="twitter:description" content="“Reproducing”Forecasting: Principles and Practice, 3rd edition"" />
  

<meta name="author" content="Qiushi Yan" />


<meta name="date" content="2020-09-29" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="exponential-smoothing.html"/>
<link rel="next" href="arima-models.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="css\style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Notes for fpp3</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="1" data-path="getting-started.html"><a href="getting-started.html"><i class="fa fa-check"></i><b>1</b> Getting started</a></li>
<li class="chapter" data-level="2" data-path="time-series-graphics.html"><a href="time-series-graphics.html"><i class="fa fa-check"></i><b>2</b> Time series graphics</a><ul>
<li class="chapter" data-level="2.1" data-path="time-series-graphics.html"><a href="time-series-graphics.html#tsibble-objects"><i class="fa fa-check"></i><b>2.1</b> <code>tsibble</code> objects</a><ul>
<li class="chapter" data-level="2.1.1" data-path="time-series-graphics.html"><a href="time-series-graphics.html#manipulation"><i class="fa fa-check"></i><b>2.1.1</b> manipulation</a></li>
<li class="chapter" data-level="2.1.2" data-path="time-series-graphics.html"><a href="time-series-graphics.html#importing"><i class="fa fa-check"></i><b>2.1.2</b> importing</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="time-series-graphics.html"><a href="time-series-graphics.html#time-plots"><i class="fa fa-check"></i><b>2.2</b> Time plots</a></li>
<li class="chapter" data-level="2.3" data-path="time-series-graphics.html"><a href="time-series-graphics.html#patterns-of-time-series-trend-seasonal-and-cyclic"><i class="fa fa-check"></i><b>2.3</b> Patterns of time series: trend, seasonal and cyclic</a></li>
<li class="chapter" data-level="2.4" data-path="time-series-graphics.html"><a href="time-series-graphics.html#seasonal-plots"><i class="fa fa-check"></i><b>2.4</b> Seasonal plots</a><ul>
<li class="chapter" data-level="2.4.1" data-path="time-series-graphics.html"><a href="time-series-graphics.html#multiple-seasonal-periods-period-in-gg_season"><i class="fa fa-check"></i><b>2.4.1</b> Multiple seasonal periods: <code>period</code> in <code>gg_season()</code></a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="time-series-graphics.html"><a href="time-series-graphics.html#seasonal-subseries-plots"><i class="fa fa-check"></i><b>2.5</b> Seasonal subseries plots</a><ul>
<li class="chapter" data-level="2.5.1" data-path="time-series-graphics.html"><a href="time-series-graphics.html#example-australian-holiday-tourism"><i class="fa fa-check"></i><b>2.5.1</b> Example: Australian holiday tourism</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="time-series-graphics.html"><a href="time-series-graphics.html#visualization-between-time-series"><i class="fa fa-check"></i><b>2.6</b> Visualization between time series</a><ul>
<li class="chapter" data-level="2.6.1" data-path="time-series-graphics.html"><a href="time-series-graphics.html#scatterplot-matrices"><i class="fa fa-check"></i><b>2.6.1</b> scatterplot matrices</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="time-series-graphics.html"><a href="time-series-graphics.html#lag-plots"><i class="fa fa-check"></i><b>2.7</b> Lag plots</a><ul>
<li class="chapter" data-level="2.7.1" data-path="time-series-graphics.html"><a href="time-series-graphics.html#autocorrelation"><i class="fa fa-check"></i><b>2.7.1</b> Autocorrelation</a></li>
<li class="chapter" data-level="2.7.2" data-path="time-series-graphics.html"><a href="time-series-graphics.html#trend-and-seasonality-in-acf-plots"><i class="fa fa-check"></i><b>2.7.2</b> Trend and seasonality in ACF plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="time-series-graphics.html"><a href="time-series-graphics.html#calendar-plots"><i class="fa fa-check"></i><b>2.8</b> Calendar plots</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html"><i class="fa fa-check"></i><b>3</b> Time series decomposition</a><ul>
<li class="chapter" data-level="3.1" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#transformation-and-adjustments"><i class="fa fa-check"></i><b>3.1</b> Transformation and adjustments</a><ul>
<li class="chapter" data-level="3.1.1" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#calendar-adjustments"><i class="fa fa-check"></i><b>3.1.1</b> Calendar adjustments</a></li>
<li class="chapter" data-level="3.1.2" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#population-adjustments"><i class="fa fa-check"></i><b>3.1.2</b> Population adjustments</a></li>
<li class="chapter" data-level="3.1.3" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#inflation-adjustments"><i class="fa fa-check"></i><b>3.1.3</b> Inflation adjustments</a></li>
<li class="chapter" data-level="3.1.4" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#box-cox"><i class="fa fa-check"></i><b>3.1.4</b> Mathematical transformation (Box-Cox)</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#moving-averages"><i class="fa fa-check"></i><b>3.2</b> Moving averages</a><ul>
<li class="chapter" data-level="3.2.1" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#moving-averages-of-moving-averages"><i class="fa fa-check"></i><b>3.2.1</b> Moving averages of moving averages</a></li>
<li class="chapter" data-level="3.2.2" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#estimating-the-trend-cycle-component-with-seasonal-data"><i class="fa fa-check"></i><b>3.2.2</b> Estimating the trend-cycle component with seasonal data</a></li>
<li class="chapter" data-level="3.2.3" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#weighted-moving-averages"><i class="fa fa-check"></i><b>3.2.3</b> Weighted moving averages</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#classical-decomposition"><i class="fa fa-check"></i><b>3.3</b> Classical decomposition</a></li>
<li class="chapter" data-level="3.4" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#x11-decomposition"><i class="fa fa-check"></i><b>3.4</b> X11 decomposition</a></li>
<li class="chapter" data-level="3.5" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#seats-decomposition"><i class="fa fa-check"></i><b>3.5</b> SEATS decomposition</a></li>
<li class="chapter" data-level="3.6" data-path="time-series-decomposition.html"><a href="time-series-decomposition.html#stl-decomposition"><i class="fa fa-check"></i><b>3.6</b> STL decomposition</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="time-series-features.html"><a href="time-series-features.html"><i class="fa fa-check"></i><b>4</b> Time series features</a><ul>
<li class="chapter" data-level="4.1" data-path="time-series-features.html"><a href="time-series-features.html#simple-statistics"><i class="fa fa-check"></i><b>4.1</b> Simple statistics</a></li>
<li class="chapter" data-level="4.2" data-path="time-series-features.html"><a href="time-series-features.html#acf-features"><i class="fa fa-check"></i><b>4.2</b> ACF features</a></li>
<li class="chapter" data-level="4.3" data-path="time-series-features.html"><a href="time-series-features.html#stl-features"><i class="fa fa-check"></i><b>4.3</b> STL features</a></li>
<li class="chapter" data-level="4.4" data-path="time-series-features.html"><a href="time-series-features.html#other-features"><i class="fa fa-check"></i><b>4.4</b> Other features</a></li>
<li class="chapter" data-level="4.5" data-path="time-series-features.html"><a href="time-series-features.html#exporing-australian-tourism-data"><i class="fa fa-check"></i><b>4.5</b> Exporing Australian tourism data</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html"><i class="fa fa-check"></i><b>5</b> The forecaster’s toolbox</a><ul>
<li class="chapter" data-level="5.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#a-tidy-forecasting-workflow"><i class="fa fa-check"></i><b>5.1</b> A tidy forecasting workflow</a><ul>
<li class="chapter" data-level="5.1.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#data-preparation-tidy"><i class="fa fa-check"></i><b>5.1.1</b> Data preparation (tidy)</a></li>
<li class="chapter" data-level="5.1.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#visualize"><i class="fa fa-check"></i><b>5.1.2</b> Visualize</a></li>
<li class="chapter" data-level="5.1.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#define-a-model-specify"><i class="fa fa-check"></i><b>5.1.3</b> Define a model (specify)</a></li>
<li class="chapter" data-level="5.1.4" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#train-the-model-estimate"><i class="fa fa-check"></i><b>5.1.4</b> Train the model (estimate)</a></li>
<li class="chapter" data-level="5.1.5" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#check-model-performance-evaluate"><i class="fa fa-check"></i><b>5.1.5</b> Check model performance (evaluate)</a></li>
<li class="chapter" data-level="5.1.6" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#produce-forecasts-forecast"><i class="fa fa-check"></i><b>5.1.6</b> Produce forecasts (forecast)</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#some-simple-forecasting-methods"><i class="fa fa-check"></i><b>5.2</b> Some simple forecasting methods</a><ul>
<li class="chapter" data-level="5.2.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#mean-method"><i class="fa fa-check"></i><b>5.2.1</b> Mean method</a></li>
<li class="chapter" data-level="5.2.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#naive-method"><i class="fa fa-check"></i><b>5.2.2</b> Naive method</a></li>
<li class="chapter" data-level="5.2.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#seasonal-naive-method"><i class="fa fa-check"></i><b>5.2.3</b> Seasonal naive method</a></li>
<li class="chapter" data-level="5.2.4" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#drift-method"><i class="fa fa-check"></i><b>5.2.4</b> Drift method</a></li>
<li class="chapter" data-level="5.2.5" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#australian-quarterly-beer-production"><i class="fa fa-check"></i><b>5.2.5</b> Australian quarterly beer production</a></li>
<li class="chapter" data-level="5.2.6" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#example-googles-daily-closing-stock-price"><i class="fa fa-check"></i><b>5.2.6</b> Example: Google’s daily closing stock price</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#fitted-values-and-residuals"><i class="fa fa-check"></i><b>5.3</b> Fitted values and residuals</a><ul>
<li class="chapter" data-level="5.3.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#residuals"><i class="fa fa-check"></i><b>5.3.1</b> Residuals</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#residual-diagnostics"><i class="fa fa-check"></i><b>5.4</b> Residual diagnostics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#white-noise"><i class="fa fa-check"></i><b>5.4.1</b> White noise</a></li>
<li class="chapter" data-level="5.4.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#example-forecasting-the-google-daily-closing-stock-price"><i class="fa fa-check"></i><b>5.4.2</b> Example: Forecasting the Google daily closing stock price</a></li>
<li class="chapter" data-level="5.4.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#portmanteau-tests-for-autocorrelation"><i class="fa fa-check"></i><b>5.4.3</b> Portmanteau tests for autocorrelation</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#predict-interval"><i class="fa fa-check"></i><b>5.5</b> Prediction intervals</a><ul>
<li class="chapter" data-level="5.5.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#one-step-prediction-intervals"><i class="fa fa-check"></i><b>5.5.1</b> One-step prediction intervals</a></li>
<li class="chapter" data-level="5.5.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#multi-step-prediction-intervals"><i class="fa fa-check"></i><b>5.5.2</b> Multi-step prediction intervals</a></li>
<li class="chapter" data-level="5.5.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#prediction-intervals-from-bootstrapped-residuals"><i class="fa fa-check"></i><b>5.5.3</b> Prediction intervals from bootstrapped residuals</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#evaluating-model-accuracy"><i class="fa fa-check"></i><b>5.6</b> Evaluating model accuracy</a><ul>
<li class="chapter" data-level="5.6.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#forecast-errors"><i class="fa fa-check"></i><b>5.6.1</b> Forecast errors</a></li>
<li class="chapter" data-level="5.6.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#scale-dependent-errors"><i class="fa fa-check"></i><b>5.6.2</b> Scale dependent errors</a></li>
<li class="chapter" data-level="5.6.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#percentage-errors"><i class="fa fa-check"></i><b>5.6.3</b> Percentage errors</a></li>
<li class="chapter" data-level="5.6.4" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#scaled-errors"><i class="fa fa-check"></i><b>5.6.4</b> Scaled errors</a></li>
<li class="chapter" data-level="5.6.5" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#examples-beer-production"><i class="fa fa-check"></i><b>5.6.5</b> Examples: beer production</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#time-series-cross-validation"><i class="fa fa-check"></i><b>5.7</b> Time series cross-validation</a><ul>
<li class="chapter" data-level="5.7.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#example-forecast-horizon-accuracy-with-cross-validation"><i class="fa fa-check"></i><b>5.7.1</b> Example: Forecast horizon accuracy with cross-validation</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#forecasting-using-transformations"><i class="fa fa-check"></i><b>5.8</b> Forecasting using transformations</a><ul>
<li class="chapter" data-level="5.8.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#prediction-intervals-with-transformations"><i class="fa fa-check"></i><b>5.8.1</b> Prediction intervals with transformations</a></li>
<li class="chapter" data-level="5.8.2" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#forecasting-with-constraints"><i class="fa fa-check"></i><b>5.8.2</b> Forecasting with constraints</a></li>
<li class="chapter" data-level="5.8.3" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#bias-adjustments"><i class="fa fa-check"></i><b>5.8.3</b> Bias adjustments</a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#forecasting-with-decomposition"><i class="fa fa-check"></i><b>5.9</b> Forecasting with decomposition</a><ul>
<li class="chapter" data-level="5.9.1" data-path="the-forecasters-toolbox.html"><a href="the-forecasters-toolbox.html#example-employment-in-the-us-retail-sector"><i class="fa fa-check"></i><b>5.9.1</b> Example: Employment in the US retail sector</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html"><i class="fa fa-check"></i><b>6</b> Judgmental forecasts</a><ul>
<li class="chapter" data-level="6.1" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#beware-of-limitations"><i class="fa fa-check"></i><b>6.1</b> Beware of limitations</a></li>
<li class="chapter" data-level="6.2" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#key-principles"><i class="fa fa-check"></i><b>6.2</b> Key principles</a></li>
<li class="chapter" data-level="6.3" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#the-delphi-method"><i class="fa fa-check"></i><b>6.3</b> The Delphi method</a><ul>
<li class="chapter" data-level="6.3.1" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#experts-and-anonymity"><i class="fa fa-check"></i><b>6.3.1</b> Experts and anonymity</a></li>
<li class="chapter" data-level="6.3.2" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#setting-the-forecasting-task-in-a-delphi"><i class="fa fa-check"></i><b>6.3.2</b> Setting the forecasting task in a Delphi</a></li>
<li class="chapter" data-level="6.3.3" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#iteration"><i class="fa fa-check"></i><b>6.3.3</b> Iteration</a></li>
<li class="chapter" data-level="6.3.4" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#final-forecasts"><i class="fa fa-check"></i><b>6.3.4</b> Final forecasts</a></li>
<li class="chapter" data-level="6.3.5" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#limitations-and-variations"><i class="fa fa-check"></i><b>6.3.5</b> Limitations and variations</a></li>
<li class="chapter" data-level="6.3.6" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#the-facilitator"><i class="fa fa-check"></i><b>6.3.6</b> The facilitator</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#forecasting-by-analogy"><i class="fa fa-check"></i><b>6.4</b> Forecasting by analogy</a></li>
<li class="chapter" data-level="6.5" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#scenario-forecasting"><i class="fa fa-check"></i><b>6.5</b> Scenario forecasting</a></li>
<li class="chapter" data-level="6.6" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#new-product-forecasting"><i class="fa fa-check"></i><b>6.6</b> New product forecasting</a><ul>
<li class="chapter" data-level="6.6.1" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#sales-force-composite"><i class="fa fa-check"></i><b>6.6.1</b> Sales force composite</a></li>
<li class="chapter" data-level="6.6.2" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#executive-opinion"><i class="fa fa-check"></i><b>6.6.2</b> Executive opinion</a></li>
<li class="chapter" data-level="6.6.3" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#customer-intentions"><i class="fa fa-check"></i><b>6.6.3</b> Customer intentions</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#judgmental-adjustments"><i class="fa fa-check"></i><b>6.7</b> Judgmental adjustments</a><ul>
<li class="chapter" data-level="6.7.1" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#use-adjustments-sparingly"><i class="fa fa-check"></i><b>6.7.1</b> Use adjustments sparingly</a></li>
<li class="chapter" data-level="6.7.2" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#apply-a-structured-approach"><i class="fa fa-check"></i><b>6.7.2</b> Apply a structured approach</a></li>
<li class="chapter" data-level="6.7.3" data-path="judgmental-forecasts.html"><a href="judgmental-forecasts.html#example-tourism-forecasting-committee-tfc"><i class="fa fa-check"></i><b>6.7.3</b> Example: Tourism Forecasting Committee (TFC)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html"><i class="fa fa-check"></i><b>7</b> Time series regression models</a><ul>
<li class="chapter" data-level="7.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#the-linear-model"><i class="fa fa-check"></i><b>7.1</b> The linear model</a><ul>
<li class="chapter" data-level="7.1.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#simple-linear-regression"><i class="fa fa-check"></i><b>7.1.1</b> Simple linear regression</a></li>
<li class="chapter" data-level="7.1.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#multiple-linear-regression"><i class="fa fa-check"></i><b>7.1.2</b> Multiple linear regression</a></li>
<li class="chapter" data-level="7.1.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#assumptions"><i class="fa fa-check"></i><b>7.1.3</b> Assumptions</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#least-squares-estimation"><i class="fa fa-check"></i><b>7.2</b> Least squares estimation</a><ul>
<li class="chapter" data-level="7.2.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#fitted-values"><i class="fa fa-check"></i><b>7.2.1</b> Fitted values</a></li>
<li class="chapter" data-level="7.2.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#goodness-of-fit"><i class="fa fa-check"></i><b>7.2.2</b> Goodness of fit</a></li>
<li class="chapter" data-level="7.2.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#standard-error-of-the-regression"><i class="fa fa-check"></i><b>7.2.3</b> Standard error of the regression</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#evaluating-a-regression-model"><i class="fa fa-check"></i><b>7.3</b> Evaluating a regression model</a><ul>
<li class="chapter" data-level="7.3.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#residual-plots-against-predictors"><i class="fa fa-check"></i><b>7.3.1</b> Residual plots against predictors</a></li>
<li class="chapter" data-level="7.3.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#residual-plots-against-fitted-values"><i class="fa fa-check"></i><b>7.3.2</b> Residual plots against fitted values</a></li>
<li class="chapter" data-level="7.3.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#outliers-and-influential-observations"><i class="fa fa-check"></i><b>7.3.3</b> Outliers and influential observations</a></li>
<li class="chapter" data-level="7.3.4" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#the-performance-package"><i class="fa fa-check"></i><b>7.3.4</b> The <code>performance</code> package</a></li>
<li class="chapter" data-level="7.3.5" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#spurious-regression"><i class="fa fa-check"></i><b>7.3.5</b> Spurious regression</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#some-useful-predictors"><i class="fa fa-check"></i><b>7.4</b> Some useful predictors</a><ul>
<li class="chapter" data-level="7.4.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#trend"><i class="fa fa-check"></i><b>7.4.1</b> Trend</a></li>
<li class="chapter" data-level="7.4.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#seasonal-dummy-variables"><i class="fa fa-check"></i><b>7.4.2</b> Seasonal dummy variables</a></li>
<li class="chapter" data-level="7.4.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#example-australian-quarterly-beer-production"><i class="fa fa-check"></i><b>7.4.3</b> Example: Australian quarterly beer production</a></li>
<li class="chapter" data-level="7.4.4" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#intervention-variables"><i class="fa fa-check"></i><b>7.4.4</b> Intervention variables</a></li>
<li class="chapter" data-level="7.4.5" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#trading-days"><i class="fa fa-check"></i><b>7.4.5</b> Trading days</a></li>
<li class="chapter" data-level="7.4.6" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#distributed-lags"><i class="fa fa-check"></i><b>7.4.6</b> Distributed lags</a></li>
<li class="chapter" data-level="7.4.7" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#easter"><i class="fa fa-check"></i><b>7.4.7</b> Easter</a></li>
<li class="chapter" data-level="7.4.8" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#fourier-sereis"><i class="fa fa-check"></i><b>7.4.8</b> Fourier sereis</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#selecting-predictors"><i class="fa fa-check"></i><b>7.5</b> Selecting predictors</a><ul>
<li class="chapter" data-level="7.5.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#adjusst-r-square"><i class="fa fa-check"></i><b>7.5.1</b> Adjusst R square</a></li>
<li class="chapter" data-level="7.5.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#cross-validation"><i class="fa fa-check"></i><b>7.5.2</b> Cross validation</a></li>
<li class="chapter" data-level="7.5.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#akaikes-information-criterion"><i class="fa fa-check"></i><b>7.5.3</b> Akaike’s Information Criterion</a></li>
<li class="chapter" data-level="7.5.4" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#bayesian-information-criterion"><i class="fa fa-check"></i><b>7.5.4</b> Bayesian Information Criterion</a></li>
<li class="chapter" data-level="7.5.5" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#which-measure-should-we-suse"><i class="fa fa-check"></i><b>7.5.5</b> Which measure should we suse</a></li>
<li class="chapter" data-level="7.5.6" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#example-us-consumption"><i class="fa fa-check"></i><b>7.5.6</b> Example: US consumption</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#forecasting-with-regression"><i class="fa fa-check"></i><b>7.6</b> Forecasting with regression</a><ul>
<li class="chapter" data-level="7.6.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#ex-ante-versus-ex-post-forecasts"><i class="fa fa-check"></i><b>7.6.1</b> Ex-ante versus ex-post forecasts</a></li>
<li class="chapter" data-level="7.6.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#example-australian-quarterly-beer-production-1"><i class="fa fa-check"></i><b>7.6.2</b> Example: Australian quarterly beer production</a></li>
<li class="chapter" data-level="7.6.3" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#scenario-based-forecasting"><i class="fa fa-check"></i><b>7.6.3</b> Scenario based forecasting</a></li>
<li class="chapter" data-level="7.6.4" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#prediction-intervals"><i class="fa fa-check"></i><b>7.6.4</b> Prediction intervals</a></li>
<li class="chapter" data-level="7.6.5" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#building-a-predictive-regression-model"><i class="fa fa-check"></i><b>7.6.5</b> Building a predictive regression model</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#matrix-formulation"><i class="fa fa-check"></i><b>7.7</b> Matrix formulation</a></li>
<li class="chapter" data-level="7.8" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#nonlinear-regression"><i class="fa fa-check"></i><b>7.8</b> Nonlinear regression</a><ul>
<li class="chapter" data-level="7.8.1" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#forecasting-with-a-nonlinear-trend"><i class="fa fa-check"></i><b>7.8.1</b> Forecasting with a nonlinear trend</a></li>
<li class="chapter" data-level="7.8.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#example-boston-marathon-winning-times"><i class="fa fa-check"></i><b>7.8.2</b> Example: Boston marathon winning times</a></li>
</ul></li>
<li class="chapter" data-level="7.9" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#correlation-causation-and-forecasting"><i class="fa fa-check"></i><b>7.9</b> Correlation, causation and forecasting</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html"><i class="fa fa-check"></i><b>8</b> Exponential smoothing</a><ul>
<li class="chapter" data-level="8.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#simple-exponential-smoothing"><i class="fa fa-check"></i><b>8.1</b> Simple exponential smoothing</a><ul>
<li class="chapter" data-level="8.1.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#weighted-average-form"><i class="fa fa-check"></i><b>8.1.1</b> Weighted average form</a></li>
<li class="chapter" data-level="8.1.2" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#component-form"><i class="fa fa-check"></i><b>8.1.2</b> Component form</a></li>
<li class="chapter" data-level="8.1.3" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#flat-forecast"><i class="fa fa-check"></i><b>8.1.3</b> Flat forecast</a></li>
<li class="chapter" data-level="8.1.4" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#estimation"><i class="fa fa-check"></i><b>8.1.4</b> Estimation</a></li>
<li class="chapter" data-level="8.1.5" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-algerian-exports"><i class="fa fa-check"></i><b>8.1.5</b> Example: Algerian exports</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#methods-with-trend-and-seasonality"><i class="fa fa-check"></i><b>8.2</b> Methods with trend and seasonality</a><ul>
<li class="chapter" data-level="8.2.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#holts-linear-trend-method"><i class="fa fa-check"></i><b>8.2.1</b> Holt’s linear trend method</a></li>
<li class="chapter" data-level="8.2.2" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-australian-population"><i class="fa fa-check"></i><b>8.2.2</b> Example: Australian population</a></li>
<li class="chapter" data-level="8.2.3" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#damped-trend-methods"><i class="fa fa-check"></i><b>8.2.3</b> Damped trend methods</a></li>
<li class="chapter" data-level="8.2.4" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-australian-population-continued"><i class="fa fa-check"></i><b>8.2.4</b> Example: Australian Population (continued)</a></li>
<li class="chapter" data-level="8.2.5" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-internet-usage"><i class="fa fa-check"></i><b>8.2.5</b> Example: Internet usage</a></li>
<li class="chapter" data-level="8.2.6" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#holt-winters-additive-method"><i class="fa fa-check"></i><b>8.2.6</b> Holt-Winters’ additive method</a></li>
<li class="chapter" data-level="8.2.7" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#holt-winters-multiplicative-method"><i class="fa fa-check"></i><b>8.2.7</b> Holt-Winters’ multiplicative method</a></li>
<li class="chapter" data-level="8.2.8" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-domestic-overnight-trips-in-australia"><i class="fa fa-check"></i><b>8.2.8</b> Example: Domestic overnight trips in Australia</a></li>
<li class="chapter" data-level="8.2.9" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#holt-winters-damped-method"><i class="fa fa-check"></i><b>8.2.9</b> Holt-Winters’ damped method</a></li>
<li class="chapter" data-level="8.2.10" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-holt-winters-method-with-daily-data"><i class="fa fa-check"></i><b>8.2.10</b> Example: Holt-Winters method with daily data</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#a-taxonomy-of-exponential-smoothing-methods"><i class="fa fa-check"></i><b>8.3</b> A taxonomy of exponential smoothing methods</a></li>
<li class="chapter" data-level="8.4" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#innovations-state-space-models-for-exponential-smoothing"><i class="fa fa-check"></i><b>8.4</b> Innovations state space models for exponential smoothing</a><ul>
<li class="chapter" data-level="8.4.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#etsann-simple-exponential-smoothing-with-additive-errors"><i class="fa fa-check"></i><b>8.4.1</b> ETS(A,N,N): simple exponential smoothing with additive errors</a></li>
<li class="chapter" data-level="8.4.2" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#etsmnn-simple-exponential-smoothing-with-multiplicative-errors"><i class="fa fa-check"></i><b>8.4.2</b> ETS(M,N,N): simple exponential smoothing with multiplicative errors</a></li>
<li class="chapter" data-level="8.4.3" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#etsaan-holts-linear-method-with-additive-errors"><i class="fa fa-check"></i><b>8.4.3</b> ETS(A,A,N): Holt’s linear method with additive errors</a></li>
<li class="chapter" data-level="8.4.4" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#ets-man"><i class="fa fa-check"></i><b>8.4.4</b> ETS(M,A,N): Holt’s linear method with multiplicative errors</a></li>
<li class="chapter" data-level="8.4.5" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#other-ets-models"><i class="fa fa-check"></i><b>8.4.5</b> Other ETS models</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#estimation-and-model-selection"><i class="fa fa-check"></i><b>8.5</b> Estimation and model selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#estimating-ets-models"><i class="fa fa-check"></i><b>8.5.1</b> Estimating ETS models</a></li>
<li class="chapter" data-level="8.5.2" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#model-selection-criteria"><i class="fa fa-check"></i><b>8.5.2</b> Model selection criteria</a></li>
<li class="chapter" data-level="8.5.3" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-domestic-holiday-tourist-visitor-nights-in-australia"><i class="fa fa-check"></i><b>8.5.3</b> Example: Domestic holiday tourist visitor nights in Australia</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#forecasting-with-ets-models"><i class="fa fa-check"></i><b>8.6</b> Forecasting with ETS models</a><ul>
<li class="chapter" data-level="8.6.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#example-australia-gas-production"><i class="fa fa-check"></i><b>8.6.1</b> Example: Australia gas production</a></li>
<li class="chapter" data-level="8.6.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#prediction-intervals"><i class="fa fa-check"></i><b>8.6.2</b> Prediction intervals</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html"><i class="fa fa-check"></i><b>9</b> Univariate stationary processes</a><ul>
<li class="chapter" data-level="9.1" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#stationarity"><i class="fa fa-check"></i><b>9.1</b> Stationarity</a><ul>
<li class="chapter" data-level="9.1.1" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#ch9-white-noise"><i class="fa fa-check"></i><b>9.1.1</b> White noise</a></li>
<li class="chapter" data-level="9.1.2" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#tests-for-autocorrelation-and-normality"><i class="fa fa-check"></i><b>9.1.2</b> Tests for autocorrelation and normality</a></li>
<li class="chapter" data-level="9.1.3" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#the-wold-decomposition"><i class="fa fa-check"></i><b>9.1.3</b> The Wold Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#backshift-notation"><i class="fa fa-check"></i><b>9.2</b> Backshift notation</a></li>
<li class="chapter" data-level="9.3" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#autoregressive-models"><i class="fa fa-check"></i><b>9.3</b> Autoregressive models</a><ul>
<li class="chapter" data-level="9.3.1" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#stimulating-an-arp-process"><i class="fa fa-check"></i><b>9.3.1</b> Stimulating an AR(p) process</a></li>
<li class="chapter" data-level="9.3.2" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#decision-of-order-p"><i class="fa fa-check"></i><b>9.3.2</b> Decision of order p</a></li>
<li class="chapter" data-level="9.3.3" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#random-walk"><i class="fa fa-check"></i><b>9.3.3</b> Random walk</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#moving-average-models"><i class="fa fa-check"></i><b>9.4</b> Moving average models</a><ul>
<li class="chapter" data-level="9.4.1" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#simulating-an-maq-process"><i class="fa fa-check"></i><b>9.4.1</b> Simulating an MA(q) process</a></li>
<li class="chapter" data-level="9.4.2" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#decision-of-order-q"><i class="fa fa-check"></i><b>9.4.2</b> Decision of order q</a></li>
<li class="chapter" data-level="9.4.3" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#koyck-transformation-and-invertibility"><i class="fa fa-check"></i><b>9.4.3</b> Koyck transformation and Invertibility</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#arma-models"><i class="fa fa-check"></i><b>9.5</b> ARMA models</a><ul>
<li class="chapter" data-level="9.5.1" data-path="univariate-stationary-processes.html"><a href="univariate-stationary-processes.html#three-representations-of-an-arma-model"><i class="fa fa-check"></i><b>9.5.1</b> Three representations of an ARMA model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="arima-models.html"><a href="arima-models.html"><i class="fa fa-check"></i><b>10</b> ARIMA models</a><ul>
<li class="chapter" data-level="10.1" data-path="arima-models.html"><a href="arima-models.html#differencing"><i class="fa fa-check"></i><b>10.1</b> Differencing</a><ul>
<li class="chapter" data-level="10.1.1" data-path="arima-models.html"><a href="arima-models.html#second-order-differencing"><i class="fa fa-check"></i><b>10.1.1</b> Second-order differencing</a></li>
<li class="chapter" data-level="10.1.2" data-path="arima-models.html"><a href="arima-models.html#seasonal-differencing"><i class="fa fa-check"></i><b>10.1.2</b> Seasonal differencing</a></li>
<li class="chapter" data-level="10.1.3" data-path="arima-models.html"><a href="arima-models.html#unit-root-tests"><i class="fa fa-check"></i><b>10.1.3</b> Unit root tests</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="arima-models.html"><a href="arima-models.html#non-seasonal-arima-models"><i class="fa fa-check"></i><b>10.2</b> Non-seasonal ARIMA models</a><ul>
<li class="chapter" data-level="10.2.1" data-path="arima-models.html"><a href="arima-models.html#understanding-arima-models"><i class="fa fa-check"></i><b>10.2.1</b> Understanding ARIMA models</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="arima-models.html"><a href="arima-models.html#estimation-and-order-selection"><i class="fa fa-check"></i><b>10.3</b> Estimation and order selection</a><ul>
<li class="chapter" data-level="10.3.1" data-path="arima-models.html"><a href="arima-models.html#mle"><i class="fa fa-check"></i><b>10.3.1</b> MLE</a></li>
<li class="chapter" data-level="10.3.2" data-path="arima-models.html"><a href="arima-models.html#information-criteria"><i class="fa fa-check"></i><b>10.3.2</b> Information Criteria</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="arima-models.html"><a href="arima-models.html#the-arima-function"><i class="fa fa-check"></i><b>10.4</b> The <code>ARIMA()</code> function</a><ul>
<li class="chapter" data-level="10.4.1" data-path="arima-models.html"><a href="arima-models.html#algorithm"><i class="fa fa-check"></i><b>10.4.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.4.2" data-path="arima-models.html"><a href="arima-models.html#modelling-procedure"><i class="fa fa-check"></i><b>10.4.2</b> Modelling procedure</a></li>
<li class="chapter" data-level="10.4.3" data-path="arima-models.html"><a href="arima-models.html#example-seasonally-adjusted-electrical-equipment-orders"><i class="fa fa-check"></i><b>10.4.3</b> Example: Seasonally adjusted electrical equipment orders</a></li>
<li class="chapter" data-level="10.4.4" data-path="arima-models.html"><a href="arima-models.html#plotting-the-characteristic-roots"><i class="fa fa-check"></i><b>10.4.4</b> Plotting the characteristic roots</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="arima-models.html"><a href="arima-models.html#forecasting-with-arima-models"><i class="fa fa-check"></i><b>10.5</b> Forecasting with ARIMA models</a><ul>
<li class="chapter" data-level="10.5.1" data-path="arima-models.html"><a href="arima-models.html#point-forecasts"><i class="fa fa-check"></i><b>10.5.1</b> Point forecasts</a></li>
<li class="chapter" data-level="10.5.2" data-path="time-series-regression-models.html"><a href="time-series-regression-models.html#prediction-intervals"><i class="fa fa-check"></i><b>10.5.2</b> Prediction intervals</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="arima-models.html"><a href="arima-models.html#seasonal-arima-models"><i class="fa fa-check"></i><b>10.6</b> Seasonal ARIMA models</a><ul>
<li class="chapter" data-level="10.6.1" data-path="arima-models.html"><a href="arima-models.html#acf-and-pacf"><i class="fa fa-check"></i><b>10.6.1</b> ACF and PACF</a></li>
<li class="chapter" data-level="10.6.2" data-path="arima-models.html"><a href="arima-models.html#example-european-quarterly-retail-trade"><i class="fa fa-check"></i><b>10.6.2</b> Example: European quarterly retail trade</a></li>
<li class="chapter" data-level="10.6.3" data-path="arima-models.html"><a href="arima-models.html#example-corticosteroid-drug-sales-in-australia"><i class="fa fa-check"></i><b>10.6.3</b> Example: Corticosteroid drug sales in Australia</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="arima-models.html"><a href="arima-models.html#ets-and-arima"><i class="fa fa-check"></i><b>10.7</b> ETS and ARIMA</a><ul>
<li class="chapter" data-level="10.7.1" data-path="arima-models.html"><a href="arima-models.html#example-comparing-arima-and-ets-on-non-seasonal-data"><i class="fa fa-check"></i><b>10.7.1</b> Example: Comparing <code>ARIMA()</code> and <code>ETS()</code> on non-seasonal data</a></li>
<li class="chapter" data-level="10.7.2" data-path="arima-models.html"><a href="arima-models.html#example-comparing-arima-and-ets-on-seasonal-data"><i class="fa fa-check"></i><b>10.7.2</b> Example: Comparing <code>ARIMA()</code> and <code>ETS()</code> on seasonal data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html"><i class="fa fa-check"></i><b>11</b> Dynamic regression models</a><ul>
<li class="chapter" data-level="11.1" data-path="exponential-smoothing.html"><a href="exponential-smoothing.html#estimation"><i class="fa fa-check"></i><b>11.1</b> Estimation</a></li>
<li class="chapter" data-level="11.2" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#regression-with-arima-errors"><i class="fa fa-check"></i><b>11.2</b> Regression with ARIMA errors</a><ul>
<li class="chapter" data-level="11.2.1" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#example-us-personal-consumption-and-income"><i class="fa fa-check"></i><b>11.2.1</b> Example: US Personal Consumption and Income</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#forecasting"><i class="fa fa-check"></i><b>11.3</b> Forecasting</a><ul>
<li class="chapter" data-level="11.3.1" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#example-forecasting-electricity-demand"><i class="fa fa-check"></i><b>11.3.1</b> Example: Forecasting electricity demand</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#deterministic-and-stochastic-trends"><i class="fa fa-check"></i><b>11.4</b> Deterministic and stochastic trends</a><ul>
<li class="chapter" data-level="11.4.1" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#example-international-visitors-to-australia"><i class="fa fa-check"></i><b>11.4.1</b> Example: International visitors to Australia</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#dynamic-harmonic-regression"><i class="fa fa-check"></i><b>11.5</b> Dynamic harmonic regression</a><ul>
<li class="chapter" data-level="11.5.1" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#example-australian-eating-out-expenditure"><i class="fa fa-check"></i><b>11.5.1</b> Example: Australian eating out expenditure</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#lagged-predictors"><i class="fa fa-check"></i><b>11.6</b> Lagged predictors</a><ul>
<li class="chapter" data-level="11.6.1" data-path="dynamic-regression-models.html"><a href="dynamic-regression-models.html#example-tv-advertising-and-insurance-quotations"><i class="fa fa-check"></i><b>11.6.1</b> Example: TV advertising and insurance quotations</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">written with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Notes for “Forecasting: Principles and Practice, 3rd edition”</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="univariate-stationary-processes" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Univariate stationary processes</h1>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="univariate-stationary-processes.html#cb167-1"></a><span class="kw">library</span>(tsibble)</span>
<span id="cb167-2"><a href="univariate-stationary-processes.html#cb167-2"></a><span class="kw">library</span>(tsibbledata)</span>
<span id="cb167-3"><a href="univariate-stationary-processes.html#cb167-3"></a><span class="kw">library</span>(fable)</span>
<span id="cb167-4"><a href="univariate-stationary-processes.html#cb167-4"></a><span class="kw">library</span>(feasts)</span>
<span id="cb167-5"><a href="univariate-stationary-processes.html#cb167-5"></a><span class="kw">library</span>(lubridate)</span>
<span id="cb167-6"><a href="univariate-stationary-processes.html#cb167-6"></a><span class="kw">library</span>(pins)</span>
<span id="cb167-7"><a href="univariate-stationary-processes.html#cb167-7"></a><span class="kw">library</span>(slider)</span>
<span id="cb167-8"><a href="univariate-stationary-processes.html#cb167-8"></a><span class="kw">library</span>(patchwork)</span></code></pre></div>
<div id="stationarity" class="section level2">
<h2><span class="header-section-number">9.1</span> Stationarity</h2>
<p>Given a stochastic process (a data generating process), <span class="math inline">\(y_1, y_2, \dots, y_T\)</span>, the real numbers called time series <span class="math inline">\(y_1^{(1)}, y_2^{(1)}, \cdots, y_T^{(1)}\)</span> is just an set of observed value, or a realization of the process.It is obvious, however, that there
is not just one realisation of such a process, but, in principle, an arbitrary
number of realisations which all have the same statistical properties as they
all result from the same data generating process.</p>
<p>In the following, a time series is considered as one realisation of the underlying stochastic process. We can also regard the stochastic process as the entirety of all of its possible realisations. To make the notation as simple as possible, we will not distinguish between the process itself and itsrealisation. This can be taken out of the context.</p>
<p>We can define a <strong>strict-sense</strong> stationarity of a time series to be :</p>
<p><span class="math display">\[
\begin{aligned}
F_Y(y_{t_1+\alpha}, y_{t_2+\alpha}, \dots, y_{t_n+\alpha}) = F_Y(y_{t_1+}, y_{t_2}, \dots, y_{t_n})
\end{aligned}
\]</span>
Where <span class="math inline">\(F_Y(*)\)</span> stands for the multivariate distribution of a specific window of that time series. This basically means the distribution stays the same stays same when you pick a window on that time series and then make a shift <span class="math inline">\(\alpha\)</span>.In the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time.</p>
<p>However, strict-sense stationarity are rarely practical or helpful in most cases. For this reason we place more emphasis on <strong>weak</strong> or <strong>wide-sense stationarity</strong>. A stochastic process is weak stationary if <span class="math inline">\(\forall k\)</span> we have:</p>
<p><span class="math display">\[
\begin{aligned}
\text{mean stationarity}: \text{E}(y_t) &amp;= \mu_t  = \mu\\
\text{variance stationarity}:\text{Var}(y_t) &amp;= \sigma_t^2 = \sigma^2  \\
\text{covariance stationarity}: \text{Cov}(y_t, y_s) &amp;= \text{E}(y_t - \mu)(y_s - \mu) = f(|s-t|)
\end{aligned}
\]</span></p>
<p>(covariance stationarity means that covariance is only a function of the distance <span class="math inline">\(|s-t|\)</span>, but not related to point <span class="math inline">\(t\)</span>).</p>
<p>Because we only assume this kind of stationarity in the following, we will mostly drop the adjective weak.</p>
<p>As variance stationarity immediately results from covariance stationarity for <span class="math inline">\(s = t\)</span>, a stochastic process is weakly stationary when it is mean and covariance stationary.</p>
<p>We can also refer to a time series to be stationary if the underlying stochastic process is stationary. So for a stationary time series, what do we expect to see?</p>
<ol style="list-style-type: decimal">
<li>No trend and seasonality (constant mean, local compared to global)</li>
<li>No sharp rise and fall (constant variance, local compared to global)</li>
</ol>
<p><a href="https://towardsdatascience.com/stationarity-in-time-series-analysis-90c94f27322">Why does stationarity matter</a></p>
<p>Why is this important? First, because stationary processes are easier to analyze. Without a formal definition for processes generating time series data, it is already clear that stationary processes are a sub-class of a wider family of possible models of reality. This sub-class is much easier to model and investigate. The above informal definition also hints that such processes should be possible to predict, as the way they change is predictable.</p>
<p>Although it sounds a bit streetlight effect-ish that simpler theories or models should become more prominent, it is actually quite a common pattern in science, and for good reason. In many cases simple models can be surprisingly useful, either as building blocks in constructing more elaborate ones, or as helpful approximations to complex phenomena. As it turns out, this also true for stationary processes.</p>
<p>Due to these properties, stationarity has become a common assumption for many practices and tools in time series analysis. These include trend estimation, forecasting and causal inference, among others.</p>
<div id="ch9-white-noise" class="section level3">
<h3><span class="header-section-number">9.1.1</span> White noise</h3>
<p>White noise is first introduced in Section <a href="the-forecasters-toolbox.html#white-noise">5.4.1</a>. As it turns out, the stochastic process behind a white noise is called a <strong>pure random process</strong> or simply white noise process. Such a process satisfies <span class="math inline">\(\text{E}(y_t) = 0\)</span>, <span class="math inline">\(\text{Var}(y_t) = \sigma^2\)</span> and <span class="math inline">\(\forall k \not= 0, \text{Cov}(y_t, y_{t+k}) = 0\)</span>.</p>
<p>Apparently, white noise is generated by a special case of stationary process. Notably it should have contant mean 0, constant variance <span class="math inline">\(\sigma^2\)</span> and no autocorrelation.</p>
</div>
<div id="tests-for-autocorrelation-and-normality" class="section level3">
<h3><span class="header-section-number">9.1.2</span> Tests for autocorrelation and normality</h3>
<p>In Section <a href="the-forecasters-toolbox.html#portmanteau-tests-for-autocorrelation">5.4.3</a> we mentioned statistical tests to decide whether there is no significant correlation among residuals. And here are some details.</p>
<p>First, it is important to know that white noise, like <strong>all stationary time series</strong>, has some nice <strong>consistent estimators</strong> to estimate its mean, variance and covariance:</p>
<p><span class="math display">\[
\begin{aligned}
\hat{\mu} &amp;= \frac{1}{T}\sum_{t=1}^{T}y_t \\
\hat{\gamma}(0) = \hat{\sigma^2} &amp;= \frac{1}{T}\sum_{t=1}^{T}(y_t - \hat{\mu})^2  \\
\hat{\gamma}(k) = \hat{\text{Cov}}(y_t, y_{t+k}) &amp;= \frac{1}{T}\sum_{t=1}^{T-k}{(y_t - \hat{\mu})(y_{t+k} - \hat{\mu})} \quad k = 1, 2, 3, \dots
\end{aligned}
\]</span></p>
<p>We can also get the consistent estimator of autocorrelation coefficient:</p>
<p><span class="math display">\[
r_k = \frac{\sum_{t=1}^{T-k} = {(y_t - \hat{\mu})(y_{t+k} - \hat{\mu})}}{\sum_{t=1}^{T}(y_t - \hat{\mu})^2} = \frac{\hat{\gamma}(k)}{\hat{\gamma}(0)}
\]</span></p>
<p>For white noise processes, its variance can be approximated by <span class="math inline">\(1/T\)</span> and is <strong>asymptotically normally distributed</strong>. Due to this, pointwise <span class="math inline">\(95\)</span> percent confidence intervals of <span class="math inline">\(\pm1.96 /\sqrt{T}\)</span> ? are often indicated for the estimated autocorrelation coefficients.</p>
<p>In order to evaluate estimated time series models, it is important to know whether the residuals of the model really have the properties of a pure random process, in particular, whether they are uncorrelated. Thus, the null hypothesis to be tested is the first <span class="math inline">\(h\)</span> <span class="math inline">\(r_k\)</span> is zero:</p>
<p><span class="math display">\[
H_0: r_1 = r_2 = \cdots = r_h, h&lt;T
\]</span>
The first possibility to check this is to apply the 95 percent confidence limits <span class="math inline">\(\pm2/\sqrt{T}\)</span> valid under the null hypothesis to every estimated correlation coefficient. Under <span class="math inline">\(H_0\)</span> at most 5 percent of autocorrelation coefficients may lie outside these limits.</p>
<p>To make a global statement, i.e. to test the common hypothesis whether a given number of <span class="math inline">\(h\)</span> autocorrelation coefficients are zero altogether, GEORGE E.P. BOX and DAVID A. PIERCE (1970) have developed the following test statistic:</p>
<p><span class="math display">\[
Q = T \sum_{k = 1}^{h}{r^2_k}
\]</span></p>
<p>Under the null hypothesis it is asymptotically <span class="math inline">\(\chi^2\)</span> distributed with <span class="math inline">\(h - K\)</span> degrees of freedom, <span class="math inline">\(K\)</span> being the number of estimated parameters. It is suggested that use <span class="math inline">\(h = 10\)</span> for non-seasonal data and <span class="math inline">\(h = 2m\)</span> for seasonal data, where <span class="math inline">\(m\)</span> is the period of seasonality.</p>
<p>As – strictly applied – the distribution of this test statistic holds only asymptotically (under large sample), GRETA M. LJUNG and GEORGE E.P. BOX (1978) proposed the following modification for small samples, known as the <strong>ljung-box test</strong></p>
<p><span class="math display">\[
Q^* = T(T + 2)\sum_{k = 1}^{h}{(T-K)^{-1}}r_k^2
\]</span>
<span class="math inline">\(Q^*\)</span> is also asymptotically <span class="math inline">\(\chi^2\)</span> distributed with <span class="math inline">\(m - k\)</span> degrees of freedom,</p>
<p>An alternative to these testing procedures is the <strong>Breusch-Godfrey test</strong> (BG test), also known asa the <strong>Lagrange-Multiplier Test</strong> (LM Test) developed by TREVOR S. BREUSCH (1978) and LESLIE G. GODFREY (1978). Like for the <span class="math inline">\(Q\)</span> (<span class="math inline">\(Q^*\)</span>) test the null hypothesis is which is tested against the alternative that the residuals follow an autoregressive or a moving average process of order h. The test can be performed with an auxiliary regression. The estimated residuals are regressed on the explanatory variables of the main model and on the lagged residuals, up to order h. The test statistic which is <span class="math inline">\(\chi^2\)</span> distributed with h degrees of freedom is given by <span class="math inline">\(TR^2\)</span> the auxiliary regression, with T being the number of observations. Alternatively, an <span class="math inline">\(F\)</span> test can be used for testing the combined significance of the lagged residuals in the auxiliary regression.</p>
<p>The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models.</p>
<p>Compared to the Durbin-Watson test which is used in traditional econometrics for testing autocorrelation of the residuals of an estimated model, the <span class="math inline">\(Q\)</span>, <span class="math inline">\(Q^*\)</span> as well as the BG test have two major advantages: firstly, they can check for autocorrelation of any order, and not only one order at a time. Secondly, the results are also correct if there are lagged endogenous variables in the regression equation, whereas in such cases the results of the Durbin-Watson test are biased in favour of the null hypothesis.</p>
<p>As mentioned in Section <a href="the-forecasters-toolbox.html#white-noise">5.4.1</a>, we often assume normally distributed white noise. But the fact that the residuals are not autocorrelated does not imply that they are stochastic independence if the variauted, as the usual testing procedures are based on this aspecially the third (skewness) and fourth moments (kurtosis) are important (0 and 3 respectively based on normality hypothesis).</p>
<p><span class="math display">\[
\begin{aligned}
\text{skewness}: \hat{S} &amp;= \frac{1}{T}\frac{\sum_{t = 1}^{T}{(x_i - \mu)^3}}{\hat{\gamma}(0)^{3/2}} \\
\text{kurtosis}: \hat{K} &amp;= \frac{1}{T}\frac{\sum_{t = 1}^{T}{(x_i - \mu)^4}}{\hat{\gamma}(0)^{2}}
\end{aligned}
\]</span></p>
<p>Using the skewness S and the kurtosis K, CARLOS M. JARQUE and A NIL K. BERA (1980) proposed a test for normality. It can be applied directly on the time series itself (or on its differences). Usually, however, it is applied to check estimated regression residuals. The test statistic</p>
<p><span class="math display">\[
\text{JB} = \frac{T}{6}[\hat{S}^2 + \frac{1}{4}(\hat{K} - 3)^2] 
\]</span>
is <span class="math inline">\(\chi^2\)</span> distributed with 2 degrees of freedom. T is again the sample size. The
hypothesis that the variable is normally distributed is rejected whenever the values of the test statistic are larger than the corresponding critical values.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="univariate-stationary-processes.html#cb168-1"></a><span class="co"># use arima.sim() to stimulate a AR(1) process</span></span>
<span id="cb168-2"><a href="univariate-stationary-processes.html#cb168-2"></a><span class="co"># list specification for AR(1) model with phi = 0.5, and  the std dev of the Gaussian errors to be 0.5</span></span>
<span id="cb168-3"><a href="univariate-stationary-processes.html#cb168-3"></a>AR_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">ar =</span> <span class="fl">0.9</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb168-4"><a href="univariate-stationary-processes.html#cb168-4"></a>AR1 &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> AR_spec)</span>
<span id="cb168-5"><a href="univariate-stationary-processes.html#cb168-5"></a></span>
<span id="cb168-6"><a href="univariate-stationary-processes.html#cb168-6"></a>AR1_aug &lt;-<span class="st"> </span>AR1 <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb168-7"><a href="univariate-stationary-processes.html#cb168-7"></a><span class="st">  </span><span class="kw">as_tsibble</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb168-8"><a href="univariate-stationary-processes.html#cb168-8"></a><span class="st">  </span><span class="kw">model</span>(<span class="kw">ARIMA</span>(value <span class="op">~</span><span class="st"> </span><span class="kw">PDQ</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>))) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb168-9"><a href="univariate-stationary-processes.html#cb168-9"></a><span class="st">  </span><span class="kw">augment</span>()</span>
<span id="cb168-10"><a href="univariate-stationary-processes.html#cb168-10"></a></span>
<span id="cb168-11"><a href="univariate-stationary-processes.html#cb168-11"></a><span class="co"># box pierce test, non-seasonal data h = 10</span></span>
<span id="cb168-12"><a href="univariate-stationary-processes.html#cb168-12"></a>AR1_aug <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">features</span>(.resid, box_pierce, <span class="dt">lag =</span> <span class="dv">10</span>, <span class="dt">dof =</span> <span class="dv">0</span>)</span>
<span id="cb168-13"><a href="univariate-stationary-processes.html#cb168-13"></a><span class="co">#&gt; # A tibble: 1 x 3</span></span>
<span id="cb168-14"><a href="univariate-stationary-processes.html#cb168-14"></a><span class="co">#&gt;   .model                      bp_stat bp_pvalue</span></span>
<span id="cb168-15"><a href="univariate-stationary-processes.html#cb168-15"></a><span class="co">#&gt;   &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;</span></span>
<span id="cb168-16"><a href="univariate-stationary-processes.html#cb168-16"></a><span class="co">#&gt; 1 ARIMA(value ~ PDQ(1, 0, 0))    4.08     0.944</span></span>
<span id="cb168-17"><a href="univariate-stationary-processes.html#cb168-17"></a><span class="co"># ljung box test</span></span>
<span id="cb168-18"><a href="univariate-stationary-processes.html#cb168-18"></a>AR1_aug <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">features</span>(.resid, ljung_box, <span class="dt">lag =</span> <span class="dv">10</span>, <span class="dt">dof =</span> <span class="dv">0</span>)</span>
<span id="cb168-19"><a href="univariate-stationary-processes.html#cb168-19"></a><span class="co">#&gt; # A tibble: 1 x 3</span></span>
<span id="cb168-20"><a href="univariate-stationary-processes.html#cb168-20"></a><span class="co">#&gt;   .model                      lb_stat lb_pvalue</span></span>
<span id="cb168-21"><a href="univariate-stationary-processes.html#cb168-21"></a><span class="co">#&gt;   &lt;chr&gt;                         &lt;dbl&gt;     &lt;dbl&gt;</span></span>
<span id="cb168-22"><a href="univariate-stationary-processes.html#cb168-22"></a><span class="co">#&gt; 1 ARIMA(value ~ PDQ(1, 0, 0))    5.01     0.890</span></span>
<span id="cb168-23"><a href="univariate-stationary-processes.html#cb168-23"></a><span class="co"># Breusch Godfrey test: lmtest::bgtest()</span></span>
<span id="cb168-24"><a href="univariate-stationary-processes.html#cb168-24"></a><span class="co"># durbin watson test </span></span>
<span id="cb168-25"><a href="univariate-stationary-processes.html#cb168-25"></a>AR1_aug <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(.resid) <span class="op">%&gt;%</span><span class="st"> </span>car<span class="op">::</span><span class="kw">durbinWatsonTest</span>(<span class="dt">max.lag =</span> <span class="dv">10</span>)</span>
<span id="cb168-26"><a href="univariate-stationary-processes.html#cb168-26"></a><span class="co">#&gt;  [1] 1.892617 1.987601 1.948205 1.773167 2.037944 1.857135 1.832362 1.445376</span></span>
<span id="cb168-27"><a href="univariate-stationary-processes.html#cb168-27"></a><span class="co">#&gt;  [9] 1.114101 1.588983</span></span>
<span id="cb168-28"><a href="univariate-stationary-processes.html#cb168-28"></a><span class="co"># jb test (normality)</span></span>
<span id="cb168-29"><a href="univariate-stationary-processes.html#cb168-29"></a>AR1_aug <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">pull</span>(.resid) <span class="op">%&gt;%</span><span class="st"> </span>normtest<span class="op">::</span><span class="kw">jb.norm.test</span>()</span>
<span id="cb168-30"><a href="univariate-stationary-processes.html#cb168-30"></a><span class="co">#&gt; </span></span>
<span id="cb168-31"><a href="univariate-stationary-processes.html#cb168-31"></a><span class="co">#&gt; 	Jarque-Bera test for normality</span></span>
<span id="cb168-32"><a href="univariate-stationary-processes.html#cb168-32"></a><span class="co">#&gt; </span></span>
<span id="cb168-33"><a href="univariate-stationary-processes.html#cb168-33"></a><span class="co">#&gt; data:  .</span></span>
<span id="cb168-34"><a href="univariate-stationary-processes.html#cb168-34"></a><span class="co">#&gt; JB = 6.5604, p-value = 0.028</span></span></code></pre></div>
</div>
<div id="the-wold-decomposition" class="section level3">
<h3><span class="header-section-number">9.1.3</span> The Wold Decomposition</h3>
<p>The Wold Decomposition is a general property that all stationary process share. Actually it exists for every covariance stationary, purely non-deterministic stochastic process: After subtracting the mean function, each of such processes can be represented by a
linear combination of a series of uncorrelated random variables with zero mean and constant variance, which are the errors made in forecasting <span class="math inline">\(y\)</span> t on
the basis of a linear function of lagged <span class="math inline">\(y\)</span>(i.e., the RHS of a MA(<span class="math inline">\(\infty\)</span>) model after subtracting the constant term).</p>
<p><em>Purely non-deterministic</em> means that all additive deterministic components of a time series have to be subtracted in advance. By using its own lagged values, any deterministic component can be perfectly predicted in advance. This holds, for example, for a constant mean, as well as for periodic, polynomial, or exponential series in t. Thus, one can write (see proof in Seciton <a href="univariate-stationary-processes.html#koyck-transformation-and-invertibility">9.4.3</a>):</p>
<p><span class="math display" id="eq:wold-decomposition">\[\begin{equation}
\tag{9.1}
y_t - \mu_t = \sum_{i-0}^{\infty}\psi_i\varepsilon_i
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\varepsilon_t\)</span> is white noise and <span class="math inline">\(\psi_i\)</span> satisfy <span class="math inline">\(\sum_{i-0}^{\infty}\psi_i^2 &lt; \infty\)</span>. The quadratic convergence of the series of the <span class="math inline">\(\psi_i\)</span> guarantees the existence
of second moments of the process <span class="math inline">\(y_t\)</span>. There is no need of any distributional assumption for this decomposition to hold. Especially, there is no need of the <span class="math inline">\(\varepsilon_t\)</span> to be independent, it is sufficient that they are (linearly) uncorrelated.</p>
<p>From Wold Decomposition we can derive <span class="math inline">\(y_t\)</span> has properties as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\text{E}(y_t) &amp;= \mu_t \\
\text{Var}(y_t) = \text{E}[(y_t - \mu_t)^2] &amp;= E[(\varepsilon_t + \psi_1\varepsilon_{t-1} + ...)^2] = \sigma^2\sum_{i=0}^{\infty}\psi_i^2 \\ 
\end{aligned}
\]</span>
And as for variance:
<span class="math display">\[
\begin{split}
\text{Cov}(y_t, y_{t+k}) &amp;= \text{E}[(y_t - \mu_t)(y_{t+k} - \mu_{t+k})] \\ 
&amp;= \text{E}[(\varepsilon_t + \psi_1\varepsilon_{t-1} + \cdots + \psi_k\varepsilon_{t-k} + \psi_{k+1}\varepsilon_{t-k-1}) * (\varepsilon_{t+k} + \psi_1\varepsilon_{t+k-1} + \cdots + \psi_{k}\varepsilon_t) + \psi_{k+1}\varepsilon_{t-1})] \\
\end{split}
\]</span>
We know that for <span class="math inline">\(i \not= j, \text{E}(\varepsilon_i\varepsilon_j) = \text{Cov}(\varepsilon_i, \varepsilon_j) = 0\)</span> and <span class="math inline">\(\text{E}(\varepsilon_i^2) = \text{Var}(\varepsilon_i^2) = \sigma^2\)</span>, so the result can be simplfied as :</p>
<p><span class="math display">\[
\begin{split}
\text{Cov}(y_t, y_{t+k}) &amp;= \sigma^2(1 ·\psi_k + \psi_1\psi_{k+1} + \psi_{2}\psi_{k+2} + \cdots) \\
&amp;= \sigma^2\sum_{i=0}^{\infty}{\psi_i\psi_{k+i}}
\end{split}
\]</span>
This derivation matches where we started, that <span class="math inline">\(y_t\)</span> is a <strong>covariance stationary</strong> stochastic process(and by extension variance stationary), since its variance and covariance function is not related to <span class="math inline">\(t\)</span>.</p>
<p>All stationary models discussed in the following chapters can be represented on the basis of the Wold Decomposition <a href="univariate-stationary-processes.html#eq:wold-decomposition">(9.1)</a>. However, this representation is, above all, interesting for theoretical reasons: in practice, applications of models with an infinite number of parameters are hardly useful.</p>
</div>
</div>
<div id="backshift-notation" class="section level2">
<h2><span class="header-section-number">9.2</span> Backshift notation</h2>
<p>The backward shift operator <span class="math inline">\(B\)</span> (or <span class="math inline">\(L\)</span> in some references) is a useful notational device when working with time series lags:</p>
<p><span class="math display">\[
By_t = y_{t-1}
\]</span></p>
<p><span class="math inline">\(B\)</span> can be treated as a number in arithmetic.Two applications of <span class="math inline">\(B\)</span> to <span class="math inline">\(y_t\)</span> shifts the data back two periods:</p>
<p><span class="math display">\[
B(By_t) = B^2y_t
\]</span></p>
<p>The backward shift operator is convenient for describing the process of differencing. A first difference can be written as</p>
<p><span class="math display">\[
y&#39;_t = y_t - y_{t-1} = y_t  - By_{y-1} = (1 - B)y_t
\]</span>
Similarly, the second-order difference would be</p>
<p><span class="math display">\[
\begin{split}
y_t&#39;&#39; &amp;= (y_t - y_{t-1}) - (y_{t-1} - y_{t - 2})  \\ 
      &amp;= By_t - 2By_{t} + B^2y_t \\ 
      &amp;= (1-B)^2y_t
\end{split}
\]</span></p>
<p>In general, a <span class="math inline">\(d\)</span>th-order difference can be written as</p>
<p><span class="math display">\[
(1 - B)^dy_t
\]</span></p>
<p>Backshift notation is particularly useful when combining differences, as the operator can be treated using ordinary algebraic rules. In particular, terms involving <span class="math inline">\(B\)</span> can be multiplied together.</p>
<p>For example, a seasonal difference followed by a first difference can be written as</p>
<p><span class="math display">\[
\begin{split}
(1 - B)(1 - B^m) &amp;= (1 - B^m - B + B^{m + 1})y_t \\
                 &amp;= y_t - y_{t-m} - y_{t-1} + y_{t-m-1}
\end{split}
\]</span></p>
</div>
<div id="autoregressive-models" class="section level2">
<h2><span class="header-section-number">9.3</span> Autoregressive models</h2>
<p>In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself. Thus, an autoregressive model of order <span class="math inline">\(p\)</span> can be written as</p>
<p><span class="math display">\[
y_t =  c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \dots + \phi_p y_{t-p} + \varepsilon_t
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_t\)</span> is white noise. We refer to this model as <strong>AR(p) model</strong>, a autoregressive model with order <span class="math inline">\(p\)</span></p>
<p>We can write out an AR(p) model using the backshift operator:</p>
<p><span class="math display">\[
\begin{aligned}
y_t - \phi_1y_{t-1} - \phi_2y_{t-2} - \cdots - \phi_py_{t-p} &amp;= \varepsilon_t + c\\ 
(1 - \phi_1B - \phi_2B^2 - \cdots - \phi_pB^p)y_t &amp;= \varepsilon_t + c\\
\phi_p(B)y_t &amp;= \varepsilon_t + c
\end{aligned}
\]</span></p>

<div class="todo">
The relationship between AR models and stationarity,
</div>

<p>Whether autoregressive process is stationary is dependent on the values of <span class="math inline">\(\phi_1, \phi_2, \dots\)</span>. To yield a stationary autoregressive process, some constraints on the values of the parameters are required.</p>
<ul>
<li><p>For an AR(1) model: <span class="math inline">\(-1 &lt; \phi_1 &lt; -1\)</span> (Consider the denominator of the sum of a inifite geometric series)</p></li>
<li><p>For an AR(2) model: <span class="math inline">\(-1 &lt; \phi_2 &lt; 1\)</span>, <span class="math inline">\(\phi_1 + \phi_2 &lt; 1, \phi_2 - \phi_1 &lt; 1\)</span>.</p></li>
</ul>
<p>Generally, for a AR(p) model, if we treat <strong>B</strong> as number (or numbers), we can write out the the formula (<span class="math inline">\(\phi_p(B)\)</span> below) as</p>
<p><span class="math display">\[
\begin{aligned}
\phi_p(B)y_t &amp;= \varepsilon_t + c\\
&amp;\Downarrow \\
\phi_p(B) &amp;= 0
\end{aligned}
\]</span>
To be stationary, all complex roots (<span class="math inline">\(B_1\)</span>, <span class="math inline">\(B_2\)</span>, …, <span class="math inline">\(B_p\)</span>) of the characteristic equation <span class="math inline">\(\Phi(B) = 1 - \phi_1B - \phi_2B^2 - \cdots - \phi_pB^p\)</span> must <strong>exceed</strong> 1 in absolute value (actually all of them should lie ouside of the unit circle). We can thus derive conditions on the valuse of <span class="math inline">\(\phi_1, \cdots, \phi_p\)</span>, but it’s quite complicated, and R takes care of these restrictions when estimating a model.</p>
<p>For example, consider this AR(1) model :</p>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= 0.5y_{t-1} + \varepsilon_t \\
y_t - 0.5y_{t-1} &amp;= \varepsilon_t \\
(1 - 0.5B)y_t &amp;= \varepsilon_t \\
&amp;\Downarrow \\
1 - 0.5B &amp;= 0 \\
B &amp;= 2
\end{aligned}
\]</span>
This model is indeed stationary because <span class="math inline">\(|B| &gt; 1\)</span>. From this we can also derive <span class="math inline">\(|\phi_1| &lt; 1\)</span> for AR(1) model.</p>
<div id="stimulating-an-arp-process" class="section level3">
<h3><span class="header-section-number">9.3.1</span> Stimulating an AR(p) process</h3>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="univariate-stationary-processes.html#cb169-1"></a><span class="kw">set.seed</span>(<span class="dv">2020</span>)</span>
<span id="cb169-2"><a href="univariate-stationary-processes.html#cb169-2"></a><span class="co"># specification for AR(1) model with small coef</span></span>
<span id="cb169-3"><a href="univariate-stationary-processes.html#cb169-3"></a>AR_small_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">ar =</span> <span class="fl">0.1</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb169-4"><a href="univariate-stationary-processes.html#cb169-4"></a><span class="co"># specification for AR(1) model with large coef</span></span>
<span id="cb169-5"><a href="univariate-stationary-processes.html#cb169-5"></a>AR_large_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">ar =</span> <span class="fl">0.9</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb169-6"><a href="univariate-stationary-processes.html#cb169-6"></a><span class="co">## simulate AR(1)</span></span>
<span id="cb169-7"><a href="univariate-stationary-processes.html#cb169-7"></a>AR1_small &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> AR_small_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb169-8"><a href="univariate-stationary-processes.html#cb169-8"></a>AR1_large &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> AR_large_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb169-9"><a href="univariate-stationary-processes.html#cb169-9"></a></span>
<span id="cb169-10"><a href="univariate-stationary-processes.html#cb169-10"></a><span class="kw">wrap_plots</span>(</span>
<span id="cb169-11"><a href="univariate-stationary-processes.html#cb169-11"></a>  <span class="kw">autoplot</span>(AR1_small) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.1&quot;</span>))),</span>
<span id="cb169-12"><a href="univariate-stationary-processes.html#cb169-12"></a>  <span class="kw">autoplot</span>(AR1_large) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.9&quot;</span>))),</span>
<span id="cb169-13"><a href="univariate-stationary-processes.html#cb169-13"></a>  <span class="kw">ACF</span>(AR1_small) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.1&quot;</span>))),</span>
<span id="cb169-14"><a href="univariate-stationary-processes.html#cb169-14"></a>  <span class="kw">ACF</span>(AR1_large) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.9&quot;</span>)))</span>
<span id="cb169-15"><a href="univariate-stationary-processes.html#cb169-15"></a>)</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-5-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>It looks like the time series with the smaller AR coefficient is more “choppy” and seems to stay closer to 0 whereas the time series with the larger AR coefficient appears to wander around more. Remember that as the coefficient in an AR(1) model goes to 0, the model approaches a WN sequence, which is stationary in both the mean and variance. As the coefficient goes to 1, however, the model approaches a random walk (<a href="univariate-stationary-processes.html#random-walk">9.3.3</a>), which is not stationary in either the mean or covariance.</p>
<p>Next, let’s generate two AR(1) models that have the same magnitude coeficient, but opposite signs, and compare their behavior.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="univariate-stationary-processes.html#cb170-1"></a><span class="kw">set.seed</span>(<span class="dv">2020</span>)</span>
<span id="cb170-2"><a href="univariate-stationary-processes.html#cb170-2"></a><span class="co"># specification for AR(1) model with positive coef</span></span>
<span id="cb170-3"><a href="univariate-stationary-processes.html#cb170-3"></a>AR_pos_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">ar =</span> <span class="fl">0.5</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb170-4"><a href="univariate-stationary-processes.html#cb170-4"></a><span class="co"># specification for AR(1) model with negative coef</span></span>
<span id="cb170-5"><a href="univariate-stationary-processes.html#cb170-5"></a>AR_neg_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">ar =</span> <span class="fl">-0.5</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb170-6"><a href="univariate-stationary-processes.html#cb170-6"></a><span class="co"># simulate AR(1)</span></span>
<span id="cb170-7"><a href="univariate-stationary-processes.html#cb170-7"></a>AR1_pos &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> AR_pos_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb170-8"><a href="univariate-stationary-processes.html#cb170-8"></a>AR1_neg &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> AR_neg_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb170-9"><a href="univariate-stationary-processes.html#cb170-9"></a></span>
<span id="cb170-10"><a href="univariate-stationary-processes.html#cb170-10"></a><span class="kw">wrap_plots</span>(</span>
<span id="cb170-11"><a href="univariate-stationary-processes.html#cb170-11"></a>  <span class="kw">autoplot</span>(AR1_pos) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.5&quot;</span>))),</span>
<span id="cb170-12"><a href="univariate-stationary-processes.html#cb170-12"></a>  <span class="kw">autoplot</span>(AR1_neg) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = -0.5&quot;</span>))),</span>
<span id="cb170-13"><a href="univariate-stationary-processes.html#cb170-13"></a>  <span class="kw">ACF</span>(AR1_pos) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.5&quot;</span>))),</span>
<span id="cb170-14"><a href="univariate-stationary-processes.html#cb170-14"></a>  <span class="kw">ACF</span>(AR1_neg) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = -0.5&quot;</span>)))</span>
<span id="cb170-15"><a href="univariate-stationary-processes.html#cb170-15"></a>)</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-6-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Now it appears like both time series vary around the mean by about the same amount, but the model with the negative coefficient produces a much more “sawtooth” time series. It turns out that any AR(1) model with <span class="math inline">\(−1 &lt; \phi_1 &lt; 0\)</span> will exhibit the 2-point oscillation we see here.</p>
<p>We can simulate higher order AR(p) models in the same manner, but care must be exercised when choosing a set of coefficients that result in a stationary model or else <code>arima.sim()</code> will fail and report an error. For example, an AR(2) model with both coefficients equal to 0.5 is not stationary, and therefore this function call will not work:</p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="univariate-stationary-processes.html#cb171-1"></a><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">100</span>, <span class="dt">model =</span> <span class="kw">list</span>(<span class="kw">order</span>(<span class="dv">2</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="dt">ar =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>)))</span>
<span id="cb171-2"><a href="univariate-stationary-processes.html#cb171-2"></a><span class="co">#&gt; Error in arima.sim(n = 100, model = list(order(2, 0, 0), ar = c(0.5, 0.5))): &#39;ar&#39; part of model is not stationary</span></span></code></pre></div>
</div>
<div id="decision-of-order-p" class="section level3">
<h3><span class="header-section-number">9.3.2</span> Decision of order p</h3>
<p>How do we decide the order $p $of a AR model? A rule of thumb is to look at <strong>partial autocorrelation coefficients</strong>, PACF. PACF measures the direct effect of a lagged value on its previous value. Suppose we want to measure the effect of <span class="math inline">\(y_{t-2}\)</span> on <span class="math inline">\(y_{t}\)</span>, while <span class="math inline">\(r_2\)</span> could be high, it could also carry the effect of <span class="math inline">\(y_{t-2} \rightarrow y_{t-1} \rightarrow y_t\)</span>, especially when <span class="math inline">\(r_1\)</span> is also high. This is when partial autocorrelation come to resuce, consider a AR(2) process (which means past values earlier than <span class="math inline">\(y_{t-2}\)</span> cannot have an effect on <span class="math inline">\(y_t\)</span>)</p>
<p><span class="math display">\[
y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \varepsilon_t
\]</span></p>
<p>The partial correlation coefficient for <span class="math inline">\(y_{t-k}\)</span> is simply defined as <span class="math inline">\(\phi_k\)</span> in the model, i.e., <span class="math inline">\(\phi_1\)</span> for <span class="math inline">\(y_{t-1}\)</span> and <span class="math inline">\(\phi_2\)</span> for <span class="math inline">\(y_{t-2}\)</span>. Partial correlation coefficient for <span class="math inline">\(k &gt; 2\)</span> is think of as zero. In practice, there are more efficient algorithms for computing <span class="math inline">\(\phi_k\)</span> than fitting all of these autoregressions, but they give the same results.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="univariate-stationary-processes.html#cb172-1"></a>fpp3<span class="op">::</span>aus_airpassengers <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb172-2"><a href="univariate-stationary-processes.html#cb172-2"></a><span class="st">  </span><span class="kw">PACF</span>(<span class="dt">lag_max =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb172-3"><a href="univariate-stationary-processes.html#cb172-3"></a><span class="st">  </span><span class="kw">autoplot</span>()</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-8-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>This tells us only PAC at <span class="math inline">\(\text{lag} = 1\)</span> is significantly different than 0. As such only among <span class="math inline">\(y_{t-1}, y_{t-2}, \dots, y_{t-10}\)</span>, only <span class="math inline">\(y_{t-1}\)</span> has a significant <strong>direct</strong> effect on the response, so a AR(1) model may be appropriate. We can compare this to the ACF plot</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="univariate-stationary-processes.html#cb173-1"></a>fpp3<span class="op">::</span>aus_airpassengers <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb173-2"><a href="univariate-stationary-processes.html#cb173-2"></a><span class="st">  </span><span class="kw">ACF</span>(<span class="dt">lag_max =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb173-3"><a href="univariate-stationary-processes.html#cb173-3"></a><span class="st">  </span><span class="kw">autoplot</span>()</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-9-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Another characteristic of AR(p) is that ACF plots tails slowly.</p>
<div class="sourceCode" id="cb174"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb174-1"><a href="univariate-stationary-processes.html#cb174-1"></a><span class="co"># model with larger phi has longer tail </span></span>
<span id="cb174-2"><a href="univariate-stationary-processes.html#cb174-2"></a><span class="kw">wrap_plots</span>(</span>
<span id="cb174-3"><a href="univariate-stationary-processes.html#cb174-3"></a>  <span class="kw">ACF</span>(AR1_small) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.1&quot;</span>))),</span>
<span id="cb174-4"><a href="univariate-stationary-processes.html#cb174-4"></a>  <span class="kw">ACF</span>(AR1_large) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(phi, <span class="st">&quot; = 0.9&quot;</span>))),</span>
<span id="cb174-5"><a href="univariate-stationary-processes.html#cb174-5"></a>  <span class="dt">nrow =</span> <span class="dv">2</span></span>
<span id="cb174-6"><a href="univariate-stationary-processes.html#cb174-6"></a>)</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-10-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Further illustration: the ACF for an AR(p) process tails off toward zero very slowly, but the PACF goes to zero for lags &gt; <span class="math inline">\(p\)</span>.</p>
<p><img src="images/AR_decision.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="random-walk" class="section level3">
<h3><span class="header-section-number">9.3.3</span> Random walk</h3>
<p>Random walk is a special case of autoregressive process, namely AR(1). A time series is a random walk if</p>
<p><span class="math display">\[
y_t = 
\begin{cases}
\varepsilon_1 &amp;t = 1 \\
y_{t - 1} + \varepsilon_t&amp; t = 2, 3, \dots 
\end{cases}
\]</span>
where <span class="math inline">\(\varepsilon_t\)</span> comes from a white noise process. It can also be expressed in the following form:</p>
<p><span class="math display">\[
y_t = \sum_{i=1}^{t}{\varepsilon_t}
\]</span></p>
<p>According to this, we also have (covariance can be viewed as autocorrelation in this sense)</p>
<p><span class="math display">\[
\text{mean}:  \text{E}(y_t) = 0 \\
\text{variance}: \text{Var}(y_t) = t\sigma^2 \\
\text{ACF}: r_k(t) = \frac{t\sigma^2}{\sqrt{t\sigma^2(t+k)\sigma^2}}
\]</span></p>
<p>Note that a random walk process has changable variance and a ACF not only related to the distance, meaning that it is <strong>not statinary</strong>(since <span class="math inline">\(\phi_1 = 1 \not&lt;|1|\)</span>). In other words, it does not satisfy variance stationarity and covariance stationarity. This non-stationary process is often suitable fro describing economic phenomena.</p>
<p>Random walk can be extended by adding a <strong>drift</strong> <span class="math inline">\(\mu\)</span>, which would be the new expectation of the process. Random walk with a drift (also called a <em>biased random walk</em>) is the process behind a drift model</p>
<p><span class="math display">\[
\begin{split}
y_t &amp;= \mu + y_{t-1} + \varepsilon_t \\ 
    &amp;= \mu + \mu + y_{t-2} + \varepsilon_t + \varepsilon_{t-1}\\ 
    \vdots \\
    &amp;=  t\mu + \sum_{i=1}^{t}\varepsilon_i 
\end{split}
\]</span></p>
<p>With little effort, we can show that for a drift process <span class="math inline">\(\text{E}(y_t) = \mu\)</span>, <span class="math inline">\(\text{Var}(y_t) = t\sigma^2\)</span>, and ACF stays the same, so that a biased random walk is still non-stationary. A random walk with drift can be considered as a curve fluctuating around line <span class="math inline">\(y = \mu t\)</span> with increasing volatility as <span class="math inline">\(t\)</span> increases.</p>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="univariate-stationary-processes.html#cb175-1"></a><span class="co"># stimulate a random walk with drift</span></span>
<span id="cb175-2"><a href="univariate-stationary-processes.html#cb175-2"></a>RW &lt;-<span class="st"> </span><span class="cf">function</span>(N, x0, mu, variance) {</span>
<span id="cb175-3"><a href="univariate-stationary-processes.html#cb175-3"></a>  z&lt;-<span class="kw">cumsum</span>(<span class="kw">rnorm</span>(<span class="dt">n =</span> N, </span>
<span id="cb175-4"><a href="univariate-stationary-processes.html#cb175-4"></a>                  <span class="dt">mean =</span> <span class="dv">0</span>, </span>
<span id="cb175-5"><a href="univariate-stationary-processes.html#cb175-5"></a>                  <span class="dt">sd=</span><span class="kw">sqrt</span>(variance)))</span>
<span id="cb175-6"><a href="univariate-stationary-processes.html#cb175-6"></a>  t &lt;-<span class="st"> </span><span class="dv">1</span><span class="op">:</span>N</span>
<span id="cb175-7"><a href="univariate-stationary-processes.html#cb175-7"></a>  x &lt;-<span class="st"> </span>x0 <span class="op">+</span><span class="st"> </span>t<span class="op">*</span>mu<span class="op">+</span>z</span>
<span id="cb175-8"><a href="univariate-stationary-processes.html#cb175-8"></a>  </span>
<span id="cb175-9"><a href="univariate-stationary-processes.html#cb175-9"></a>  x</span>
<span id="cb175-10"><a href="univariate-stationary-processes.html#cb175-10"></a>  }</span>
<span id="cb175-11"><a href="univariate-stationary-processes.html#cb175-11"></a><span class="co"># mu is the drift</span></span>
<span id="cb175-12"><a href="univariate-stationary-processes.html#cb175-12"></a><span class="kw">set.seed</span>(<span class="dv">2020</span>)</span>
<span id="cb175-13"><a href="univariate-stationary-processes.html#cb175-13"></a>rw &lt;-<span class="st"> </span><span class="kw">RW</span>(<span class="dv">500</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">1</span>) <span class="op">%&gt;%</span></span>
<span id="cb175-14"><a href="univariate-stationary-processes.html#cb175-14"></a><span class="st">  </span><span class="kw">enframe</span>() <span class="op">%&gt;%</span><span class="st"> </span></span>
<span id="cb175-15"><a href="univariate-stationary-processes.html#cb175-15"></a><span class="st">  </span><span class="kw">as_tsibble</span>(<span class="dt">index =</span> name)</span>
<span id="cb175-16"><a href="univariate-stationary-processes.html#cb175-16"></a></span>
<span id="cb175-17"><a href="univariate-stationary-processes.html#cb175-17"></a>rw <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">geom_abline</span>(<span class="dt">slope =</span> <span class="dv">1</span>, <span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">color =</span> <span class="st">&quot;red&quot;</span>)</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb176"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb176-1"><a href="univariate-stationary-processes.html#cb176-1"></a>rw <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">ACF</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-12-2.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="moving-average-models" class="section level2">
<h2><span class="header-section-number">9.4</span> Moving average models</h2>
<p>Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></p>
<p><span class="math display">\[
y_t = c + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \dots + \theta_q\varepsilon_{t-q} + \varepsilon_{t}
\]</span></p>
<p>where <span class="math inline">\(\varepsilon_t\)</span> is white noise. We refer to this as a MA(q) model, a moving average model of order <span class="math inline">\(q\)</span>. Of course, we do not observe the values of <span class="math inline">\(\varepsilon_t\)</span>, so it is not really a regression in the usual sense.</p>
<p>Notice that each value of <span class="math inline">\(y_t\)</span> can be thought of as a weighted moving average of the past few forecast errors. However, moving average models should not be confused with the moving average smoothing we discussed in Section <a href="time-series-decomposition.html#moving-averages">3.2</a>. A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values.</p>
<p>It is easy to show <strong>MA(q) process is always stationary</strong>. A MA(1) process has the following properties</p>
<p><span class="math display">\[
\begin{aligned}
\text{Mean stationarity}: \text{E}(y_t) &amp;= c \\
\text{Variance stationarity}:\text{Var}(y_t) &amp;= (1 + \theta_1^2)\sigma^2 \\
\text{Covariance stationarity}:\text{ACF} = \gamma(k) &amp;= 
\begin{cases}
\frac{\theta_1}{1 + \theta_1^2} &amp; k = 1\\  
0 &amp; \text{otherwise}
\end{cases}
\end{aligned}
\]</span></p>
<p>Proof for ACF :
<span class="math display">\[
\begin{split}
\frac{\gamma(1)}{\gamma(0)} &amp;= \frac{\text{Covariance for lag} 1}{\text{variance for lag}1} \\
           &amp;= \frac{E[(y_t - E(y_t))(y_{t-1} - E(y_{t-1}))]}{(1 + \theta_1^2)\sigma^2} \\ 
           &amp;= \frac{E[(\varepsilon_{t} + \theta_1\varepsilon_{t-1})(\varepsilon_{t-1} + \theta_1\varepsilon_{t-2})]}{(1 + \theta_1^2)\sigma^2} \\
           &amp;= \frac{E(\varepsilon_t \varepsilon_{t-1} + \theta_1\varepsilon_t\varepsilon_{t-2} + \theta_1 \varepsilon_{t-1}^2 + \theta_1^2\varepsilon_{t-1}\varepsilon_{t-2})}{(1 + \theta_1^2)\sigma^2} \\
           &amp;= \frac{\theta_1E(\varepsilon_{t-1}^2)}{(1 + \theta_1^2)\sigma^2} \\
           &amp;= \frac{\theta_1\sigma^2}{(1 + \theta_1^2)\sigma^2} \\
           &amp;= \frac{\theta_1}{1 + \theta_1^2}
\end{split}
\]</span>
For <span class="math inline">\(\text{lag} &gt; 1\)</span>, there will be no square term like <span class="math inline">\(\varepsilon_{t-k}^2\)</span>, but only cross terms like <span class="math inline">\(\varepsilon_t\varepsilon_{t-k}\)</span>，whose expectation would be zero by defination, so that ACF will be zero, which leads to covariance stationarity.</p>
<div id="simulating-an-maq-process" class="section level3">
<h3><span class="header-section-number">9.4.1</span> Simulating an MA(q) process</h3>
<p>We can simulate MA(q) processes just as we did for AR(p) processes using <code>arima.sim()</code>. Here are 3 different ones with contrasting <span class="math inline">\(\theta\)</span>:</p>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="univariate-stationary-processes.html#cb177-1"></a><span class="kw">set.seed</span>(<span class="dv">2020</span>)</span>
<span id="cb177-2"><a href="univariate-stationary-processes.html#cb177-2"></a><span class="co">## list description for MA(1) model with small coef</span></span>
<span id="cb177-3"><a href="univariate-stationary-processes.html#cb177-3"></a>MA_sm_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">ma =</span> <span class="fl">0.2</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb177-4"><a href="univariate-stationary-processes.html#cb177-4"></a><span class="co">## list description for MA(1) model with large coef</span></span>
<span id="cb177-5"><a href="univariate-stationary-processes.html#cb177-5"></a>MA_lg_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">ma =</span> <span class="fl">0.8</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb177-6"><a href="univariate-stationary-processes.html#cb177-6"></a><span class="co">## list description for MA(1) model with large coef</span></span>
<span id="cb177-7"><a href="univariate-stationary-processes.html#cb177-7"></a>MA_neg_spec &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">order =</span> <span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">ma =</span> <span class="fl">-0.5</span>, <span class="dt">sd =</span> <span class="fl">0.1</span>)</span>
<span id="cb177-8"><a href="univariate-stationary-processes.html#cb177-8"></a><span class="co">## simulate MA(1)</span></span>
<span id="cb177-9"><a href="univariate-stationary-processes.html#cb177-9"></a>MA1_sm &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> MA_sm_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb177-10"><a href="univariate-stationary-processes.html#cb177-10"></a>MA1_lg &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> MA_lg_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb177-11"><a href="univariate-stationary-processes.html#cb177-11"></a>MA1_neg &lt;-<span class="st"> </span><span class="kw">arima.sim</span>(<span class="dt">n =</span> <span class="dv">50</span>, <span class="dt">model =</span> MA_neg_spec) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as_tsibble</span>()</span>
<span id="cb177-12"><a href="univariate-stationary-processes.html#cb177-12"></a></span>
<span id="cb177-13"><a href="univariate-stationary-processes.html#cb177-13"></a><span class="kw">wrap_plots</span>(</span>
<span id="cb177-14"><a href="univariate-stationary-processes.html#cb177-14"></a>  <span class="kw">autoplot</span>(MA1_sm) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(theta, <span class="st">&quot; = 0.2&quot;</span>))),</span>
<span id="cb177-15"><a href="univariate-stationary-processes.html#cb177-15"></a>  <span class="kw">autoplot</span>(MA1_lg) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(theta, <span class="st">&quot; = 0.8&quot;</span>))),</span>
<span id="cb177-16"><a href="univariate-stationary-processes.html#cb177-16"></a>  <span class="kw">autoplot</span>(MA1_neg) <span class="op">+</span><span class="st"> </span><span class="kw">ggtitle</span>(<span class="kw">expression</span>(<span class="kw">paste</span>(theta, <span class="st">&quot; = -0.5&quot;</span>)))</span>
<span id="cb177-17"><a href="univariate-stationary-processes.html#cb177-17"></a>)</span></code></pre></div>
<p><img src="stationary_files/figure-html/unnamed-chunk-13-1.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="decision-of-order-q" class="section level3">
<h3><span class="header-section-number">9.4.2</span> Decision of order q</h3>
<p>The derivation of the ACF function has shed light on how we would decide the order <span class="math inline">\(q\)</span>. For a MA(2) process, ACF would be 0 for <span class="math inline">\(k = 3, 4, \dots\)</span>. In general, for a MA(q) process, ACF will be nonzero for <span class="math inline">\(k = 1, 2, \dots,q\)</span> and zero after <span class="math inline">\(q\)</span>.</p>
<p>Let’s prove this, a MA(q) model can be written as:
<span class="math display">\[
y_{t-k} = c + \theta_1\varepsilon_{t-k -1} + \theta_2\varepsilon_{t- k - 2} + \dots + \theta_q\varepsilon_{t-k -q} + \varepsilon_{t-q}
\]</span>
To see at which point ACF will be zero, consider the covariance (if covariance between <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-k}\)</span> is zero, then <span class="math inline">\(r_k\)</span> is zero)
<span class="math display">\[
\text{Cov}(y_t, y_{t-k}) = \text{E}(y_ty_{t-k}) - \text{E}(y_t)\text{E}(y_{t-k})
\]</span></p>
<p>We know that <span class="math inline">\(\text{E}(y_t)\text{E}(y_{t-k}) = c^2\)</span>. So the problem is, what is included in <span class="math inline">\(E(y_ty_{t-k})\)</span>, and when will it be <span class="math inline">\(c^2\)</span>, so that the covariance would be zero?</p>
<p>Follow MA(q), <span class="math inline">\(y_t\)</span> includes <span class="math inline">\((c, \varepsilon_t, \varepsilon_{t-1}, \varepsilon_{t-2}, \dots, \varepsilon_{t-q})\)</span>, and <span class="math inline">\(y_{t-k}\)</span> includes <span class="math inline">\((c, \varepsilon_{t-k}, \varepsilon_{t-k -1}, \varepsilon_{t-k -2}, \dots, \varepsilon_{t - k -q})\)</span>. When they are multiplied together, there will be</p>
<p><span class="math display">\[
\begin{aligned}
\text{constant} &amp;: c^2 \rightarrow E(c^2) = c^2 \\
\text{cross terms} &amp;: \varepsilon_i\varepsilon_j, i \not= j \rightarrow E(\varepsilon_i\varepsilon_j) = 0 \\
\text{square terms} &amp;: \varepsilon_i^2 \rightarrow E(\varepsilon_i^2) = \text{Var}(\varepsilon_i) + \text{E}(\varepsilon_i)^2 = \sigma^2 &gt; 0
\end{aligned}
\]</span>
Now everything is clear, ACF will be zero, if and only if there is no <strong>square term</strong>. That is to say, there is no same error term when we decompose <span class="math inline">\(y_t\)</span> and <span class="math inline">\(y_{t-k}\)</span>. This means the earliest error in <span class="math inline">\((\varepsilon_t, \varepsilon_{t-1}, \varepsilon_{t-2}, \dots, \varepsilon_{t-q})\)</span> is still earlier than the latest error term in of <span class="math inline">\((\varepsilon_{t-k}, \varepsilon_{t-k -1}, \varepsilon_{t-k -2}, \dots, \varepsilon_{t - k -q})\)</span></p>
<p><span class="math display">\[
\begin{aligned}
t - q &amp;&gt; t - k \\
k &amp;&gt; q
\end{aligned}
\]</span></p>
<p>Now that we have proved that when <span class="math inline">\(k &gt; q\)</span>, ACF would be zero and otherwise non-zero. So, a sample ACF with significant autocorrelations at lag 1 to lag q, but non-significant autocorrelations for after q indicates a possible MA(q) model(i.e., an ACF plot that cuts off at lag q).</p>
<p>Another characteristic of a MA(q) process is that their PACF tails off toward zero very slowly, in contrast to AR(p) whose ACF plot has a long tail.</p>
<p><img src="images/MA_decision.png" width="100%" style="display: block; margin: auto;" /></p>
<p><img src="images/AR_MA_decision.png" width="100%" style="display: block; margin: auto;" /></p>
</div>
<div id="koyck-transformation-and-invertibility" class="section level3">
<h3><span class="header-section-number">9.4.3</span> Koyck transformation and Invertibility</h3>
<p><em>Koyck transformation</em> is one that converts a AR(p) model to MA(<span class="math inline">\(\infty\)</span>) model . Take an AR(1) model for example, which can be written as</p>
<p><span class="math display">\[
\begin{aligned}
(1 - \phi_1B)y_t &amp;= \varepsilon_t + c \\
y_t &amp;= \frac{\varepsilon_t + c}{1 - \phi_1B} 
\end{aligned}
\]</span></p>
<p>Recall that the infinite sum of a geometric series is</p>
<p><span class="math display">\[
\begin{aligned}
s &amp;= a + ar + ar^2 + \cdots \\
  &amp;= \frac{a}{1 - r} \quad (|r| &lt; 1)
\end{aligned}
\]</span></p>
<p>It follows that we can rewritten our formula as (backshift operator on a constant is still that constant, so <span class="math inline">\(|\phi_1| &lt; 1\)</span> ):</p>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= (\varepsilon_t + c) + (\varepsilon_t + c)\phi_1B + (\varepsilon_t + c)(\phi_1B)^2 + \cdots \\
    &amp;= \varepsilon_t + (c + \phi_1c + \phi_1^2c + \dots) + (\phi_1\varepsilon_{t-1} + \phi_1^2\varepsilon_{t-2} + \cdots) 
\end{aligned}
\]</span></p>
<p>For a AR(1) model, we have <span class="math inline">\(|\phi_1| &lt; 1\)</span>. so that <span class="math inline">\((c + \phi_1c + \phi_1^2c + c\dots)\)</span> will converge to a constant. And what we derived is exactly a MA(<span class="math inline">\(\infty\)</span>) model.</p>
<p>This transformation is also a special case of the Wold Decomposition. Since when AR(1) when <span class="math inline">\(|\phi_1| &lt; 1\)</span> is covariance stationary, and <span class="math inline">\(\text{E}(y_t) = c + \phi_1c + \phi_1^2c + \dots\)</span>. If we substract the mean from the RHS then the equation is Equation <a href="univariate-stationary-processes.html#eq:wold-decomposition">(9.1)</a></p>
<p><em>Invertibility</em> is like the opposite of Koyck transformation. It describes the fact that it is possible to write any stationary MA(<span class="math inline">\(\infty\)</span>) model as a AR(p) model. Again, we demonstrate this with an MA(1) model (here I put a negative sign on <span class="math inline">\(\theta_1\)</span> just for convenience):</p>
<p><span class="math display">\[
\begin{aligned}
y_t &amp;= c -  \theta_1\varepsilon_{t-1} + \varepsilon_t \\
y_t &amp;= (1 - \theta_1B)\varepsilon_t + c\\
\varepsilon_t &amp;= \frac{y_t - c}{1 - \theta_1B}
\end{aligned}
\]</span>
The rest is the same, <span class="math inline">\(\frac{y_t - c}{1 - \theta_1B}\)</span> can be considered as the infinite sum of a geometric series, when <span class="math inline">\(|\theta_1| &lt; 1\)</span>. And the final equation is</p>
<p><span class="math display">\[
y_t = (c + \theta_1c + \theta_1^2c + \cdots) + (\theta_1y_{t-1} + \theta_1^2y_{t-2} + \cdots) + \varepsilon_t
\]</span>
With <span class="math inline">\(|\theta_1| &lt; 1\)</span>, <span class="math inline">\((c + \theta_1c + \theta_1^2c + \cdots)\)</span> will converge to a constant and the equation qualifies for a AR(<span class="math inline">\(\infty\)</span>) model. Thus, when <span class="math inline">\(|\theta_1| &lt; 1\)</span> the MA model is <strong>invertible</strong>. (From another perspective, if <span class="math inline">\(|\theta_1| &gt; 1\)</span> <span class="math inline">\(\phi_1, ... ,\phi_p\)</span> in MA(<span class="math inline">\(\infty\)</span>) model will not met stationary conditions).</p>
<p>Although we no longer need to place constraints on MA models like we did on AR models since it is always stationary, we do hope that MA models are invertible, because only when an MA(q) process is invertible can it be <strong>uniquely identified</strong>. Consider, for example, the following case in which we know that for a MA(1) model <span class="math inline">\(\gamma(1) = r_1\)</span>, and the way we would compute <span class="math inline">\(\theta_1\)</span>:</p>
<p><span class="math display">\[
\frac{\theta}{1 + \theta^2} = r_1 \\
(1 + \theta^2)r_1 - \theta = 0 \\
\theta^2 + \frac{1}{r_1}\theta + 1 = 0
\]</span></p>
<p>The equation has the following two roots:</p>
<p><span class="math display">\[
\theta_{1,2} = -\frac{1}{2r}(1 \pm \sqrt{1 - 4r_1^2}) 
\]</span></p>
<p>Because <span class="math inline">\(|r_1| = |\frac{\theta_1}{1 + \theta_1^2}| &lt; 1/2\)</span> the quadratic equation always results in real roots. They also have the property that <span class="math inline">\(\theta_1\theta_2 = 1\)</span> (Vieta theorem). This gives us the possibility to model the same autocorrelation structure with two different parameters, where one is the inverse of the other.</p>
<p>n order to get a unique parameterisation, we require a further property
of the MA(1) process. We ask under which conditions the MA(1) process can have an autoregressive representation. And we already know that this is true when <span class="math inline">\(|\theta| &lt; 1\)</span>, and this condition also helps us to choose between <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2\)</span>, so that unique model is identified.</p>
<p>For higher order models, given <span class="math inline">\(\gamma(1), \gamma(2), \dots, \gamma(q)\)</span> (we already proved that when k &gt; q <span class="math inline">\(\gamma(k) = 0\)</span>), we would still have multiple solution set <span class="math inline">\(\theta_1, \theta_2, \dots, \theta_q\)</span>. To get a unique parameterisation, the invertibility condition is again required, i.e. it must be possible to represent the MA(q) process as a stationary AR(p) process. The condition for order q, similar to AR(p), is that all roots of</p>
<p><span class="math display">\[
1 + \theta_1B + \theta_2B^2 + \dots + \theta_qB^q = 0 
\]</span></p>
<p>are larger than one in absolute value.</p>
</div>
</div>
<div id="arma-models" class="section level2">
<h2><span class="header-section-number">9.5</span> ARMA models</h2>
<p>In the following, we introduce processes which contain both an autoregressive (AR) term of finite order p and a moving average (MA) term of finite order q. Hence, these mixed processes are denoted as ARMA(p, q) processes. They enable us to describe processes in which neither the autocorrelation nor the partial autocorrelation function breaks off after a finite
number of lags. Again, we start with the simplest case, the ARMA(1, 1) process.</p>
<p>An ARMA(1, 1) process can be written as follows (Note the negative sign before <span class="math inline">\(\theta_1\)</span>)</p>
<p><span class="math display">\[
y_t = c + \phi_1y_{t-1} - \theta_1\varepsilon_{t-1}  + \varepsilon_t
\]</span>
using the backshift notation</p>
<p><span class="math display">\[
(1 - \phi_1B)y_t = c + (1 - \theta_1B)\varepsilon_t
\]</span>
where <span class="math inline">\(\varepsilon_t\)</span> comes from pure random process. To get the Wold representation of an
ARMA(1, 1) process, we solve the equation below for <span class="math inline">\(y_t\)</span></p>
<p><span class="math display" id="eq:arma11">\[\begin{equation}
\tag{9.2}
y_t = \frac{c}{1 - \phi_1B} + \frac{1 - \theta_1B}{1 - \phi_1B}\varepsilon_t
\end{equation}\]</span></p>
<p>It is obvious that <span class="math inline">\(\phi_1 \not= \theta_1\)</span> must hold, because otherwise <span class="math inline">\(y_t\)</span> would be a pure
random process fluctuating around the mean <span class="math inline">\(\mu = c/(1 – \phi_1B)\)</span>.</p>
<p>Recall the Wold Decomposition in Equation <a href="univariate-stationary-processes.html#eq:wold-decomposition">(9.1)</a>. The <span class="math inline">\(\phi_i,i = 0, 1, 2, ...\)</span>, can be determined as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{1 - \theta_1B}{1 - \phi_1B} &amp;= \psi_0 + \psi_1B + \psi_2B^2 + \psi_3B^2 + \cdots \\
1 - \theta_1B &amp;= (1 - \phi_1B)( \psi_0 + \psi_1B + \psi_2B^2 + \psi_3B^2 + \cdots) \\
1 - \theta_1B &amp;= \psi_0 + \psi_1B + \psi_2B^2 + \psi_3B^2 +\cdots - \phi_1\psi_0B -  \phi_1\psi_1B^2 -  \phi_1\psi_2B^3 - \cdots
\end{aligned}
\]</span></p>
<p>Comparing the coefficients of the two lag polynomials we get</p>
<p><span class="math display">\[
\begin{aligned}
B^0&amp;:  \psi_0 = 1 \\
B^1&amp;:  \psi_1 - \phi_1\psi_0 = \theta_1 \Rightarrow \psi_1 = \phi_1 - \theta_1 \\
B^2&amp;:  \psi_2 - \phi_1\psi_1 = 0 \Rightarrow \psi_2 = \phi_1(\phi_1 - \theta_1) \\
B^3&amp;:  \psi_3 - \phi_1\psi_2= 0 \Rightarrow \psi_3 = \phi_1^2(\phi_1 - \theta_1) \\
&amp; \vdots \\
B^i&amp;: \psi_i - \phi_1\psi_{i-1} = 0 \Rightarrow \psi_i = \phi_1^{i-1}(\phi_1 - \theta_1)^2
\end{aligned}
\]</span></p>
<p>Note that the Wold Decomposition also requires <span class="math inline">\(\sum_{i-0}^{\infty}\psi_i^2 &lt; \infty\)</span> (this is in the mean function), and this can be only satisfied when <span class="math inline">\(|\phi_1| &lt; 1\)</span>. This corresponds to the stability condition of the AR term.</p>
<p><strong>Thus, the ARMA(1, 1) process is stationary if, with stochastic initial conditions, it has a stable AR(1) term.</strong> The Wold representation is</p>
<p><span class="math display">\[
y_t = \frac{c}{1 - \phi_1} + \varepsilon_t + (\phi_1 - \theta_1)\varepsilon_{t-1} + \phi_1(\phi_1 - \theta_1)\varepsilon_{t-2} + \phi_1^2(\phi_1 - \theta_1)\varepsilon_{t-3}
\]</span></p>
<p>Thus, the ARMA(1, 1) process can be written as an MA(<span class="math inline">\(\infty\)</span>) process.</p>
<p>To invert the MA(1) part, <span class="math inline">\(|\theta_1| &lt; 1\)</span> must hold. From Equation <a href="univariate-stationary-processes.html#eq:arma11">(9.2)</a> we know:</p>
<p><span class="math display">\[
\varepsilon_t = \frac{-c}{1 - \theta_1} + \frac{1 - \phi_1B}{1 - \theta_1B}y_t
\]</span></p>
<p>If <span class="math inline">\(1/(1 – \theta_1B)\)</span> is developed into a geometric series we get</p>
<p><span class="math display">\[
\begin{aligned}
\varepsilon_t &amp;= \frac{-c}{1 + \theta_1} + (1 - \phi_1B)(1 + \theta_1B + \theta_1^2B^2 + \cdots)y_t \\
&amp;= \frac{-c}{1 + \theta_1} + y_t + (\theta_1 - \phi_1)y_{t-1} + \theta_1(\theta_1 - \phi_1)y_{t-2} + \theta_1^2(\theta_1 - \phi_1)y_{t-3} + \cdots
\end{aligned}
\]</span>
This proves to be an AR(<span class="math inline">\(\infty\)</span>) representation. It shows that the combination of an AR(1) and an MA(1) term leads to a process with both MA(<span class="math inline">\(\infty\)</span>) and AR(<span class="math inline">\(\infty\)</span>) representation if the AR term is stable and the MA term invertible.</p>
<p>More generally, <strong>ARMA(p, q) is staionary and invertible if the AR term is stationary and the MA term invertible</strong> (all complex roots of <span class="math inline">\(\Phi(B)\)</span> and that of <span class="math inline">\(\Theta(B)\)</span> exceeds 1 in absolute value, Section @ref(plotting-the-characteristic-roots introduces <code>gg_arma()</code> to visualized characteristic roots)). It can either be represented as an AR(<span class="math inline">\(\infty\)</span>) or as an MA(<span class="math inline">\(\infty\)</span>) process. Thus, neither its autocorrelation nor its partial autocorrelation function breaks off. In short, it is possible to generate stationary stochastic processes with infinite AR and MA orders by using only a finite number of parameters.</p>
<p>It can be proved that an stationary ARMA(1, 1) has the following properties:</p>
<p><span class="math display">\[
\begin{aligned}
\text{E}(y_t) &amp;= \frac{c}{1 - \phi_1} \\ 
\text{Var}(y_t) = \gamma(0) &amp;= \frac{1 + \theta_1^2 - 2\phi_1\theta_1}{1 - \alpha^2}\sigma^2 \\
\text{Cov}(y_t, y_{t+k}) = \gamma(k) &amp;= \phi_1\gamma(k-1)
\end{aligned}
\]</span></p>
<p>We also know that <span class="math inline">\(r_1\)</span>, (the initial value of <span class="math inline">\(r_k\)</span>)</p>
<p><span class="math display">\[
r_1 = \frac{(\phi_1 - \theta_1)(1 - \phi_1\theta_1)}{1 + \theta_1^2 - 2\phi_1\theta_1}
\]</span></p>
<p>If the process is stable and invertible, i.e. for <span class="math inline">\(|\phi_1| &lt; 1\)</span> and <span class="math inline">\(|\theta_1| &lt; 1\)</span>, the sign of <span class="math inline">\(r_k\)</span> is determined by the sign of <span class="math inline">\((\phi_1 - \theta_1)\)</span> because of<span class="math inline">\((1 + \theta_1^2 -2\phi_1 \theta_1) = (\theta_1 - \phi_1)^2 + 1 - \phi_1^2 &gt; 0\)</span> and <span class="math inline">\((1 – \phi_1\theta_1) &gt; 0\)</span>. Moreover, it follows from <span class="math inline">\(\gamma(k)\)</span> that the autocorrelation function – as in the AR(1) process – is monotonic for <span class="math inline">\(\phi_1 &gt; 0\)</span> and oscillating for <span class="math inline">\(\phi_1 &lt; 0\)</span>. Due to <span class="math inline">\(|\phi_1| &lt; 1\)</span> with <span class="math inline">\(k\)</span> increasing, the autocorrelation function also decreases in absolute value.</p>
<p>Thus, the following typical autocorrelation structures are possible:</p>
<ul>
<li><p><span class="math inline">\(\phi_1 &gt; 0\)</span> and <span class="math inline">\(\phi_1 &gt; \theta_1\)</span>: The autocorrelation function is always positive.</p></li>
<li><p><span class="math inline">\(\phi_1 &lt; 0\)</span> and <span class="math inline">\(\phi_1 &lt; \theta_1\)</span>: The autocorrelation function oscillates; the initial
condition <span class="math inline">\(r_1\)</span> is negative.</p></li>
<li><p><span class="math inline">\(\phi_1 &gt; 0\)</span> and <span class="math inline">\(\phi_1 &lt; \theta_1\)</span>: The autocorrelation function is negative from <span class="math inline">\(r_1\)</span> onwards.</p></li>
<li><p><span class="math inline">\(\phi_1 &lt; 0\)</span> and <span class="math inline">\(\phi_1 &gt; \theta_1\)</span>: The autocorrelation function oscillates; the initial
condition <span class="math inline">\(r_1\)</span> is positive.</p></li>
</ul>
<p><img src="images/arma11.png" width="100%" style="display: block; margin: auto;" /></p>
<div id="three-representations-of-an-arma-model" class="section level3">
<h3><span class="header-section-number">9.5.1</span> Three representations of an ARMA model</h3>
<p>This is a supplementary section elaborating on the details among the relationship between ARMA, AR, and MA models.</p>
<p>For a stationary ARMA(p, q) model:</p>
<p><span class="math display">\[
(1 - \phi_1 B - \cdots - \phi_p B^p)y_t = c + (1 + \theta_1 B + \cdots +\theta_q B^q)\varepsilon_t
\]</span></p>
<p>For the AR and MA representations, we use long division of two polynomials. Given two polynomials
<span class="math inline">\(\Phi(B) = 1 - \sum_{i=1}^{p}\phi_iB^i\)</span> and <span class="math inline">\(\Theta(B) = 1 - \sum_{i=1}^{q}\theta_iB^i\)</span> we can obtain, by long division, that the following results take form of</p>
<p><span class="math display">\[
\frac{\Theta(B)}{\Phi(B)} = 1 + \psi_1 B + \psi_2 B^2 + \cdots  \equiv \Psi(B) \tag{1}
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\frac{\Phi(B)}{\Theta(B)}= 1 - \pi_1 B - \pi_2 B ^2 - \cdots \equiv \Pi(B) \tag{2}
\]</span></p>
<div id="ar-representation" class="section level4">
<h4><span class="header-section-number">9.5.1.1</span> AR representation</h4>
<p>From Eq (2), the ARMA(p, q) model can be written as</p>
<p><span class="math display">\[
\frac{\Phi(B)}{\Theta(B)}y_t = c + \varepsilon_t
\]</span>
So</p>
<p><span class="math display">\[
y_t = c + \pi_1y_{t-1} + \pi_2y_{t-2} + \pi_3y_{t-3} + \cdots + \varepsilon_t 
\]</span></p>
</div>
<div id="ma-representation" class="section level4">
<h4><span class="header-section-number">9.5.1.2</span> MA representation</h4>
<p>From Eq(1), the ARMA(p, q) model can be written as</p>
<p><span class="math display">\[
y_t = \frac{c}{\Phi(B)} + \frac{\Theta(B)}{\Phi(B)}\varepsilon_t
\]</span>
So</p>
<p><span class="math display">\[
y_t = \frac{c}{\Phi(B)} + \psi_1\varepsilon_{t-1} + \psi_2\varepsilon_{t-2} + \cdots  + \varepsilon_{t}
\]</span></p>

</div>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="2">
<li id="fn2"><p>Many textbooks and software programs define the model with negative signs before the <span class="math inline">\(\theta\)</span> terms (R uses positive signs). This doesn’t change the general theoretical properties of the model, although it does flip the algebraic signs of estimated coefficient values and (unsquared) <span class="math inline">\(\theta\)</span> terms in formulas for ACFs and variances<a href="univariate-stationary-processes.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="exponential-smoothing.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="arima-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "serif",
"size": 2
},
"edit": {
"link": "https://github.com/enixam/fpp/edit/master/stationary.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
