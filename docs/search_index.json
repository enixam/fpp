[
["index.html", "Notes for “Forecasting: Principles and Practice, 3rd edition” Preface", " Notes for “Forecasting: Principles and Practice, 3rd edition” Qiushi Yan 2020-09-29 Preface This project contains my learning notes and code for Forecasting: Principles and Practice, 3rd edition. My solutions to its exercises can be found at https://qiushi.rbind.io/fpp-exercises Other references include: Applied Time Series Analysis for Fisheries and Environmental Sciences Kirchgässner, G., Wolters, J., &amp; Hassler, U. (2012). Introduction to modern time series analysis. Springer Science &amp; Business Media. a video tutorial on time series concepts presented in this youtube watchlist by ritvikmath also a video tutorial on econometrics presented by Ben Lambert an online course offered by the Department of Statistics at PennState My contributions include More supplementary mathematical proofs: they are not hard assuming introductory level statistics knowledge, but skimmed in the book An additional chapter separating basic concepts of univariate processes from ARIMA models, so that Chapter 10 will not be too verbose Up-to-date tidyverse code: for example gather() use cases are changed into pivot_longer() I will continue to add new topics as I learn new things related to time series modelling. tidyverse is assumed to be loaded before each chapter. library(tidyverse) "],
["getting-started.html", "Chapter 1 Getting started", " Chapter 1 Getting started What can be forecast? The predictability of an event or a quantity depends on several factors including: how well we understand the factors that contribute to it; how much data is available; whether the forecasts can affect the thing we are trying to forecast. It is important to know when something could be forcast with precision: Good forecasts capture the genuine patterns and relationships which exist in the historical data, but do not replicate past events that will not occur again. Usual assumption of forecasting: What is normally assumed is that the way in which the environment is changing will continue into the future Two forecasting methods: quantitative forecasting: If there are no data available, or if the data available are not relevant to the forecasts qualitative forecasting numerical information about the past is available it is reasonable to assume that some aspects of the past patterns will continue into the future There is a wide range of quantitative forecasting methods, often developed within specific disciplines for specific purposes. Each method has its own properties, accuracies, and costs that must be considered when choosing a specific method. Most quantitative prediction problems use either time series data (collected at regular intervals over time) or cross-sectional data (collected at a single point in time). Here we only focus on time series data. Anything that is observed sequentially over time is a time series. In this book, we will only consider time series that are observed at regular intervals of time (e.g., hourly, daily, weekly, monthly, quarterly, annually). When forecasting time series data, the aim is to estimate how the sequence of observations will continue into the future. 3 types of models: explanatory model (it helps explain what causes the variation in outcome.) \\[ ED = f(current\\;temperature,\\, strength\\;of\\;economy,\\;population,time\\;of\\;day, \\;error) \\] time series model (based on past values of a variable, but not on external variables which may affect the system): \\[ ED_{t+1} = f(ED_t,\\,ED_{t−1},\\,ED_{t−2},\\,ED_{t−3},\\,...,\\, error) \\] dynamic regression models, panel data models, longitudinal models, transfer function models, and linear system models (assuming that \\(f\\) is linear) \\[ ED_{t+1} = f(ED_t,\\,current \\; temperature, time \\; of \\; day,\\,day\\;of\\;week,\\,error) \\] An explanatory model is useful because it incorporates information about other variables, rather than only historical values of the variable to be forecast. However, there are several reasons a forecaster might select a time series model rather than an explanatory or mixed model. First, the system may not be understood, and even if it was understood it may be extremely difficult to measure the relationships that are assumed to govern its behaviour. Second, it is necessary to know or forecast the future values of the various predictors in order to be able to forecast the variable of interest, and this may be too difficult. Third, the main concern may be only to predict what will happen, not to know why it happens. Finally, the time series model may give more accurate forecasts than an explanatory or mixed model. The model to be used in forecasting depends on the resources and data available, the accuracy of the competing models, and the way in which the forecasting model is to be used. 5 steps in a forecasting task: 1. problem definition 2. gathering information 3. exploratory data analysis 4. chossing and fitting models 5. using and evaluating the model When we obtain a forecast, we are estimating the middle of the range of possible values the random variable could take. Often, a forecast is accompanied by a prediction interval giving a range of values the random variable could take with relatively high probability. For example, a 95% prediction interval contains a range of values which should include the actual future value with probability \\(95\\%\\). Forecast distribution is the probability distribution of the outcome of interest at time \\(t\\), \\(y_t\\), given all information related, \\(I\\). When we talk about the “forecast”, we usually mean the average value of the forecast distribution, and we put a “hat” over \\(y\\) to show this. Thus, we write the forecast of \\(y_t\\) as \\(\\hat{y}_t\\), meaning the average of the possible values that \\(y_t\\) could take given everything we know. Occasionally, we will use \\(\\hat{y}_t\\) to refer to the median (or middle value) of the forecast distribution instead. \\(y_{T+h|T}\\) means the forecast of \\(y_{T+h}\\) taking account of \\(y_1,\\,...,y_T\\) (i.e., an h-step forecast taking account of all observations up to time \\(T\\)). "],
["time-series-graphics.html", "Chapter 2 Time series graphics 2.1 tsibble objects 2.2 Time plots 2.3 Patterns of time series: trend, seasonal and cyclic 2.4 Seasonal plots 2.5 Seasonal subseries plots 2.6 Visualization between time series 2.7 Lag plots 2.8 Calendar plots", " Chapter 2 Time series graphics 2.1 tsibble objects library(tsibble) library(tsibbledata) library(feasts) library(lubridate) library(patchwork) x &lt;- tsibble(Year = 2015:2019, Observation = c(123, 39, 78, 52, 110), index = Year) x #&gt; # A tsibble: 5 x 2 [1Y] #&gt; Year Observation #&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 2015 123 #&gt; 2 2016 39 #&gt; 3 2017 78 #&gt; 4 2018 52 #&gt; 5 2019 110 Multiple time series, key: olympic_running #&gt; # A tsibble: 312 x 4 [4Y] #&gt; # Key: Length, Sex [14] #&gt; Year Length Sex Time #&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1896 100 men 12 #&gt; 2 1900 100 men 11 #&gt; 3 1904 100 men 11 #&gt; 4 1908 100 men 10.8 #&gt; 5 1912 100 men 10.8 #&gt; 6 1916 100 men NA #&gt; # ... with 306 more rows key must be specified in one of the following form: 2.1.1 manipulation tidyverse-based manipulation PBS #&gt; # A tsibble: 65,219 x 9 [1M] #&gt; # Key: Concession, Type, ATC1, ATC2 [336] #&gt; Month Concession Type ATC1 ATC1_desc ATC2 ATC2_desc Scripts Cost #&gt; &lt;mth&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1991 Jul Concession~ Co-pa~ A Alimentary ~ A01 STOMATOLOG~ 18228 67877 #&gt; 2 1991 Aug Concession~ Co-pa~ A Alimentary ~ A01 STOMATOLOG~ 15327 57011 #&gt; 3 1991 Sep Concession~ Co-pa~ A Alimentary ~ A01 STOMATOLOG~ 14775 55020 #&gt; 4 1991 Oct Concession~ Co-pa~ A Alimentary ~ A01 STOMATOLOG~ 15380 57222 #&gt; 5 1991 Nov Concession~ Co-pa~ A Alimentary ~ A01 STOMATOLOG~ 14371 52120 #&gt; 6 1991 Dec Concession~ Co-pa~ A Alimentary ~ A01 STOMATOLOG~ 15028 54299 #&gt; # ... with 65,213 more rows PBS %&gt;% filter(ATC2 == &quot;A10&quot;) #&gt; # A tsibble: 816 x 9 [1M] #&gt; # Key: Concession, Type, ATC1, ATC2 [4] #&gt; Month Concession Type ATC1 ATC1_desc ATC2 ATC2_desc Scripts Cost #&gt; &lt;mth&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1991 Jul Concession~ Co-pa~ A Alimentary ~ A10 ANTIDIABE~ 89733 2.09e6 #&gt; 2 1991 Aug Concession~ Co-pa~ A Alimentary ~ A10 ANTIDIABE~ 77101 1.80e6 #&gt; 3 1991 Sep Concession~ Co-pa~ A Alimentary ~ A10 ANTIDIABE~ 76255 1.78e6 #&gt; 4 1991 Oct Concession~ Co-pa~ A Alimentary ~ A10 ANTIDIABE~ 78681 1.85e6 #&gt; 5 1991 Nov Concession~ Co-pa~ A Alimentary ~ A10 ANTIDIABE~ 70554 1.69e6 #&gt; 6 1991 Dec Concession~ Co-pa~ A Alimentary ~ A10 ANTIDIABE~ 75814 1.84e6 #&gt; # ... with 810 more rows # index column is automatically selected PBS %&gt;% filter(ATC2==&quot;A10&quot;) %&gt;% select(Concession, Type, ATC1) #&gt; # A tsibble: 816 x 4 [1M] #&gt; # Key: Concession, Type, ATC1 [4] #&gt; Concession Type ATC1 Month #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;mth&gt; #&gt; 1 Concessional Co-payments A 1991 Jul #&gt; 2 Concessional Co-payments A 1991 Aug #&gt; 3 Concessional Co-payments A 1991 Sep #&gt; 4 Concessional Co-payments A 1991 Oct #&gt; 5 Concessional Co-payments A 1991 Nov #&gt; 6 Concessional Co-payments A 1991 Dec #&gt; # ... with 810 more rows All key variable must be explicitly selected, that means we must at least select Concession and Type in the case above, since ATC2 and ATC1 are no longer key variables after filter(ATC == \"A10\") PBS %&gt;% filter(ATC2 == &quot;A10&quot;) %&gt;% as_tibble() %&gt;% count(Concession, Type, ATC1) #&gt; # A tibble: 4 x 4 #&gt; Concession Type ATC1 n #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 Concessional Co-payments A 204 #&gt; 2 Concessional Safety net A 204 #&gt; 3 General Co-payments A 204 #&gt; 4 General Safety net A 204 As to summarize(), index is automatically used as the grouping variable: PBS %&gt;% filter(ATC2 == &quot;A10&quot;) %&gt;% select(Month, Concession, Type, Cost) %&gt;% summarise(total_cost = sum(Cost)) #&gt; # A tsibble: 204 x 2 [1M] #&gt; Month total_cost #&gt; &lt;mth&gt; &lt;dbl&gt; #&gt; 1 1991 Jul 3526591 #&gt; 2 1991 Aug 3180891 #&gt; 3 1991 Sep 3252221 #&gt; 4 1991 Oct 3611003 #&gt; 5 1991 Nov 3565869 #&gt; 6 1991 Dec 4306371 #&gt; # ... with 198 more rows The mutate(), here we change the units from dollars to millions of dollars: PBS %&gt;% filter(ATC2 == &quot;A10&quot;) %&gt;% select(Month, Concession, Type, Cost) %&gt;% summarise(total_cost = sum(Cost)) %&gt;% mutate(cost = total_cost / 1e6) -&gt; a10 a10 #&gt; # A tsibble: 204 x 3 [1M] #&gt; Month total_cost cost #&gt; &lt;mth&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1991 Jul 3526591 3.53 #&gt; 2 1991 Aug 3180891 3.18 #&gt; 3 1991 Sep 3252221 3.25 #&gt; 4 1991 Oct 3611003 3.61 #&gt; 5 1991 Nov 3565869 3.57 #&gt; 6 1991 Dec 4306371 4.31 #&gt; # ... with 198 more rows 2.1.2 importing prison &lt;- vroom::vroom(&quot;https://OTexts.com/fpp3/extrafiles/prison_population.csv&quot;) prison &lt;- prison %&gt;% mutate(quarter = yearquarter(Date)) %&gt;% select(-Date) %&gt;% as_tsibble(key = c(State, Gender, Legal, Indigenous), index = quarter) prison #&gt; # A tsibble: 3,072 x 6 [1Q] #&gt; # Key: State, Gender, Legal, Indigenous [64] #&gt; State Gender Legal Indigenous Count quarter #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;qtr&gt; #&gt; 1 ACT Female Remanded ATSI 0 2005 Q1 #&gt; 2 ACT Female Remanded ATSI 1 2005 Q2 #&gt; 3 ACT Female Remanded ATSI 0 2005 Q3 #&gt; 4 ACT Female Remanded ATSI 0 2005 Q4 #&gt; 5 ACT Female Remanded ATSI 1 2006 Q1 #&gt; 6 ACT Female Remanded ATSI 1 2006 Q2 #&gt; # ... with 3,066 more rows 2.2 Time plots (melsyd_economy &lt;- ansett %&gt;% filter(Airports == &quot;MEL-SYD&quot;, Class == &quot;Economy&quot;)) #&gt; # A tsibble: 282 x 4 [1W] #&gt; # Key: Airports, Class [1] #&gt; Week Airports Class Passengers #&gt; &lt;week&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1987 W26 MEL-SYD Economy 20167 #&gt; 2 1987 W27 MEL-SYD Economy 20161 #&gt; 3 1987 W28 MEL-SYD Economy 19993 #&gt; 4 1987 W29 MEL-SYD Economy 20986 #&gt; 5 1987 W30 MEL-SYD Economy 20497 #&gt; 6 1987 W31 MEL-SYD Economy 20770 #&gt; # ... with 276 more rows melsyd_economy %&gt;% autoplot(Passengers) + labs(title = &quot;Ansett economy class passengers&quot;, subtitle = &quot;Melbourne-Sydney&quot;, x = &quot;Year&quot;) autoplot() automatically produces an appropriate plot of whatever you pass to it in the first argument. When there are multiple time series in a tsibble(), they are plotted separately: olympic_running %&gt;% autoplot(.vars = Time) 2.3 Patterns of time series: trend, seasonal and cyclic Trend: A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes we will refer to a trend as “changing direction”, when it might go from an increasing trend to a decreasing trend. Seasonal: A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known period(within a season, a year, etc.). Cyclic: A cycle occurs when the data exhibit rises and falls that are not of a fixed frequency. These fluctuations are usually due to economic conditions, and are often related to the “business cycle”. The duration of these fluctuations is usually at least 2 years. Many people confuse cyclic behaviour with seasonal behaviour, but they are really quite different. If the fluctuations are not of a fixed frequency then they are cyclic; if the frequency is unchanging and associated with some aspect of the calendar, then the pattern is seasonal. In general, the average length of cycles is longer than the length of a seasonal pattern, and the magnitudes of cycles tend to be more variable than the magnitudes of seasonal patterns. (As we will see soon, gg_season() provides a useful tool in distinguishing between seasonal and cyclic patterns.) It’s crucial to first identify the time series patterns in the data, and then choose a method that is able to capture the patterns properly. An example of combined patterns: topleft: storng seasonality, cyclic period in 6 - 10 years, no trend topright: decreasing trend, no trend or cyclic bottomleft: increasing trend, seasonality (perhaps on a yearly basis?) bottomright: random fluctuation 2.4 Seasonal plots A seasonal plot is similar to a time plot except that the data are plotted against the individual “seasons” in which the data were observed. A seasonal plot allows the underlying seasonal pattern to be seen more clearly, and is especially useful in identifying years in which the pattern changes. # y can be automatically chosen a10 %&gt;% gg_season(y = cost, labels = &quot;both&quot;) + ylab(&quot;$ million&quot;) + ggtitle(&quot;Seasonal plot: antidiabetic drug sales&quot;) Here labels = \"both\" means labels of years are displayed on both sides of the plot. In this case, it is clear that there is a plummet in sales in January each year. And there is an aberrant decrease in March 2008, since most years would see an increase from Feb to Mar. A way to reproduce the plot generated by gg_season(): a10 %&gt;% mutate(month = month(Month), year = year(Month)) %&gt;% # accessor function from lubridate package as_tibble() %&gt;% group_by(year, month) %&gt;% summarize(cost = sum(cost)) %&gt;% ggplot() + geom_line(aes(month, cost, color = year, group = year)) 2.4.1 Multiple seasonal periods: period in gg_season() Where the data has more than one seasonal pattern, the period argument can be used to select which seasonal plot is required. The vic_elec data contains half-hourly electricity demand for the state of Victoria, Australia. We can plot the daily pattern, weekly pattern or yearly pattern as follows. daily: vic_elec %&gt;% gg_season(period = &quot;day&quot;) weekly: vic_elec %&gt;% gg_season(period = &quot;week&quot;) yearly: # this plot in fact contains 4 lines vic_elec %&gt;% gg_season(period = &quot;year&quot;) + theme(legend.position = &quot;top&quot;) Different from autoplot(), when gg_season() and the gg_subseries() (later introduced) encounter multiple time series, they are displayed in facets. 2.5 Seasonal subseries plots An alternative plot that emphasises the seasonal patterns is where the data for each season are collected together by facet. a10 %&gt;% gg_subseries(cost) + ylab(&quot;$ million&quot;) + xlab(&quot;Year&quot;) + ggtitle(&quot;Seasonal subseries plot: antidiabetic drug sales&quot;) The blue horizontal lines indicate the means for each month. This form of plot enables the underlying seasonal pattern to be seen clearly, and also shows the changes in seasonality over time. It is especially useful in identifying changes within particular seasons. In this example, the plot is not particularly revealing; but in some cases, this is the most useful way of viewing seasonal changes over time. 2.5.1 Example: Australian holiday tourism holiday_tourism contains holiday tourists from 1998 to 2017, deaggregated by Region, State: holiday_tourism &lt;- tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% select(-Purpose) holiday_tourism #&gt; # A tsibble: 6,080 x 4 [1Q] #&gt; # Key: Region, State [76] #&gt; Quarter Region State Trips #&gt; &lt;qtr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1998 Q1 Adelaide South Australia 224. #&gt; 2 1998 Q2 Adelaide South Australia 130. #&gt; 3 1998 Q3 Adelaide South Australia 156. #&gt; 4 1998 Q4 Adelaide South Australia 182. #&gt; 5 1999 Q1 Adelaide South Australia 185. #&gt; 6 1999 Q2 Adelaide South Australia 135. #&gt; # ... with 6,074 more rows Then we can get total visitors by states(i.e., ignoring regions), and then plot it using autoplot(), note this is a multiple time series: holidays &lt;- holiday_tourism %&gt;% group_by(State) %&gt;% # no need for group_by(Quarter, Region) summarize(trips = sum(Trips)) %&gt;% ungroup() holidays %&gt;% autoplot() + labs(x = &quot;Year&quot;, y = NULL, title = &quot;Australian domestic holiday nights&quot;, subtitle = &quot;y: Thousands of trips&quot;) Time plots of each series shows that there is strong seasonality for most states, but that the seasonal peaks do not coincide. To see the timing of the seasonal peaks in each state, we can use a season plot. holidays %&gt;% gg_season(trips) + labs(y = NULL, title = &quot;Australian domestic holiday nights&quot;, subtitle = &quot;y: Thousands of trips&quot;) Here it is clear that the southern states of Australia (Tasmania, Victoria and South Australia) have strongest tourism in Q1 (their summer), while the northern states (Queensland and the Northern Territory) have the strongest tourism in Q3 (their dry season). The corresponding subseries plots are shown below: holidays %&gt;% gg_subseries(trips) + labs(y = NULL, title = &quot;Australian domestic holiday nights&quot;, subtitle = &quot;y: Thousands of trips&quot;) This figure makes it evident that Western Australian tourism has jumped markedly in recent years, while Victorian tourism has increased in Q1 and Q4 but not in the middle of the year. 2.6 Visualization between time series While autoplot(), gg_season() and gg_subseries() is instrumental in visualizing individual time series, it is also useful to explore relationships between time series. vic_elec half-hourly electricity Demand (in Gigawatts) and Temperature (in degrees Celsius), for 2014 in Victoria, Australia. The temperatures are for Melbourne, the largest city in Victoria, while the demand values are for the entire state. Addition to draw two separate time series, we can make a scatter plot to see relationship between the two: library(ggpointdensity) vic_elec %&gt;% ggplot() + geom_pointdensity(aes(Demand, Temperature)) + scale_color_viridis_c() 2.6.1 scatterplot matrices When there are several potential predictor variables, it is useful to plot each variable against each other variable. Consider the eight time series shown below, showing quarterly visitor numbers across states and territories of Australia. visitors &lt;- tourism %&gt;% group_by(State) %&gt;% summarize(trips = sum(Trips)) visitors %&gt;% ggplot(aes(x = Quarter, y = trips)) + geom_line() + facet_grid(vars(State), scales = &quot;free_y&quot;) + labs(title = &quot;Number of visitor nights each quarter (millions)&quot;) To better illustrate how number of visitors in different states are related, we could draw a scatterplot matrix by GGally；:ggpairs() library(GGally) visitors %&gt;% pivot_wider(names_from = State, values_from = trips) %&gt;% ggpairs(columns = 2:9) 2.7 Lag plots Lag plot is another useful tool in discerning seaonality, which display \\(y_t\\) against \\(y_{t-k}\\) in a time series, \\(k\\) being a constant for each individual plot. Here we use the Australian beer production data to make a lag plot, in gg_lag(), lags determines different values of \\(k\\) displayed (defaults to 1:9) and geom types of geometry: recent_production &lt;- aus_production %&gt;% filter(year(Quarter) &gt;= 1992) recent_production %&gt;% gg_lag(y = Beer, lags = 1:9, geom = &quot;point&quot;) It’s no surprise that strong relationship is detected for \\(k = 4\\) and \\(k = 8\\), since in these 2 panels data points are collected from the same quarter, and from this pattern strong seasonality of beer production can be found. 2.7.1 Autocorrelation Just as correlation measures the extent of a linear relationship between two variables, autocorrelation measures the linear relationship between lagged values of a time series. There are several autocorrelation coefficients, corresponding to each panel in the lag plot. For example, \\(r_1\\) measures the relationship between \\(y_t\\) and \\(y_{t−1}\\), \\(r_2\\) measures the relationship between \\(y_t\\) and \\(y_{t−2}\\) and so on. The sample correlation coefficient (pearson) is defined as: \\[ r_{x, y} = \\frac{\\sum{(x_t - \\bar{x})(y_t - \\bar{y})}}{\\sqrt{\\sum{(x_t - \\bar{x}})^2} {\\sqrt{\\sum{(y_t - \\bar{y}})^2}}} \\] given \\(x_t = y_t\\) and \\(y_t = y_{t-k}\\) we get the autocorrelation coefficient: \\[ r_k = r_{y_t,y_{t-k}} = \\frac{\\sum{(y_t - \\bar{y})(y_{t-k} - \\bar{y})}}{\\sum{(y_t - \\bar{y})^2}} \\] The autocorrelation coefficients make up the autocorrelation function or ACF. The autocorrelation coefficients for the beer production data can be computed using the ACF(.data, lag_max) function, with lag_max defaulting to \\(10 \\times \\log{\\frac{N}{M}}\\) where \\(N\\) is the number of observations and \\(M\\) the number of series.. recent_production %&gt;% ACF(Beer) #&gt; # A tsibble: 18 x 2 [1Q] #&gt; lag acf #&gt; &lt;lag&gt; &lt;dbl&gt; #&gt; 1 1Q -0.102 #&gt; 2 2Q -0.657 #&gt; 3 3Q -0.0603 #&gt; 4 4Q 0.869 #&gt; 5 5Q -0.0892 #&gt; 6 6Q -0.635 #&gt; # ... with 12 more rows Similarly, the ACF() result could be plotted by autoplot(), which is oftern referred to as correlogram: recent_production %&gt;% ACF(Beer) %&gt;% autoplot() In this graph: \\(r_4\\) is higher than for the other lags. This is due to the seasonal pattern in the data: the peaks tend to be four quarters apart and the troughs tend to be four quarters apart. \\(r_2\\) is more negative than for the other lags because troughs tend to be two quarters behind peaks. The dashed blue lines indicate whether the correlations are significantly different from zero. This seasonal pattern could also be visualized by `gg_season()`` recent_production %&gt;% gg_season() recent_production %&gt;% gg_subseries() 2.7.2 Trend and seasonality in ACF plots When data have a trend, the autocorrelations for small lags tend to be large and positive because observations nearby in time are also nearby in size. So the ACF of trended time series tend to have positive values that slowly decrease as the lags increase. When data are seasonal, the autocorrelations will be larger for the seasonal lags (at multiples of the seasonal frequency) than for other lags. When data are both trended and seasonal, you see a combination of these effects, as shown below in the case of a10. a10 %&gt;% ACF(cost, lag_max = 48) %&gt;% autoplot() a10 %&gt;% autoplot() 2.8 Calendar plots Calendar-based graphics arranges the values according to the corresponding dates into a calendar layout, which is comprised of weekdays in columns and weeks of a month in rows for a common monthly calendar. In sugrrants the built-in calendars include monthly, weekly, and daily types for the purpose of comparison between different temporal components. # remotes::install_github(&quot;earowang/sugrrants&quot;) library(sugrrants) facet_calender() lay out panels in a calendar format, and a monthly calendar is set up as a 5 by 7 layout matrix hourly_peds %&gt;% filter(Date &lt; ymd(&quot;2016-04-01&quot;)) %&gt;% ggplot(aes(x = Time, y = Hourly_Counts, colour = Sensor_Name)) + geom_line() + facet_calendar(~ Date) + # a variable contains dates theme_bw() + theme(legend.position = &quot;bottom&quot;) In contrast, frame_calendar() display calendar plot not by facetting, but by computing a more compact calendar grids as a data frame or a tibble according to its data input, and ggplot2 takes care of the plotting as you usually do with a data frame. This is a faster, more flexible alternative. Note center is just a normal tibble: # sensor only–Melbourne Convention Exhibition Centre center &lt;- hourly_peds %&gt;% filter(Year == &quot;2017&quot;, Sensor_Name == &quot;Melbourne Convention Exhibition Centre&quot;) center #&gt; # A tibble: 2,880 x 10 #&gt; Date_Time Date Year Month Mdate Day Time Sensor_ID #&gt; &lt;dttm&gt; &lt;date&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2017-01-01 00:00:00 2017-01-01 2017 Janu~ 1 Sund~ 0 25 #&gt; 2 2017-01-01 01:00:00 2017-01-01 2017 Janu~ 1 Sund~ 1 25 #&gt; 3 2017-01-01 02:00:00 2017-01-01 2017 Janu~ 1 Sund~ 2 25 #&gt; 4 2017-01-01 03:00:00 2017-01-01 2017 Janu~ 1 Sund~ 3 25 #&gt; 5 2017-01-01 04:00:00 2017-01-01 2017 Janu~ 1 Sund~ 4 25 #&gt; 6 2017-01-01 05:00:00 2017-01-01 2017 Janu~ 1 Sund~ 5 25 #&gt; # ... with 2,874 more rows, and 2 more variables: Sensor_Name &lt;chr&gt;, #&gt; # Hourly_Counts &lt;dbl&gt; The first argument in frame_calendar()is the data so that the data frame can directly be piped into the function using %&gt;%. x is a variable indicating time of day could be, y a value variable, date a Date variable to organise the data into a correct chronological order In center, we have 24 Hourly_Counts observations per day, with the day indicated by Date, and time in that day by Time, \\(\\text{Time} = 1, 2, \\dots, 24\\). So x = Time, y = Hourly_Counts, date = Date. We also use calendar = \"monthly\" to specify a calendar type, this could also be “weekly” or “daily” center_calendar &lt;- center %&gt;% frame_calendar(x = Time, y = Hourly_Counts, date = Date, calendar = &quot;monthly&quot;) This generates two new columns .x and .y, in this case .Time and .Hourly_Counts. This two columns together give new corrdinates computed for different types of calenders. date groups the same dates in a chronical order which is useful geom_line() and geom_path(). The basic use is ggplot(aes(x = .x, y = .y, group = date)) + geom_* p1 &lt;- center_calendar %&gt;% ggplot(aes(x = .Time, y = .Hourly_Counts, group = Date)) + geom_line() p1 prettify() can be used to make the former plot more informative. It takes a ggplot object and gives sensible breaks and labels. It can be noted that the calendar-based graphic depicts time of day, day of week, and other calendar effects like public holiday in a clear manner. prettify(p1) It’s not necessarily working with lines but other geoms too. And y can take multiple variable names in combination with vars(). The rectangular glyphs arranged on a “weekly” calendar are plotted to illustrate the usage of the multiple ys and the differences between sensors. two_sensors_wide &lt;- hourly_peds %&gt;% filter(year(Date) == &quot;2017&quot;, Sensor_Name %in% c(&quot;Lonsdale St (South)&quot;, &quot;Melbourne Convention Exhibition Centre&quot;)) %&gt;% select(-Sensor_ID) %&gt;% pivot_wider(names_from = Sensor_Name, values_from = &quot;Hourly_Counts&quot;) %&gt;% rename( Lonsdale = `Lonsdale St (South)`, Centre = `Melbourne Convention Exhibition Centre` ) %&gt;% mutate( diff = Centre - Lonsdale, more = if_else(diff &gt; 0, &quot;Centre&quot;, &quot;Lonsdale&quot;) ) p2 &lt;- two_sensors_wide %&gt;% frame_calendar(x = Time, y = vars(Lonsdale, Centre), date = Date, calendar = &quot;weekly&quot;) %&gt;% ggplot(aes(.Time, group = Date)) + geom_rect(aes( xmin = .Time, xmax = .Time + 0.005, ymin = .Lonsdale, ymax = .Centre, fill = more )) + scale_fill_manual(name = &quot;Which sensor spotted more pedestrians?&quot;, values = c(&quot;#798234&quot;, &quot;#d0587e&quot;)) + theme_minimal() + theme(legend.position = &quot;top&quot;) prettify(p2) More information in Package vignette "],
["time-series-decomposition.html", "Chapter 3 Time series decomposition 3.1 Transformation and adjustments 3.2 Moving averages 3.3 Classical decomposition 3.4 X11 decomposition 3.5 SEATS decomposition 3.6 STL decomposition", " Chapter 3 Time series decomposition library(tsibble) library(tsibbledata) library(fable) library(feasts) library(lubridate) 3.1 Transformation and adjustments 3.1.1 Calendar adjustments Some of the variation seen in seasonal data may be due to simple calendar effects. In such cases, it is usually much easier to remove the variation before doing any further analysis. For example, if you are studying the total monthly sales in a retail store, there will be variation between the months simply because of the different numbers of trading days in each month, in addition to the seasonal variation across the year. It is easy to remove this variation by computing average sales per trading day in each month, rather than total sales in the month. Then we effectively remove the calendar variation. Simpler patterns are usually easier to model and lead to more accurate forecasts. 3.1.2 Population adjustments use index per capita rather than index itself (when related to population) 3.1.3 Inflation adjustments Price index such as CPI (Consumer Price Index) allows us to compare the growth or decline of industries relative to a common price value. For example, looking at aggregate “newspaper and book” retail turnover from aus_retail, and adjusting the data for inflation using CPI from global_economy allows us to understand the changes over time. print_retail &lt;- aus_retail %&gt;% filter(Industry == &quot;Newspaper and book retailing&quot;) %&gt;% index_by(Year = year(Month)) %&gt;% summarise(Turnover = sum(Turnover)) aus_economy &lt;- global_economy %&gt;% filter(Code == &quot;AUS&quot;) print_retail %&gt;% left_join(aus_economy, by = &quot;Year&quot;) %&gt;% mutate(Turnover_adj = Turnover / CPI) %&gt;% pivot_longer(c(Turnover, Turnover_adj), names_to = &quot;type&quot;, values_to = &quot;Turnover&quot;) %&gt;% ggplot() + geom_line(aes(Year, Turnover)) + facet_wrap(~ type, scales = &quot;free_y&quot;) 3.1.4 Mathematical transformation (Box-Cox) If the data shows variation that increases or decreases with the level of the series, then a transformation can be useful. A useful family of transformations, that includes both logarithms and power transformations, is the family of Box-Cox transformations, which depend on the parameter \\(\\lambda\\) and are defined as follows: \\[\\begin{equation} \\tag{3.1} w_t = \\begin{cases} \\log{y_t}&amp; \\text{if} \\; \\lambda = 0 \\\\ (y^\\lambda_t - 1)/\\lambda &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] The logarithm in a Box-Cox transformation is always a natural logarithm (i.e., to base \\(e\\)). So if \\(λ = 0\\), natural logarithms are used, but if \\(\\lambda \\not= 0\\), a power transformation is used, followed by some simple scaling. If \\(\\lambda = 1\\), then \\(w_t = y_t−1\\), so the transformed data is shifted downwards but there is no change in the shape of the time series. But for all other values of \\(\\lambda\\), the time series will change shape. A good value of is one which makes the size of the seasonal variation about the same across the whole series, as that makes the forecasting model simpler. The guerrero feature can be used to choose (estimate) a value of lambda for you. In the following case it chooses \\(\\lambda = 0.12\\). lambda &lt;- aus_production %&gt;% features(Gas, guerrero) %&gt;% pull(1) lambda #&gt; [1] 0.1204864 aus_production %&gt;% mutate(Gas_transformed = box_cox(Gas, lambda)) %&gt;% pivot_longer(c(Gas, Gas_transformed), names_to = &quot;type&quot;, values_to = &quot;value&quot;) %&gt;% ggplot() + geom_line(aes(Quarter, value)) + facet_wrap(~ type, scales = &quot;free&quot;) 3.2 Moving averages A moving average of order \\(m\\) can be written as: \\[ \\bar{T}_t = \\frac{1}{m}\\sum_{j = -k}^{k}{y_{t+j}} \\] where \\(m = 2k + 1\\) For example, a moving average of order 5 will select a data point, takes 2 points before it and 2 points after it and compute the average of these 5 points. In practice, for top or bottom observations where are no value in either the first or last \\((m - 1) / 2\\) position, the original value will be used to avoid NULL for border observations. To suppress this behaviour, use .complete = TRUE in slide_ functions. library(slider) slide(1:5, ~ mean(.x), .before = 2, .after = 2) #&gt; [[1]] #&gt; [1] 2 #&gt; #&gt; [[2]] #&gt; [1] 2.5 #&gt; #&gt; [[3]] #&gt; [1] 3 #&gt; #&gt; [[4]] #&gt; [1] 3.5 #&gt; #&gt; [[5]] #&gt; [1] 4 slide(1:5, ~ mean(.x), .before = 2, .after = 2, .complete = TRUE) #&gt; [[1]] #&gt; NULL #&gt; #&gt; [[2]] #&gt; NULL #&gt; #&gt; [[3]] #&gt; [1] 3 #&gt; #&gt; [[4]] #&gt; NULL #&gt; #&gt; [[5]] #&gt; NULL In the book slide_ functions from tsibble packaege are used to do sliding window calculation, which will be superseded by the slider package in the future. For this reason I will try to make some replacements whenever there is a need for sliding window functions library(slider) aus_exports &lt;- global_economy %&gt;% filter(Country == &quot;Australia&quot;) %&gt;% mutate( `5-MA` = slide_dbl(Exports, ~ mean(.x), .before = 2, .after = 2) ) %&gt;% pivot_longer(c(Exports, `5-MA`), names_to = &quot;type&quot;, values_to = &quot;Export&quot;) aus_exports %&gt;% ggplot(aes(Year, Export, color = type)) + geom_line() + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) Notice that the trend-cycle (in red) is smoother than the original data and captures the main movement of the time series without all of the minor fluctuations. The order of the moving average determines the smoothness of the trend-cycle estimate. In general, a larger order means a smoother curve. Figure below shows the effect of changing the order of the moving average for the Australian exports data. Simple moving averages such as these are usually of an odd order (e.g., \\(3\\), \\(5\\), \\(7\\), etc.). This is so they are symmetric: in a moving average of order \\(m = 2k + 1\\), the middle observation, and \\(k\\) observations on either side, are averaged. But if m was even, it would no longer be symmetric. 3.2.1 Moving averages of moving averages When \\(m\\) is even, we could still compute a \\(m-MA\\) though losing symmetry. A \\(4-MA\\) can be defined as : \\[ \\hat{T}_t = \\frac{1}{4}(y_{t-1} + y_t + y_{t+1} + y_{t+2}) \\] It is possible to apply a moving average to a moving average. One reason for doing this is to make an even-order moving average symmetric. For example, we might take a moving average of order \\(4\\), and then apply another moving average of order \\(2\\) to the results. And the final result, \\(2\\times 4 -MA\\) can be written as: \\[ \\begin{align} \\hat{T}_t &amp;= \\frac{1}{2}[ \\frac{1}{4}( y_{t-2} + y_{t-1} + y_t + y_{t+1}) + \\frac{1}{4}(y_{t-1} + y_{t} + y_{t + 1} + y_{t+2})] \\\\ &amp;= \\frac{1}{8}y_{t-2} + \\frac{1}{4}y_{t-1} + \\frac{1}{4}y_{t} + \\frac{1}{4}y_{t+1} + \\frac{1}{8}y_{t+2} \\end{align} \\] Now we can see that \\(2\\times 4 -MA\\) is an weighted average of 5 values centered on \\(y_t\\), and is symmetric. Other combinations of moving averages are also possible. For example, a \\(3 \\times 3-MA\\) is often used, and consists of a moving average of order \\(3\\) followed by another moving average of order \\(3\\). In general, an even order MA should be followed by an even order MA to make it symmetric. Similarly, an odd order MA should be followed by an odd order \\(MA\\). In general, a \\(2×m-MA\\) is equivalent to a weighted moving average of order \\(m+1\\) where all observations take the weight \\(1/m\\), except for the first and last terms which take weights \\(1/(2m)\\) 3.2.2 Estimating the trend-cycle component with seasonal data The most common use of centred moving averages is for estimating the trend-cycle from seasonal data. Consider the \\(2\\times 4- MA\\): \\[ \\hat{T}_t = \\frac{1}{8}y_{t-2} + \\frac{1}{4}y_{t-1} + \\frac{1}{4}y_{t} + \\frac{1}{4}y_{t+1} + \\frac{1}{8}y_{t+2} \\] When applied to quarterly data, each quarter of the year is given equal weight as the first and last terms apply to the same quarter in consecutive years. Consequently, the seasonal variation will be averaged out and the resulting values of \\(\\hat{T}_t\\) will have little or no seasonal variation remaining. A similar effect would be obtained using a \\(2×8-MA\\) or a \\(2×12-MA\\) to quarterly data. As is noted above, a \\(2×m-MA\\) is equivalent to a weighted \\(m + 1 -MA\\), which handles a \\(m\\) seasonal period when \\(m\\) is even. So, if the seasonal period is even and of order \\(m\\), we use a \\(2×m-MA\\) to estimate the trend-cycle. If the seasonal period is odd and of order \\(m\\), we use a \\(m-MA\\) to estimate the trend-cycle. In aus_livestock data we can see some sort of seasonality: aus_livestock %&gt;% filter(Animal == &quot;Pigs&quot;, State == &quot;Australian Capital Territory&quot;) %&gt;% gg_season() Since it is collected monthly, we can use a \\(2\\times12-MA\\) to average out the seasonality: capital_livestock_ma &lt;- aus_livestock %&gt;% filter(Animal == &quot;Pigs&quot;, State == &quot;Australian Capital Territory&quot;) %&gt;% mutate(`2-MA` = slide_dbl(Count, ~ mean(.x), .before = 1), `12x2-MA` = slide_dbl(`2-MA`, ~ mean(.x), .before = 5, .after = 6)) %&gt;% select(-`2-MA`) %&gt;% pivot_longer(4:5, names_to = &quot;type&quot;, values_to = &quot;count&quot;) capital_livestock_ma %&gt;% ggplot(aes(Month, count, color = type)) + geom_line() + scale_color_manual(values = c(&quot;red&quot;, &quot;gray&quot;)) 3.2.3 Weighted moving averages Combinations of moving averages result in weighted moving averages. For example, the \\(2\\times4-MA\\) discussed above is equivalent to a weighted \\(5-MA\\) with weights given by \\([\\frac{1}{8},\\frac{1}{4},\\frac{1}{4},\\frac{1}{4},\\frac{1}{8}]\\). In general, a weighted \\(m-MA\\) can be written as \\[ \\hat{T}_t = \\sum_{j = -k}^{k}{a_jy_{t+j}} \\] where \\(m = 2k + 1\\). It is important that the weights all sum to one and that they are symmetric so that \\(a_j=a_{−j}\\). The simple \\(m-MA\\) is a special case where all of the weights are equal to \\(1/m\\). A major advantage of weighted moving averages is that they yield a smoother estimate of the trend-cycle. Instead of observations entering and leaving the calculation at full weight, their weights slowly increase and then slowly decrease, resulting in a smoother curve. 3.3 Classical decomposition us_retail_employment &lt;- fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990, Title == &quot;Retail Trade&quot;) %&gt;% select(-Series_ID) classical_dcmp &lt;- us_retail_employment %&gt;% model(classical = feasts:::classical_decomposition(Employed, type = &quot;additive&quot;)) %&gt;% components() classical_dcmp %&gt;% autoplot() 3.4 X11 decomposition feasts:::X11(Employed, type = \"additive\") makes specifications about a X11 additive model, and model() were called to estimate it, and components() gives \\(\\hat{T}_t\\), \\(\\hat{S}_t\\) and \\(\\hat{R}_t\\) for each observation. This is only applicable to monthly or quarterly data. x11_dcmp &lt;- us_retail_employment %&gt;% model(x11 = feasts:::X11(Employed, type = &quot;additive&quot;)) %&gt;% components() x11_dcmp #&gt; # A dable: 357 x 7 [1M] #&gt; # Key: .model [1] #&gt; # X11 Decomposition: Employed = trend + seasonal + irregular #&gt; .model Month Employed trend seasonal irregular season_adjust #&gt; &lt;chr&gt; &lt;mth&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 x11 1990 Jan 13256. 13260. -20.5 16.0 13276. #&gt; 2 x11 1990 Feb 12966. 13248. -253. -29.1 13219. #&gt; 3 x11 1990 Mar 12938. 13237. -291. -7.47 13229. #&gt; 4 x11 1990 Apr 13012. 13227. -217. 2.31 13229. #&gt; 5 x11 1990 May 13108. 13217. -111. 2.40 13219. #&gt; 6 x11 1990 Jun 13183. 13204. -21.0 -0.192 13204. #&gt; # ... with 351 more rows Note that observations at top (and bottom) now have valid trend estimate and therefore seasonal, irregular and seasonal_adjust are no longer NA. Next plot shows the trend-cycle component and the seasonally adjusted data, along with the original data. x11_dcmp %&gt;% ggplot(aes(x = Month)) + geom_line(aes(y = Employed, colour = &quot;Data&quot;)) + geom_line(aes(y = season_adjust, colour = &quot;Seasonally Adjusted&quot;)) + geom_line(aes(y = trend, colour = &quot;Trend&quot;)) + xlab(&quot;Year&quot;) + ylab(&quot;Persons (thousands)&quot;) + ggtitle(&quot;Total employment in US retail&quot;) + scale_colour_manual(values = c(&quot;gray&quot;,&quot;blue&quot;,&quot;red&quot;), breaks = c(&quot;Data&quot;,&quot;Seasonally Adjusted&quot;,&quot;Trend&quot;)) x11_dcmp %&gt;% autoplot() It can be useful to use seasonal plots and seasonal sub-series plots of the seasonal component. These help us to visualise the variation in the seasonal component over time. In this case, there are only small changes over time. x11_dcmp %&gt;% gg_subseries(seasonal) To campare these x11 and classical model, we can specify multiple models in models, this plot shows that the estimated seasonal effect of x11 model have deviated a bit in the recent decade and that the classical model are truncated on both sides, though this two models may not practically have a meaningful difference in this case. us_retail_employment %&gt;% model(classical = feasts:::classical_decomposition(Employed, type = &quot;additive&quot;), x11 = feasts:::X11(Employed, type = &quot;additive&quot;)) %&gt;% components() %&gt;% autoplot() 3.5 SEATS decomposition The procedure works only with quarterly and monthly data. So seasonality of other kinds, such as daily data, or hourly data, or weekly data, require an alternative approach. Only monthly and quarterly. seats_dcmp &lt;- us_retail_employment %&gt;% model(seats = feasts:::SEATS(Employed)) %&gt;% components() seats_dcmp %&gt;% autoplot() + ggtitle(&quot;SEATS decomposition of total US retail employment&quot;) 3.6 STL decomposition STL is a versatile and robust method for decomposing time series. STL is an acronym for “Seasonal and Trend decomposition using Loess”, while Loess is a method for estimating nonlinear relationships. STL has several advantages over the classical, SEATS and X11 decomposition methods: Unlike SEATS and X11, STL will handle any type of seasonality, not only monthly and quarterly data. The seasonal component is allowed to change over time, and the rate of change can be controlled by the user. The smoothness of the trend-cycle can also be controlled by the user. It can be robust to outliers (i.e., the user can specify a robust decomposition), so that occasional unusual observations will not affect the estimates of the trend-cycle and seasonal components. They will, however, affect the remainder component. us_retail_employment %&gt;% model(STL( Employed ~ trend(window = 7) + season(window = Inf), robust = TRUE)) %&gt;% components() %&gt;% autoplot() "],
["time-series-features.html", "Chapter 4 Time series features 4.1 Simple statistics 4.2 ACF features 4.3 STL features 4.4 Other features 4.5 Exporing Australian tourism data", " Chapter 4 Time series features The feasts package includes functions for computing features And statistics from time series (hence the name). We have already seen some time series features. For example, autocorrelations can be considered features of a time series — they are numerical summaries computed from the series. Another feature we saw in the last chapter was the Guerroro estimate of the Box-Cox transformation parameter — again, this is a number computed from a time series. We can compute many different features on many different time series, and use them to explore the properties of the series. In this chapter we will look at some features that have been found useful in time series exploration, and how they can be used to uncover interesting information about your data. library(tsibble) library(tsibbledata) library(fable) library(feasts) 4.1 Simple statistics Any numerical summary computed from a time series is a feature of that time series — the mean, minimum or maximum, for example. The features can be computed using the features() function. For example, let’s compute the means of all the series in the Australian tourism data. tourism %&gt;% features(Trips, mean) #&gt; # A tibble: 304 x 4 #&gt; Region State Purpose ...1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Adelaide South Australia Business 156. #&gt; 2 Adelaide South Australia Holiday 157. #&gt; 3 Adelaide South Australia Other 56.6 #&gt; 4 Adelaide South Australia Visiting 205. #&gt; 5 Adelaide Hills South Australia Business 2.66 #&gt; 6 Adelaide Hills South Australia Holiday 10.5 #&gt; # ... with 298 more rows The new column V1 contains the mean of each time series. It is useful to give the resulting feature columns names to help us remember where they came from. This can be done by using a list of functions. tourism %&gt;% features(Trips, list(mean = mean)) %&gt;% arrange(desc(mean)) #&gt; # A tibble: 304 x 4 #&gt; Region State Purpose mean #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Sydney New South Wales Visiting 747. #&gt; 2 Melbourne Victoria Visiting 619. #&gt; 3 Sydney New South Wales Business 602. #&gt; 4 North Coast NSW New South Wales Holiday 588. #&gt; 5 Sydney New South Wales Holiday 550. #&gt; 6 Gold Coast Queensland Holiday 528. #&gt; # ... with 298 more rows Rather than compute one feature at a time, it is convenient to compute many features at once. A common short summary of a data set is to compute five summary statistics: the minimum, first quartile, median, third quartile and maximum. These divide the data into four equal-size sections, each containing 25% of the data. The quantile() function can be used to compute them. # as of now lambda expressions are not supported 2020.2.13 tourism %&gt;% features(Trips, quantile, prob = seq(0, 1, 0.25)) #&gt; # A tibble: 304 x 8 #&gt; Region State Purpose `0%` `25%` `50%` `75%` `100%` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Adelaide South Australia Business 68.7 134. 153. 177. 242. #&gt; 2 Adelaide South Australia Holiday 108. 135. 154. 172. 224. #&gt; 3 Adelaide South Australia Other 25.9 43.9 53.8 62.5 107. #&gt; 4 Adelaide South Australia Visiting 137. 179. 206. 229. 270. #&gt; 5 Adelaide Hills South Australia Business 0 0 1.26 3.92 28.6 #&gt; 6 Adelaide Hills South Australia Holiday 0 5.77 8.52 14.1 35.8 #&gt; # ... with 298 more rows 4.2 ACF features All the autocorrelations of a series can be considered features of that series. We can also summarise the autocorrelations to produce new features; for example, the sum of the first ten squared autocorrelation coefficients is a useful summary of how much autocorrelation there is in a series, regardless of lag. We can also compute autocorrelations of transformations of a time series. A useful transformation in this context is to look at changes in the series between periods. That is, we “difference” the data and create a new time series consisting of the differences between consecutive observations. Then we can compute the autocorrelations of this new differenced series. Occasionally it is useful to apply the same differencing operation again, so we compute the differences of the differences. The autocorrelations of this double differenced series may provide useful information. Another related approach is to compute seasonal differences of a series. If we had monthly data, for example, we would compute the difference between consecutive Januaries, consecutive Februaries, and so on. This enables us to look at how the series is changing between years, rather than between months. Again, the autocorrelations of the seasonally differenced series may provide useful information. The feat_acf() function computes a selection of the autocorrelations discussed here. It will return six or seven features: the first autocorrelation coefficient from the original data; the sum of square of the first ten autocorrelation coefficients from the original data; the first autocorrelation coefficient from the differenced data; the sum of square of the first ten autocorrelation coefficients from the differenced data; the first autocorrelation coefficient from the twice differenced data; the sum of square of the first ten autocorrelation coefficients from the twice differenced data; For seasonal data, the autocorrelation coefficient at the first seasonal lag is also returned. When applied to the Australian tourism data, we get the following output. tourism %&gt;% features(Trips, feat_acf) %&gt;% select(-(1:3)) #&gt; # A tibble: 304 x 7 #&gt; acf1 acf10 diff1_acf1 diff1_acf10 diff2_acf1 diff2_acf10 season_acf1 #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.0333 0.131 -0.520 0.463 -0.676 0.741 0.201 #&gt; 2 0.0456 0.372 -0.343 0.614 -0.487 0.558 0.351 #&gt; 3 0.517 1.15 -0.409 0.383 -0.675 0.792 0.342 #&gt; 4 0.0684 0.294 -0.394 0.452 -0.518 0.447 0.345 #&gt; 5 0.0709 0.134 -0.580 0.415 -0.750 0.746 -0.0628 #&gt; 6 0.131 0.313 -0.536 0.500 -0.716 0.906 0.208 #&gt; # ... with 298 more rows 4.3 STL features A time series decomposition can be used to measure the strength of trend and seasonality in a time series. Recall that the decomposition is written as \\[ y_t = T_t + S_t + R_t \\] For strongly trended data, the seasonally adjusted data should have stronger variation than that of the remainder component . Therefore \\(\\text{Var}(R_t) / \\text{Var}(T_t + R_t)\\) should be relatively small. But for data with little or no trend, the two variances should be approximately the same. So we define the strength of trend as: \\[ F_T = \\max(0\\,, 1- \\frac{\\text{Var}(R_t)}{\\text{Var}(T_t + R_t)}) \\] The strength of seasonality is defined similarly, but with respect to the detrended data rather than the seasonally adjusted data: \\[ F_S = \\max(0\\,, 1- \\frac{\\text{Var}(R_t)}{\\text{Var}(S_t + R_t)}) \\] These measures can be useful, for example, when you have a large collection of time series, and you need to find the series with the most trend or the most seasonality. Other useful features based on STL include the timing of peaks and troughs — which month or quarter contains the largest seasonal component and which contains the smallest seasonal component. tourism %&gt;% features(Trips, feat_stl) #&gt; # A tibble: 304 x 12 #&gt; Region State Purpose trend_strength seasonal_streng~ seasonal_peak_y~ #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Adela~ Sout~ Busine~ 0.451 0.380 3 #&gt; 2 Adela~ Sout~ Holiday 0.541 0.601 1 #&gt; 3 Adela~ Sout~ Other 0.743 0.189 2 #&gt; 4 Adela~ Sout~ Visiti~ 0.433 0.446 1 #&gt; 5 Adela~ Sout~ Busine~ 0.453 0.140 3 #&gt; 6 Adela~ Sout~ Holiday 0.512 0.244 2 #&gt; # ... with 298 more rows, and 6 more variables: seasonal_trough_year &lt;dbl&gt;, #&gt; # spikiness &lt;dbl&gt;, linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, #&gt; # stl_e_acf10 &lt;dbl&gt; We can then use these features in plots to identify what type of series are heavily trended and what are most seasonal. tourism %&gt;% features(Trips, feat_stl) %&gt;% ggplot() + geom_point(aes(trend_strength, seasonal_strength_year, color = Purpose)) The most seasonal series can also be easily identified and plotted. # most seasonal tourism %&gt;% features(Trips, feat_stl) %&gt;% filter(seasonal_strength_year == max(seasonal_strength_year)) %&gt;% left_join(tourism, by = c(&quot;State&quot;, &quot;Region&quot;, &quot;Purpose&quot;)) %&gt;% ggplot(aes(x = Quarter, y = Trips)) + geom_line() + facet_grid(vars(State, Region, Purpose)) 4.4 Other features All of the features included in the feasts package can be computed in one line like this, which gives total 44 features (result not show). tourism %&gt;% features(Trips, feature_set(pkgs = &quot;feasts&quot;)) all features related to stl decomposition tourism %&gt;% features(Trips, feature_set(tags = &quot;stl&quot;)) #&gt; # A tibble: 304 x 12 #&gt; Region State Purpose trend_strength seasonal_streng~ seasonal_peak_y~ #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Adela~ Sout~ Busine~ 0.451 0.380 3 #&gt; 2 Adela~ Sout~ Holiday 0.541 0.601 1 #&gt; 3 Adela~ Sout~ Other 0.743 0.189 2 #&gt; 4 Adela~ Sout~ Visiti~ 0.433 0.446 1 #&gt; 5 Adela~ Sout~ Busine~ 0.453 0.140 3 #&gt; 6 Adela~ Sout~ Holiday 0.512 0.244 2 #&gt; # ... with 298 more rows, and 6 more variables: seasonal_trough_year &lt;dbl&gt;, #&gt; # spikiness &lt;dbl&gt;, linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, stl_e_acf1 &lt;dbl&gt;, #&gt; # stl_e_acf10 &lt;dbl&gt; 4.5 Exporing Australian tourism data Specify multiple function tourism_tsl &lt;- tourism %&gt;% features(Trips, features = list(feat_stl, feat_acf)) tourism_tsl %&gt;% select(contains(&quot;season&quot;), Purpose) %&gt;% mutate( seasonal_peak_year = glue::glue(&quot;Q{seasonal_peak_year + 1}&quot;), seasonal_trough_year = glue::glue(&quot;Q{seasonal_trough_year + 1}&quot;)) %&gt;% GGally::ggpairs(aes(color = Purpose)) "],
["the-forecasters-toolbox.html", "Chapter 5 The forecaster’s toolbox 5.1 A tidy forecasting workflow 5.2 Some simple forecasting methods 5.3 Fitted values and residuals 5.4 Residual diagnostics 5.5 Prediction intervals 5.6 Evaluating model accuracy 5.7 Time series cross-validation 5.8 Forecasting using transformations 5.9 Forecasting with decomposition", " Chapter 5 The forecaster’s toolbox library(tsibble) library(tsibbledata) library(fable) library(feasts) library(lubridate) library(patchwork) 5.1 A tidy forecasting workflow 5.1.1 Data preparation (tidy) Simplified by tidyverse and tsibble 5.1.2 Visualize global_economy %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% autoplot(GDP) + scale_y_continuous(labels = scales::label_number_si()) + ggtitle(&quot;GDP for Sweden&quot;) + ylab(&quot;$US billions&quot;) 5.1.3 Define a model (specify) A model defination TSLM(GDP ~ trend()) #&gt; &lt;TSLM model definition&gt; TSLM() specifies a time series linear model, discussed more in 7. Similar to tidymodels where we specify a linear model for regression: library(parsnip) linear_reg() %&gt;% set_engine(&quot;lm&quot;) %&gt;% set_mode(&quot;regression&quot;) #&gt; Linear Regression Model Specification (regression) #&gt; #&gt; Computational engine: lm 5.1.4 Train the model (estimate) ge_fit &lt;- global_economy %&gt;% model(trend_model = TSLM(GDP ~ trend())) ge_fit #&gt; # A mable: 263 x 2 #&gt; # Key: Country [263] #&gt; Country trend_model #&gt; &lt;fct&gt; &lt;model&gt; #&gt; 1 Afghanistan &lt;TSLM&gt; #&gt; 2 Albania &lt;TSLM&gt; #&gt; 3 Algeria &lt;TSLM&gt; #&gt; 4 American Samoa &lt;TSLM&gt; #&gt; 5 Andorra &lt;TSLM&gt; #&gt; 6 Angola &lt;TSLM&gt; #&gt; # ... with 257 more rows 5.1.5 Check model performance (evaluate) More discussed in 5.6 5.1.6 Produce forecasts (forecast) forecast() similar to predict(): ge_forecast &lt;- ge_fit %&gt;% forecast(h = &quot;3 years&quot;) # h means horizon ge_forecast #&gt; # A fable: 789 x 5 [1Y] #&gt; # Key: Country, .model [263] #&gt; Country .model Year GDP .mean #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dist&gt; &lt;dbl&gt; #&gt; 1 Afghanistan trend_model 2018 N(16205101654, 12655976452653815808) 1.62e10 #&gt; 2 Afghanistan trend_model 2019 N(16511878141, 12703887023155042304) 1.65e10 #&gt; 3 Afghanistan trend_model 2020 N(16818654627, 12753314761722146816) 1.68e10 #&gt; 4 Albania trend_model 2018 N(13733734164, 3853726871001717248) 1.37e10 #&gt; 5 Albania trend_model 2019 N(14166852711, 3891477664840101376) 1.42e10 #&gt; 6 Albania trend_model 2020 N(14599971258, 3931325725002840064) 1.46e10 #&gt; # ... with 783 more rows Here ourl trend model consists of only a trend variable \\({1, 2, \\dots, T}\\), which can be known in advance when producing forecasts. In this case forecast() only needs h for a forecast horizon. There are also cases when we need to specify the new_data argument, which will be a tsibble containing future information used to forecast. ge_forecast %&gt;% filter(Country == &quot;Sweden&quot;) %&gt;% autoplot(global_economy) + scale_y_continuous(labels = scales::label_number_si()) + ggtitle(&quot;GDP for Sweden&quot;) 5.2 Some simple forecasting methods Some forecasting methods are extremely simple and surprisingly effective. We will use four simple forecasting methods as benchmarks throughout this book. To illustrate them, we will use quarterly Australian clay brick production between 1970 and 2004. bricks &lt;- aus_production %&gt;% filter_index(&quot;1970&quot; ~ &quot;2004&quot;) bricks #&gt; # A tsibble: 137 x 7 [1Q] #&gt; Quarter Beer Tobacco Bricks Cement Electricity Gas #&gt; &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1970 Q1 387 6807 386 1049 12328 12 #&gt; 2 1970 Q2 357 7612 428 1134 14493 18 #&gt; 3 1970 Q3 374 7862 434 1229 15664 23 #&gt; 4 1970 Q4 466 7126 417 1188 13781 20 #&gt; 5 1971 Q1 410 7255 385 1058 13299 19 #&gt; 6 1971 Q2 370 8076 433 1209 15230 23 #&gt; # ... with 131 more rows 5.2.1 Mean method Here, the forecasts of all future values are equal to the average (or “mean”) of the historical data. If we let the historical data be denoted by \\(y_1,…,y_T\\), then we can write the forecasts as \\[ \\hat{y}_{T+H | T} = \\frac{1}{T}\\sum_{1}^{T}{y_t} \\] MEAN() specifies an average model: bricks %&gt;% model(MEAN(Bricks)) %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(bricks) 5.2.2 Naive method For naive forecasts, we simply set all forecasts to be the value of the last observation. That is, \\[ \\hat{y}_{T+H | T} = y_T \\] This method works remarkably well for many economic and financial time series. Because a naive forecast is optimal when data follow a random walk (see Section 9.1), these are also called random walk forecasts. NAIVE() specifies a naive model: bricks %&gt;% model(NAIVE(Bricks)) %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(bricks) 5.2.3 Seasonal naive method Seasonal naïve method A similar method is useful for highly seasonal data. In this case, we set each forecast to be equal to the last observed value from the same season of the year (e.g., the same month of the previous year). Formally, the forecast for time \\(T+h\\) is written as \\[ \\hat{y}_{T+H | T} = y_{T + H - m(k + 1)} \\] where \\(m\\) is for seaonsal period (e.g., 12 for monthly data, 4 for quarterly data), and \\(k\\) is the interger part of \\((h - 1) / m\\) bricks %&gt;% model(SNAIVE(Bricks ~ lag(&quot;year&quot;))) %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(bricks, level = NULL) The lag() function is optional here as bricks is monthly data and so a seasonal naïve model will need a one-year lag. However, for some time series there is more than one seasonal period, and then the required lag must be specified. 5.2.4 Drift method A variation on the naive method is to allow the forecasts to increase or decrease over time, where the amount of change over time (called the drift, which is the slope of the line between the first and last obeservation) is set to be the average change seen in the historical data. Thus the forecast for time T+h is given by: \\[ y_{T+h} - y_{T} = \\frac{y_{T} - y_1}{T - 1}[(T + h) - T) \\] \\[ y_{T+h} = y_T + h(\\frac{y_{T} - y_1}{T - 1}) \\] bricks %&gt;% model(NAIVE(Bricks ~ drift())) %&gt;% forecast(h = &quot;10 years&quot;) %&gt;% autoplot(bricks, level = NULL) 5.2.5 Australian quarterly beer production beer_fit &lt;- aus_production %&gt;% filter_index(&quot;1992 Q1&quot; ~ &quot;2006 Q4&quot;) %&gt;% model(Mean = MEAN(Beer), naive = NAIVE(Beer), seasonal_naive = SNAIVE(Beer)) beer_fit %&gt;% forecast(h = 14) %&gt;% # 14 quarters autoplot(aus_production %&gt;% filter_index(&quot;1992 Q1&quot; ~ &quot;2006 Q4&quot;), level = NULL) + autolayer(aus_production %&gt;% filter_index(&quot;2007 Q1&quot; ~ .), color = &quot;black&quot;) In this case, only the seasonal naïve forecasts are close to the observed values from 2007 onwards. 5.2.6 Example: Google’s daily closing stock price # Re-index based on trading days google_stock &lt;- gafa_stock %&gt;% filter(Symbol == &quot;GOOG&quot;) %&gt;% mutate(day = row_number()) %&gt;% update_tsibble(index = day, regular = TRUE) # Filter the year of interest google_2015 &lt;- google_stock %&gt;% filter(year(Date) == 2015) # Fit the models google_fit &lt;- google_2015 %&gt;% model( Mean = MEAN(Close), naive = NAIVE(Close), drift = NAIVE(Close ~ drift()) ) # Produce forecasts for the 19 trading days in January 2015 google_fc &lt;- google_fit %&gt;% forecast(h = 19) # A better way using a tsibble to determine the forecast horizons google_jan_2016 &lt;- google_stock %&gt;% filter(yearmonth(Date) == yearmonth(&quot;2016 Jan&quot;)) google_fc &lt;- google_fit %&gt;% forecast(google_jan_2016) # Plot the forecasts google_fc %&gt;% autoplot(google_2015, level = NULL) + autolayer(google_jan_2016, Close, color=&#39;black&#39;) + ggtitle(&quot;Google stock (daily ending 31 Dec 2015)&quot;) + xlab(&quot;Day&quot;) + ylab(&quot;Closing Price (US$)&quot;) + guides(colour=guide_legend(title=&quot;Forecast&quot;)) Sometimes one of these simple methods will be the best forecasting method available; but in many cases, these methods will serve as benchmarks rather than the method of choice. That is, any forecasting methods we develop will be compared to these simple methods to ensure that the new method is better than these simple alternatives. If not, the new method is not worth considering. 5.3 Fitted values and residuals Each observation in a time series can be forecast using all previous observations. We call these fitted values and they are denoted by \\(\\hat{y}_{t|t−1}\\), meaning the forecast of yt based on observations \\(y_1,\\dots,y_t−1\\) . We use these so often, we sometimes drop part of the subscript and just write \\(\\hat{y}_t\\) instead of \\(\\hat{y}_{t|t−1}\\). Fitted values always involve one-step forecasts. Actually, fitted values are often not true forecasts because any parameters involved in the forecasting method are estimated using all available observations in the time series, including future observations. For example, if we use the average method, the fitted values are given by \\[ \\hat{y}_t= \\hat{c} \\] where \\(\\hat{c} = \\frac{1}{T}\\sum_{t = 1}^{T}y_t\\), meaning that it is computed across all available observations, including those at times after \\(t\\). Similarly, for the drift method, the drift parameter is estimated using all available observations. In this case, the fitted values are given by \\[ \\hat{y}_t = y_{t-1} + \\hat{c} \\] where \\(\\hat{c} = \\frac{y_T - y_1}{T-1}\\), the “overall slope”. In both cases, there is a parameter to be estimated from the data. The “hat \\(\\hat{}\\)” above the \\(c\\) reminds us that this is an estimate. When the estimate of \\(c\\) involves observations after time \\(t\\), the fitted values are not true forecasts. On the other hand, naïve or seasonal naïve forecasts do not involve any parameters, and so fitted values are true forecasts in such cases. 5.3.1 Residuals \\[ e_t = y_t - \\hat{y}_t \\] augment(beer_fit) #&gt; # A tsibble: 180 x 6 [1Q] #&gt; # Key: .model [3] #&gt; .model Quarter Beer .fitted .resid .innov #&gt; &lt;chr&gt; &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Mean 1992 Q1 443 436. 6.55 6.55 #&gt; 2 Mean 1992 Q2 410 436. -26.4 -26.4 #&gt; 3 Mean 1992 Q3 420 436. -16.4 -16.4 #&gt; 4 Mean 1992 Q4 532 436. 95.6 95.6 #&gt; 5 Mean 1993 Q1 433 436. -3.45 -3.45 #&gt; 6 Mean 1993 Q2 421 436. -15.4 -15.4 #&gt; # ... with 174 more rows 5.4 Residual diagnostics A good forecasting method will yield residuals with the following properties: The residuals are uncorrelated. If there are correlations between residuals, then there is information left in the residuals which should be used in computing forecasts. The residuals have zero mean(i.e., \\(E(e_t) = 0\\). If the residuals have a mean other than zero, then the forecasts are biased. Any forecasting method that does not satisfy these properties can be improved. However, that does not mean that forecasting methods that satisfy these properties cannot be improved. It is possible to have several different forecasting methods for the same data set, all of which satisfy these properties. Checking these properties is important in order to see whether a method is using all of the available information, but it is not a good way to select a forecasting method. If either of these properties is not satisfied, then the forecasting method can be modified to give better forecasts. Adjusting for bias is easy: if the residuals have mean \\(m\\) , then simply add \\(m\\) to all forecasts and the bias problem is solved. Fixing the correlation problem is harder, and we will not address it until Chapter 10. In addition to these essential properties, it is useful (but not necessary) for the residuals to also have the following two properties. The residuals have constant variance. The residuals are normally distributed. These two properties make the calculation of prediction intervals easier (see Section 5.5 for an example). However, a forecasting method that does not satisfy these properties cannot necessarily be improved. Sometimes applying a Box-Cox transformation may assist with these properties, but otherwise there is usually little that you can do to ensure that your residuals have constant variance and a normal distribution. Instead, an alternative approach to obtaining prediction intervals is necessary. Again, we will not address how to do this until later in the book. 5.4.1 White noise A time series is (discrete) white noise if its values are (see mathematical definition in Section 9.1.1): independent identically distributed with a mean of zero Using time series terms, a white noise should have mean 0, no autocorrelation and no seasonality. We often assume a more stringent form of white noise, that is Gaussian white noise, see Section 9.1.2 for JB normality test: \\[ \\varepsilon_t \\stackrel{iid}{\\sim} N(0, \\sigma^2) \\] # simulate Gaussian and other white noise set.seed(2020) y &lt;- tsibble(sample = 1:50, wn = rnorm(50), index = sample) z &lt;- tsibble(sample = 1:50, wn = 2 * rbinom(50, size = 1, prob = 0.5) -1 , index = sample) y %&gt;% autoplot() + ggtitle(&quot;Gaussian white noise&quot;) z %&gt;% autoplot() + ggtitle(&quot;Other white noise&quot;) The check for autocorrelation wrap_plots( y %&gt;% gg_lag(geom = &quot;point&quot;) + ggtitle(&quot;Lag plot forGaussian white noise&quot;), y %&gt;% ACF() %&gt;% autoplot() + ggtitle(&quot;ACF plot for Gaussian white noise&quot;), z %&gt;% gg_lag(geom = &quot;point&quot;) + ggtitle(&quot;Lag plot for Other white noise&quot;), z %&gt;% ACF() %&gt;% autoplot() + ggtitle(&quot;ACF plot Other white noise&quot;) ) Since white noise are uncoorelated, we expect each autocorrelation coefficient of any order to be close to zero. Of course, they will not be exactly equal to zero as there is some random variation. For a white noise series, we expect \\(95%\\) of the spikes in the ACF plot to lie within \\(±2/\\sqrt{T}\\) where \\(T\\) is the length of the time series. It is common to plot these bounds on a graph of the ACF (the blue dashed lines above). If one or more large spikes are outside these bounds, or if substantially more than \\(5\\%\\) of spikes are outside these bounds, then the series is probably not white noise. In this example, \\(T = 50\\) and the bounds are at \\(±2/\\sqrt{50} = \\pm 0.28\\). All of the autocorrelation coefficients lie within these limits, confirming that the data are white noise. Why white noise matters? Researchers generally assume that white noise is not predictable, and contains no information related to the response of interest. It follows that we would expect that residuals computed according to a specific model to be white noise. This means the valuable part of model extract information from the data to such an extent that we do not care what’s left. If residuals \\(\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_t\\) violates any of the rules of white noise, then there is still valuable information buried under the residuals, which we will improve our models to capture. We have not a ‘best’ model at hand until residuals become unpredictable, in other words, white noise. 5.4.2 Example: Forecasting the Google daily closing stock price We will continue with the Google daily closing stock price example from the previous chapter. For stock market prices and indexes, the best forecasting method is often the naïve method. That is, each forecast is simply equal to the last observed value, or \\(\\hat{y}_t = y_{t−1}\\). Hence, the residuals are simply equal to the difference between consecutive observations: \\[ e_t = y_t - \\hat{y}_t = y_t - y_{t-1} \\] The following graph shows the Google daily closing stock price for trading days during 2015. The large jump corresponds to 17 July 2015 when the price jumped 16% due to unexpectedly strong second quarter results. google_2015 %&gt;% autoplot(Close) + labs(x = &quot;Day&quot;, y = &quot;Closing Price (US$)&quot;, title = &quot;Google Stock in 2015&quot;) Remeber what an ideal set of residuals shoule be like: they should have mean 0 and constant variance, and behave uncorrelated. In most cases we also want residuals to be normally distributed: aug &lt;- google_2015 %&gt;% model(NAIVE(Close)) %&gt;% augment() # mean, variance aug %&gt;% autoplot(.resid) + labs(title = &quot;Residuals from naïve method&quot;, x = &quot;Day&quot;, y = &quot;&quot;) # distribution aug %&gt;% ggplot() + geom_histogram(aes(.resid)) # correlation aug %&gt;% ACF(.resid) %&gt;% autoplot() Shorthand function gg_tsresiduals(): google_2015 %&gt;% model(naive = NAIVE(Close)) %&gt;% gg_tsresiduals() A qq plot to detect distribution: aug %&gt;% ggplot(aes(sample = .resid)) + stat_qq() + stat_qq_line() Some useful functions for general diagonisis from the performance are mentioned in Section 7.3.4 5.4.3 Portmanteau tests for autocorrelation In order to evaluate estimated time series models, it is important to know whether the residuals of the model really have the properties of a white noise, in particular, whether they are uncorrelated. Thus, the null hypothesis to be tested is \\[ H_0 : r_k = 0, k = 1, 2, ... \\] In order to build formal statistical tests to overcome undue reliance on ACF plots, we test whether the first \\(h\\) autocorrelations are significantly different from what would be expected from a white noise process. A test for a group of autocorrelations is called a portmanteau test, from a French word describing a suitcase containing a number of items. One such test is the Box-Pierce test, based on the following statistic: \\[ Q = T \\sum_{k = 1}^{h}{r^2_k} \\] where \\(h\\) is the maximum lag being considered and \\(T\\) is the number of observations. If each \\(r_k\\) is close to zero, then \\(Q\\) will be small. If some \\(r_k\\) values are large (positive or negative), then \\(Q\\) will be large. It is suggested that use \\(h = 10\\) for non-seasonal data and \\(h = 2m\\) for seasonal data, where \\(m\\) is the period of seasonality. However, the test is not good when \\(h\\) is large, so if these values are larger than \\(T/5\\), then use \\(h=T/5\\). A related (and more accurate) test is the Ljung-Box test, based on \\[ Q^* = T(T + 2)\\sum_{k = 1}^{h}{(T-K)^{-1}}r_k^2 \\] If the autocorrelations did come from a white noise series, then both \\(Q\\) and \\(Q^*\\) follows a distribution of \\(\\chi^2_{h - K}\\) where \\(K\\) is the number of parameters in the model. For model where there is no parameter to estimate (such as naive model), we simply set \\(K = 0\\). For a regular time series rather than residuals that come from a particular model, we also set \\(K = 0\\). Note that in ljung_box(), argument dof specifies K, rather than h - K # lag = h and dof = K aug %&gt;% features(.resid, box_pierce, lag = 10, dof = 0) #&gt; # A tibble: 1 x 4 #&gt; Symbol .model bp_stat bp_pvalue #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 GOOG NAIVE(Close) 7.74 0.654 aug %&gt;% features(.resid, ljung_box, lag = 10, dof = 0) #&gt; # A tibble: 1 x 4 #&gt; Symbol .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 GOOG NAIVE(Close) 7.91 0.637 For both \\(Q\\) and \\(Q^∗\\), the results are not significant. Thus, we can conclude that the residuals are not distinguishable from a white noise series. To see details of these tests, see Section 9.1.2 5.5 Prediction intervals A prediction interval gives an interval within which we expect \\(y_t\\) to lie with a specified probability. For example, assuming that the residuals are normally distributed, a 95% prediction interval for the h-step forecast is : \\[ y_{T+h | h} \\pm 1.96\\hat{\\sigma}_h \\] where \\(\\hat{\\sigma}_h\\) is an estimate of the standard deviation of the h-step forecast distribution. More generally, a prediction interval can be written as : \\[ y_{T+h | h} \\pm c\\hat{\\sigma}_h \\] where \\(c\\) depends on the coverage probability (i.e., \\(Z_\\alpha\\) when \\(1 - \\alpha\\) means the critical level and residuals are normally distributed). 5.5.1 One-step prediction intervals When forecasting one step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals. (In fact, the two standard deviations are identical if there are no parameters to be estimated, as is the case with the naïve method. For forecasting methods involving parameters to be estimated, the standard deviation of the forecast distribution is slightly larger than the residual standard deviation, although this difference is often ignored.) So for a naive model’s prediction interval at \\(y_{T+1}\\) \\[ \\hat{y}_{T + 1} \\pm Z_\\alpha\\hat{\\sigma} \\] where \\[ \\hat{\\sigma} = \\sqrt{\\frac{1}{n}\\sum_{t = 1}^{T}{R_t^2}} \\] 5.5.2 Multi-step prediction intervals A common feature of prediction intervals is that they increase in length as the forecast horizon increases. The further ahead we forecast, the more uncertainty is associated with the forecast, and thus the wider the prediction intervals. That is, σh usually increases with \\(h\\) (although there are some non-linear forecasting methods that do not have this property). To produce a prediction interval, it is necessary to have an estimate of \\(\\sigma_h\\) . As already noted, for one-step forecasts (\\(h = 1\\)), the residual standard deviation provides a good estimate of the forecast standard deviation \\(\\sigma_1\\). For multi-step forecasts, a more complicated method of calculation is required. These calculations assume that the residuals are uncorrelated. For the four benchmark methods, it is possible to mathematically derive the forecast standard deviation under the assumption of uncorrelated residuals. If \\(\\sigma_h\\) denotes the standard deviation of the h-step forecast distribution, and \\(\\hat{\\sigma}\\) is the residual standard deviation, then we can use the following expressions. Note that when \\(h = 1\\) and T is large, these all give the same approximate value \\(\\hat{\\sigma}\\). Prediction intervals can easily be computed for you when using the fable package. For example, here is the output when using the naïve method for the Google stock price. google_2015 %&gt;% model(NAIVE(Close)) %&gt;% forecast(h = 10) %&gt;% hilo(level = 95) #&gt; # A tsibble: 10 x 6 [1] #&gt; # Key: Symbol, .model [1] #&gt; Symbol .model day Close .mean `95%` #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dist&gt; &lt;dbl&gt; &lt;hilo&gt; #&gt; 1 GOOG NAIVE(Close) 505 N(759, 125) 759. [736.9488, 780.8112]95 #&gt; 2 GOOG NAIVE(Close) 506 N(759, 250) 759. [727.8646, 789.8954]95 #&gt; 3 GOOG NAIVE(Close) 507 N(759, 376) 759. [720.8941, 796.8659]95 #&gt; 4 GOOG NAIVE(Close) 508 N(759, 501) 759. [715.0176, 802.7424]95 #&gt; 5 GOOG NAIVE(Close) 509 N(759, 626) 759. [709.8404, 807.9196]95 #&gt; 6 GOOG NAIVE(Close) 510 N(759, 751) 759. [705.1598, 812.6002]95 #&gt; # ... with 4 more rows The hilo() function converts the forecast distributions into intervals. By default, 80% and 95% prediction intervals are returned, here I ask only for a 95% interval via the level argument. 5.5.3 Prediction intervals from bootstrapped residuals Assuming future errors will be similar to past errors, we can replace \\(e_{T+1}, e_{T+2}, \\dots\\) by sampling from the collection of errors we have seen in the past (i.e., the residuals). Thus we could get “future” \\(y_{T+1}, y_{T+2}, \\dots\\) by \\(\\hat{y}_{T+1} + e_{T+1}, \\hat{y}_{T+2} + e_{T+2}, \\dots\\). Since sampling could generate different sets of future residuals, we thus get different paths of \\(y_{T+1}, y_{T+2}, \\dots\\). And we can then derive the \\(1 - \\alpha\\) prediction interval simply by calculating a correspoing percentile based on each \\(\\hat{y}_{T+1}, \\hat{y}_{T+2}, \\dots\\) and numerous simulated path. generate() lets us see some possible futures: google_fit &lt;- google_2015 %&gt;% model(NAIVE(Close)) sim &lt;- google_fit %&gt;% generate(h = 30, times = 5, bootstrap = TRUE) sim #&gt; # A tsibble: 150 x 5 [1] #&gt; # Key: Symbol, .model, .rep [5] #&gt; Symbol .model day .rep .sim #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 GOOG NAIVE(Close) 505 1 724. #&gt; 2 GOOG NAIVE(Close) 506 1 722. #&gt; 3 GOOG NAIVE(Close) 507 1 700. #&gt; 4 GOOG NAIVE(Close) 508 1 696. #&gt; 5 GOOG NAIVE(Close) 509 1 696. #&gt; 6 GOOG NAIVE(Close) 510 1 698. #&gt; # ... with 144 more rows Here we have generated five possible sample paths for the next 30 trading days. The .rep variable provides a new key for the tsibble. The plot below shows the five sample paths along with the historical data. ggplot(google_2015) + geom_line(aes(day, Close)) + geom_line(aes(day, .sim, color = factor(.rep)), data = sim) + ggtitle(&quot;Google closing stock price&quot;) + guides(col = FALSE) This is all built into the forecast() function so we do not need to call generate() directly: google_bootstrap_fc &lt;- google_fit %&gt;% forecast(h = 30, bootstrap = TRUE) google_bootstrap_fc #&gt; # A fable: 30 x 5 [1] #&gt; # Key: Symbol, .model [1] #&gt; Symbol .model day Close .mean #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dist&gt; &lt;dbl&gt; #&gt; 1 GOOG NAIVE(Close) 505 sample[5000] 759. #&gt; 2 GOOG NAIVE(Close) 506 sample[5000] 759. #&gt; 3 GOOG NAIVE(Close) 507 sample[5000] 759. #&gt; 4 GOOG NAIVE(Close) 508 sample[5000] 759. #&gt; 5 GOOG NAIVE(Close) 509 sample[5000] 759. #&gt; 6 GOOG NAIVE(Close) 510 sample[5000] 759. #&gt; # ... with 24 more rows Notice that the forecast distribution is now represented as a simulation with 5000 sample paths. Because there is no normality assumption, the prediction intervals are not symmetric. google_bootstrap_fc %&gt;% autoplot(google_2015) + ggtitle(&quot;Bootstrapped prediction interval&quot;) 5.6 Evaluating model accuracy 5.6.1 Forecast errors \\[ e_{T+h} = y_{T+h} - \\hat{y}_{T+h | T} \\] where \\({y_1\\, \\dots, y_T}\\) is the traing set and \\({y_{T+1}, y_{T+2}\\dots}\\) the testing set. 5.6.2 Scale dependent errors The two most commonly used scale-dependent measures are based on the absolute errors or squared errors: \\[ \\begin{align} Mean \\,absolute \\,error: MAE &amp;= mean(|e_t|) \\\\ Root \\,square \\,mean \\,error: RMSE &amp;= \\sqrt{mean(e_t^2)} \\\\ \\end{align} \\] 5.6.3 Percentage errors The percentage error is given by \\(p_t = 100e_t/y_t\\). Percentage errors have the advantage of being unit-free, and so are frequently used to compare forecast performances between data sets. The most commonly used measure is: \\[ Mean \\,absolute \\,error: MAE = mean(|p_t|) \\] Measures based on percentage errors have the disadvantage of being infinite or undefined if \\(y_t = 0\\) for any \\(t\\) in the period of interest, and having extreme values if any \\(y_t\\) is close to zero. Another problem with percentage errors that is often overlooked is that they assume the unit of measurement has a meaningful zero. For example, a percentage error makes no sense when measuring the accuracy of temperature forecasts on either the Fahrenheit or Celsius scales, because temperature has an arbitrary zero point. They also have the disadvantage that they put a heavier penalty on negative errors than on positive errors. This observation led to the use of the so-called “symmetric” MAPE (sMAPE) which was used in the M3 forecasting competition. It is defined by \\[ sMAPE = mean(200|y_t - \\hat{y}_t| / (y_t + \\hat{y}_t)) \\] However, if \\(y_t\\) is close to zero, \\(\\hat{y}_t\\) is also likely to be close to zero. Thus, the measure still involves division by a number close to zero, making the calculation unstable. Also, the value of sMAPE can be negative, so it is not really a measure of “absolute percentage errors” at all. Hyndman &amp; Koehler (2006) recommend that the sMAPE not be used. It is included here only because it is widely used, although it is not used in this book. 5.6.4 Scaled errors Scaled errors were proposed by Hyndman &amp; Koehler as an alternative to using percentage errors when comparing forecast accuracy across series with different units. They proposed scaling the errors based on the training MAE from a simple forecast method. For a non-seasonal time series, a useful way to define a scaled error uses naive forecasts: \\[ q_j = \\frac{e_j}{\\frac{1}{T-1}\\sum_{t = 2}^{T}{|y_t - y_{t-1}|}} \\] Because the numerator and denominator both involve values on the scale of the original data, \\(q_j\\) is independent of the scale of the data. A scaled error is less than one if it arises from a better forecast than the average naive forecast computed on the training data. Conversely, it is greater than one if the forecast is worse than the average naive forecast computed on the training data. For seasonal time series, a scaled error can be defined using seasonal naive forecasts: \\[ q_j = \\frac{e_j}{\\frac{1}{T-m}\\sum_{t = m+1}^{T}{|y_t - y_{t-m}|}} \\] The mean absolute scaled error is simply \\[ MASE = mean(|q_j|) \\] 5.6.5 Examples: beer production recent_production &lt;- aus_production %&gt;% filter(year(Quarter) &gt;= 1992) beer_train &lt;- recent_production %&gt;% filter(year(Quarter) &lt;= 2007) beer_fit &lt;- beer_train %&gt;% model( Mean = MEAN(Beer), `Naïve` = NAIVE(Beer), `Seasonal naïve` = SNAIVE(Beer), Drift = RW(Beer ~ drift()) ) beer_fc &lt;- beer_fit %&gt;% forecast(h = 10) beer_fc %&gt;% autoplot(filter(aus_production, year(Quarter) &gt;= 1992), level = NULL) + xlab(&quot;Year&quot;) + ylab(&quot;Megalitres&quot;) + ggtitle(&quot;Forecasts for quarterly beer production&quot;) + guides(colour=guide_legend(title = &quot;Models&quot;)) accuracy(beer_fc, recent_production) #&gt; # A tibble: 4 x 9 #&gt; .model .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Drift Test -54.0 64.9 58.9 -13.6 14.6 4.12 -0.0741 #&gt; 2 Mean Test -13.8 38.4 34.8 -3.97 8.28 2.44 -0.0691 #&gt; 3 Naïve Test -51.4 62.7 57.4 -13.0 14.2 4.01 -0.0691 #&gt; 4 Seasonal naïve Test 5.2 14.3 13.4 1.15 3.17 0.937 0.132 It is obvious from the graph that the seasonal naïve method is best for these data, although it can still be improved, as we will discover later. Sometimes, different accuracy measures will lead to different results as to which forecast method is best. However, in this case, all of the results point to the seasonal naïve method as the best of these three methods for this data set. To take a non-seasonal example, consider the Google stock price. The following graph shows the closing stock prices from 2015, along with forecasts for January 2016 obtained from three different methods. google_fit &lt;- google_2015 %&gt;% model( Mean = MEAN(Close), `Naïve` = NAIVE(Close), Drift = RW(Close ~ drift()) ) google_jan_2016 &lt;- google_stock %&gt;% filter(yearmonth(Date) == yearmonth(&quot;2016 Jan&quot;)) google_fc &lt;- google_fit %&gt;% forecast(google_jan_2016) google_fc %&gt;% autoplot(rbind(google_2015, google_jan_2016), level = NULL) + ggtitle(&quot;Google stock price (daily ending 6 Dec 13)&quot;) + guides(colour=guide_legend(title=&quot;Forecast&quot;)) Check model accuracy accuracy(google_fc, google_stock) #&gt; # A tibble: 3 x 10 #&gt; .model Symbol .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Drift GOOG Test -49.8 53.1 49.8 -6.99 6.99 7.84 0.604 #&gt; 2 Mean GOOG Test 117. 118. 117. 16.2 16.2 18.4 0.496 #&gt; 3 Naïve GOOG Test -40.4 43.4 40.4 -5.67 5.67 6.36 0.496 Here, the best method is the naïve method (regardless of which accuracy measure is used). 5.7 Time series cross-validation A more sophisticated version of training/test sets is time series cross-validation. In this procedure, there are a series of test sets, each consisting of a single observation. The corresponding training set consists only of observations that occurred prior to the observation that forms the test set. Thus, no future observations can be used in constructing the forecast. Since it is not possible to obtain a reliable forecast based on a small training set, the earliest observations are not considered as test sets. The forecast accuracy is computed by averaging over the test sets. With time series forecasting, one-step forecasts may not be as relevant as multi-step forecasts. In this case, the cross-validation procedure based on a rolling forecasting origin can be modified to allow multi-step errors to be used. Suppose that we are interested in models that produce good 4-step-ahead forecasts. Then the corresponding diagram is shown below. stretch_tsibble() generates multiple folds as we specified: (may subject to change, I havn’t found a corresponding function in the slider package, perhaps vfold_cv from rsample in the future?) # demonstrate `stretch_tsibble()` on a small subset google_2015 %&gt;% slice(1:7) %&gt;% stretch_tsibble(.init = 3, .step = 1) %&gt;% # initial window size = 3, incremental step = 1 as_tibble() #&gt; # A tibble: 25 x 10 #&gt; Symbol Date Open High Low Close Adj_Close Volume day .id #&gt; &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 GOOG 2015-01-02 526. 528. 521. 522. 522. 1447600 253 1 #&gt; 2 GOOG 2015-01-05 520. 521. 510. 511. 511. 2059800 254 1 #&gt; 3 GOOG 2015-01-06 512. 513. 498. 499. 499. 2899900 255 1 #&gt; 4 GOOG 2015-01-02 526. 528. 521. 522. 522. 1447600 253 2 #&gt; 5 GOOG 2015-01-05 520. 521. 510. 511. 511. 2059800 254 2 #&gt; 6 GOOG 2015-01-06 512. 513. 498. 499. 499. 2899900 255 2 #&gt; # ... with 19 more rows google_2015_cv &lt;- google_2015 %&gt;% slice(1:(n()-1)) %&gt;% stretch_tsibble(.init = 3, .step = 1) google_cv_fc &lt;- google_2015_cv %&gt;% model(RW(Close ~ drift())) %&gt;% forecast(h = 1) # for one step forecast, no need to specify &quot;by&quot; in `accuracy()` google_cv_fc %&gt;% accuracy(google_2015) #&gt; # A tibble: 1 x 10 #&gt; .model Symbol .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 RW(Close ~ drift()) GOOG Test 0.726 11.3 7.26 0.112 1.19 1.02 0.0985 # residual accuracy google_2015 %&gt;% model(RW(Close ~ drift())) %&gt;% accuracy() #&gt; # A tibble: 1 x 10 #&gt; Symbol .model .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 GOOG RW(Close ~ dri~ Train~ -2.97e-14 11.1 7.16 -0.0267 1.18 1.00 0.0976 As expected, the accuracy measures from the residuals are smaller, as the corresponding “forecasts” are based on a model fitted to the entire data set, rather than being true forecasts. 5.7.1 Example: Forecast horizon accuracy with cross-validation The code below evaluates the forecasting performance of 1- to 8-step-ahead drift forecasts. The plot shows that the forecast error increases as the forecast horizon increases, as we would expect. google_2015_tr &lt;- google_2015 %&gt;% slice(1:(n() - 8)) %&gt;% stretch_tsibble(.init = 3, .step = 1) fc &lt;- google_2015_tr %&gt;% model(RW(Close ~ drift())) %&gt;% forecast(h = 8) %&gt;% group_by(.id) %&gt;% mutate(h = row_number()) %&gt;% ungroup() # by indicates accuracy should be averaged over same &quot;h&quot; fc %&gt;% accuracy(google_2015, by = &quot;h&quot;) #&gt; # A tibble: 8 x 9 #&gt; h .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 Test 0.729 11.3 7.26 0.113 1.20 1.02 0.0961 #&gt; 2 2 Test 1.47 16.9 11.0 0.223 1.83 1.54 0.515 #&gt; 3 3 Test 2.21 20.8 14.0 0.334 2.34 1.97 0.671 #&gt; 4 4 Test 2.96 23.9 16.0 0.451 2.66 2.26 0.749 #&gt; 5 5 Test 3.79 26.5 17.7 0.580 2.95 2.51 0.791 #&gt; 6 6 Test 4.66 28.8 19.4 0.713 3.22 2.74 0.826 #&gt; # ... with 2 more rows fc %&gt;% accuracy(google_2015, by = &quot;h&quot;) %&gt;% ggplot(aes(x = h, y = RMSE)) + geom_point() 5.8 Forecasting using transformations Box cox transformation is mentioned at 3.1.4. When forecasting from a model with transformations, we first produce forecasts of the transformed data. Then, we need to reverse the transformation (or back-transform) to obtain forecasts on the original scale. The reverse Box-Cox transformation is given by \\[\\begin{equation} y_t = \\begin{cases} \\exp{(w_t)} &amp; \\text{if} \\;\\lambda = 0 \\\\ (\\lambda w_t + 1)^{1/\\lambda} &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] 5.8.1 Prediction intervals with transformations If a transformation has been used, then the prediction interval is first computed on the transformed scale, then the end points are back-transformed to give a prediction interval on the original scale. This approach preserves the probability coverage of the prediction interval, although it will no longer be symmetric around the point forecast. The back-transformation of prediction intervals is done automatically for fable models, provided that you have used a transformation in the model formula. Transformations sometimes make little difference to the point forecasts but have a large effect on prediction intervals. 5.8.2 Forecasting with constraints One common use of transformations is to ensure the forecasts remain on the appropriate scale. For example, log transformations constrain the forecasts to stay positive (because the back transformation is \\(\\exp(w_t)\\)). Another useful transformation is the scaled logit, which can be used to ensure that the forecasts are kept within a specific interval. A scaled logit that ensures the forecasted values are between \\(a\\) and \\(b\\) (where \\(a &lt; b\\)) is given by: \\[ w_t = f(y_t) = \\log{\\frac{y_t -a}{b - y_t}} \\] Inverting this transformation gives the appropriate back-transformation of: \\[ y_t = \\frac{a + be^{w_t}}{1 + e^{w_t}} = \\frac{(b - a)e^{w_t}}{1 + e^{w_t}} + a \\] To use this transformation when modelling, we can create a new transformation with the new_transformation() function, which is essentially a function factory. This allows us to define two functions that accept the same parameters, where the observations are provided as the first argument. The first the argument of new_transformation(), transformation shoule be a function taht is used to transform the data, the second inverse is used to back-transform forecasts: scaled_logit &lt;- new_transformation( transformation = function(x, lower = 0, upper = 1) { log(x - lower) / (upper - x) }, inverse = function(x, lower = 0, upper = 1) { (upper - lower) * exp(x) / (1 + exp(x)) + lower } ) Take the beer data for example, suppose we want to constrain the forecast production of beer into a \\((300, 400)\\) interval: beer_production &lt;- aus_production %&gt;% filter_index(&quot;1992 Q1&quot; ~ &quot;2006 Q4&quot;) %&gt;% select(Quarter, Beer) beer_production %&gt;% model(SNAIVE(scaled_logit(Beer, 300, 400))) %&gt;% forecast(h = 8) %&gt;% autoplot(beer_production, level = NULL) + ggtitle(&quot;Scaled logit transformation&quot;) beer_production %&gt;% model(SNAIVE(Beer)) %&gt;% forecast(h = 8) %&gt;% autoplot(beer_production, level = NULL) + ggtitle(&quot;No transformation&quot;) 5.8.3 Bias adjustments One issue with using mathematical transformations such as Box-Cox transformations is that the back-transformed point forecast will not be the mean of the forecast distribution. In fact, it will usually be the median of the forecast distribution (assuming that the distribution on the transformed space is symmetric). For many purposes, this is acceptable, but occasionally the mean forecast is required. For example, you may wish to add up sales forecasts from various regions to form a forecast for the whole country. But medians do not add up, whereas means do. (the mean of multiple means is the mean of the whole) For a Box-Cox transformation, the back-transformed mean is given by: \\[\\begin{equation} y_t = \\tag{5.1} \\begin{cases} \\exp{(w_t)}[1 + \\frac{\\sigma_h^2}{2}] &amp; \\text{if} \\; \\lambda = 0 \\\\ (\\lambda w_t + 1)^{1/\\lambda}[1 + \\frac{\\sigma_h^2{1 - \\lambda}}{2(\\lambda w_t + 1)^2}] &amp; \\text{otherwise} \\end{cases} \\end{equation}\\] where \\(\\sigma_h^2\\) is the h-step forecast variance on the transformed scale. The larger the forecast variance, the bigger the difference between the mean and the median. Estimation of \\(\\sigma_h^2\\) is mentioned in 5.5 The difference between the simple back-transformed forecast given by Equation (3.1) and the mean by equation (5.1) is called the bias. When we use the mean, rather than the median, we say the point forecasts have been bias-adjusted. eggs &lt;- fma::eggs %&gt;% as_tsibble() eggs #&gt; # A tsibble: 94 x 2 [1Y] #&gt; index value #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1900 277. #&gt; 2 1901 315. #&gt; 3 1902 315. #&gt; 4 1903 321. #&gt; 5 1904 315. #&gt; 6 1905 318. #&gt; # ... with 88 more rows eggs_fit &lt;- eggs %&gt;% model(RW(log(value) ~ drift())) eggs_fc &lt;- eggs_fit %&gt;% forecast(h = 50) eggs_fc_biased &lt;- eggs_fit %&gt;% forecast(h = 50) eggs %&gt;% autoplot(value) + autolayer(eggs_fc, color = &quot;red&quot;, level = NULL) + autolayer(eggs_fc_biased, level = NULL, point_forecast = list(.median = median)) The blue line in shows the forecast medians while the red line shows the forecast means. Notice how the skewed forecast distribution pulls up the point forecast when we use the bias adjustment. Bias adjustments will be applied by default in the fable package. To produce point forecasts that are not bias adjusted (giving forecast medians rather than means), check the argument point_forecast in forecast(). 5.9 Forecasting with decomposition Times series decomposition (discussed in Chpater 3) can be a useful step in producing forecasts. Assuming an additive decomposition, the decomposed time series can be written as \\[ y_t = \\hat{A}_t + \\hat{S}_t \\] where \\(\\hat{A}_t = \\hat{T}_t + \\hat{R}_t\\) is the seasonally adjusted component. To forecast a decomposed time series, we forecast the seasonal component, \\(\\hat{S}_t\\), and the seasonally adjusted component \\(\\hat{A}_t\\), separately. It is usually assumed that the seasonal component is unchanging, or changing extremely slowly, so it is forecast by simply taking the last year of the estimated component. In other words, a seasonal naïve method is used for the seasonal component. To forecast the seasonally adjusted component, any non-seasonal forecasting method may be used. For example, a random walk with drift model, or Holt’s method (discussed in Chapter 8), or a non-seasonal ARIMA model (discussed in Chapter 9), may be used. 5.9.1 Example: Employment in the US retail sector us_retail_employment &lt;- fpp3::us_employment %&gt;% filter(year(Month) &gt;= 1990, Title == &quot;Retail Trade&quot;) retail_employment_dcmp &lt;- us_retail_employment %&gt;% model(STL(Employed ~ trend(window = 7), robust = TRUE)) %&gt;% components() %&gt;% select(-.model) retail_employment_dcmp %&gt;% model(NAIVE(season_adjust)) %&gt;% forecast(h = 10) %&gt;% autoplot(retail_employment_dcmp) + ylab(&quot;New orders index&quot;) + ggtitle(&quot;Naive forecasts of seasonally adjusted data&quot;) This shows naïve forecasts of the seasonally adjusted electrical equipment orders data. These are then “reseasonalised” by adding in the seasonal naïve forecasts of the seasonal component. This is made easy with the decomposition_model() model function, which allows you to compute forecasts via any additive decomposition, using other model functions to forecast each of the decomposition’s components. The first argument of decomposition_model() specifies a decomposition method we want to use, and the rest determines models on resulting components. Seasonal components of the model will be forecasted automatically using SNAIVE() if a different model isn’t specified. The function will also do the reseasonalising for you, ensuring that the resulting forecasts of the original data are shown below us_retail_employment %&gt;% model(stlf = decomposition_model( STL(Employed ~ trend(window = 7), robust = TRUE), RW(season_adjust ~ drift()) )) %&gt;% forecast() %&gt;% autoplot(us_retail_employment) The prediction intervals shown in this graph are constructed in the same way as the point forecasts. That is, the upper and lower limits of the prediction intervals on the seasonally adjusted data are “reseasonalised” by adding in the forecasts of the seasonal component. As well as the drift method, any other appropriate forecasting method can be used to model the seasonally adjusted component. "],
["judgmental-forecasts.html", "Chapter 6 Judgmental forecasts 6.1 Beware of limitations 6.2 Key principles 6.3 The Delphi method 6.4 Forecasting by analogy 6.5 Scenario forecasting 6.6 New product forecasting 6.7 Judgmental adjustments", " Chapter 6 Judgmental forecasts There are three general settings in which judgmental forecasting is used: there are no available data so that statistical methods are not applicable and judgmental forecasting is the only feasible approach data are available, statistical forecasts are generated, and these are then adjusted using judgement data are available and statistical and judgmental forecasts are generated independently and then combined It is important to recognise that judgmental forecasting is subjective and comes with limitations. However, implementing systematic and well-structured approaches can confine these limitations and markedly improve forecast accuracy. Whenever data are available, applying statistical methods (such as those discussed in other chapters of this book), is preferable and should always be used as a starting point. 6.1 Beware of limitations Judgmental forecasts are subjective, and therefore do not come free of bias or limitations. Judgmental forecasts can be inconsistent. Unlike statistical forecasts, which can be generated by the same mathematical formulas every time, judgmental forecasts depend heavily on human cognition, and are vulnerable to its limitations. Judgement can be clouded by personal or political agendas, where targets and forecasts (as defined in Chapter 1) are not segregated. Another undesirable property which is commonly seen in judgmental forecasting is the effect of anchoring. In this case, the subsequent forecasts tend to converge or be close to an initial familiar reference point 6.2 Key principles Set the forecasting task clearly and concisely All definitions should be clear and comprehensive, avoiding ambiguous and vague expressions. Also, it is important to avoid incorporating emotive terms and irrelevant information that may distract the forecaster. Implement a systematic approach Forecast accuracy and consistency can be improved by using a systematic approach to judgmental forecasting involving checklists of categories of information which are relevant to the forecasting task. Document and justify Formalising and documenting the decision rules and assumptions implemented in the systematic approach can promote consistency, as the same rules can be implemented repeatedly. Systematically evaluate forecasts Keep records of forecasts and use them to obtain feedback when the corresponding observations become available. Segregate forecasters and users Forecasters and users should be clearly segregated. A classic case is that of a new product being launched. The forecast should be a reasonable estimate of the sales volume of a new product, which may differ considerably from what management expects or hopes the sales will be in order to meet company financial objectives. In this case, a forecaster may be delivering a reality check to the user. The way in which forecasts may then be used and implemented will clearly depend on managerial decision making. For example, management may decide to adjust a forecast upwards (be over-optimistic), as the forecast may be used to guide purchasing and stock keeping levels. Such a decision may be taken after a cost-benefit analysis reveals that the cost of holding excess stock is much lower than that of lost sales. This type of adjustment should be part of setting goals or planning supply, rather than part of the forecasting process. 6.3 The Delphi method The Delphi method was invented by Olaf Helmer and Norman Dalkey of the Rand Corporation in the 1950s for the purpose of addressing a specific military problem. The method relies on the key assumption that forecasts from a group are generally more accurate than those from individuals. The aim of the Delphi method is to construct consensus forecasts from a group of experts in a structured iterative manner. A facilitator is appointed in order to implement and manage the process. The Delphi method generally involves the following stages: Assemble a panel of experts in various fields concerned Forecasting tasks/challenges are set and distributed to the experts. Experts return initial forecasts and justifications. These are compiled and summarised in order to provide feedback. Feedback is provided to the experts, who now review their forecasts in light of the feedback. This step may be iterated until a satisfactory level of consensus is reached. Final forecasts are constructed by aggregating the experts’ forecasts. 6.3.1 Experts and anonymity The first challenge of the facilitator is to identify a group of experts who can contribute to the forecasting task. The usual suggestion is somewhere between 5 and 20 experts with diverse expertise(maily decided by the facilitator). Experts submit forecasts and also provide detailed qualitative justifications for these. A key feature of the Delphi method is that the participating experts remain anonymous at all times. This means that the experts cannot be influenced by political and social pressures in their forecasts.Anonymity of the experts may be an advantage in not suppressing creativity, but could hinder collaboration. Anonymity also means Delphi method is not constrained by physical location, rendering it economical. 6.3.2 Setting the forecasting task in a Delphi It may be useful to conduct a preliminary round of information gathering from the experts before setting the forecasting tasks. Alternatively, as experts submit their initial forecasts and justifications, valuable information which is not shared between all experts can be identified by the facilitator when compiling the feedback. 6.3.3 Iteration The process of the experts submitting forecasts, receiving feedback, and reviewing their forecasts in light of the feedback, is repeated until a satisfactory level of consensus between the experts is reached. Satisfactory consensus does not mean complete convergence in the forecast value; it simply means that the variability of the responses has decreased to a satisfactory level. Usually two or three rounds are sufficient. Experts are more likely to drop out as the number of iterations increases, so too many rounds should be avoided. 6.3.4 Final forecasts The final forecasts are usually constructed by giving equal weight to all of the experts’ forecasts. However, the facilitator should keep in mind the possibility of extreme values which can distort the final forecast. 6.3.5 Limitations and variations Due to the iteration process, Delphi method can be time-consuming compared to a group meeting. Anonymity of the experts may be an advantage in not suppressing creativity, but could hinder collaboration. Also, In a group setting, personal interactions can lead to quicker and better clarifications of qualitative justifications. A variation of the Delphi method which is often applied is the “estimate-talk-estimate” method, where the experts can interact between iterations, although the forecast submissions can still remain anonymous. A disadvantage of this variation is the possibility of the loudest person exerting undue influence. 6.3.6 The facilitator The facilitator is largely responsible for the design and administration of the Delphi process. The facilitator is also responsible for providing feedback to the experts and generating the final forecasts. In this role, the facilitator needs to be experienced enough to recognise areas that may need more attention, and to direct the experts’ attention to these. Also, as there is no face-to-face interaction between the experts, the facilitator is responsible for disseminating important information. 6.4 Forecasting by analogy A useful judgmental approach which is often implemented in practice is forecasting by analogy. A common example is the pricing of a house through an appraisal process. An appraiser estimates the market value of a house by comparing it to similar properties that have sold in the area. The degree of similarity depends on the attributes considered. With house appraisals, attributes such as land size, dwelling size, numbers of bedrooms and bathrooms, and garage space are usually considered. Alternatively, a structured approach comprising a panel of experts can be implemented. The concept is similar to that of a Delphi; however, the forecasting task is completed by considering analogies . First, a facilitator is appointed. Then the structured approach involves the following steps. A panel of experts who are likely to have experience with analogous situations is assembled. Tasks/challenges are set and distributed to the experts. Experts identify and describe as many analogies as they can, and generate forecasts based on each analogy. Experts list similarities and differences of each analogy to the target situation, then rate the similarity of each analogy to the target situation on a scale. Forecasts are derived by the facilitator using a set rule. This can be a weighted average, where the weights can be guided by the ranking scores of each analogy by the experts. Because of the similarity of fundamental principle and procedure, the analogy method share what the Delphi method benifits and suffers. 6.5 Scenario forecasting A fundamentally different approach to judgmental forecasting is scenario-based forecasting. The aim of this approach is to generate forecasts based on plausible scenarios. In contrast to the two previous approaches (Delphi and forecasting by analogy) where the resulting forecast is intended to be one likely outcome, scenario foreasting is about making multiple parallel futures (i.e., scenarios), each of which may have a low possibility to occur. The scenarios are generated by considering all possible factors or drivers, their relative impacts, the interactions between them, and the targets to be forecast. Building forecasts based on scenarios allows a wide range of possible forecasts to be generated and some extremes to be identified. For example it is usual for “best”, “middle” and “worst” case scenarios to be presented, although many other scenarios will be generated. Thinking about and documenting these contrasting extremes can lead to early contingency planning. With scenario forecasting, decision makers often participate in the generation of scenarios. While this may lead to some biases, it can ease the communication of the scenario-based forecasts, and lead to a better understanding of the results. 6.6 New product forecasting A common use of judemental forecasting is to predict sales of a new product. It may be an entirely new product which has been launched, a variation of an existing product (“new and improved”), a change in the pricing scheme of an existing product, or even an existing product entering a new market. The approaches we have already outlined (Delphi, forecasting by analogy and scenario forecasting) are all applicable when forecasting the demand for a new product. Other methods which are more specific to the situation are also available. We briefly describe three such methods which are commonly applied in practice. These methods are less structured than those already discussed, and are likely to lead to more biased forecasts as a result. Whichever method of new product forecasting is used, it is important to thoroughly document the forecasts made, and the reasoning behind them, in order to be able to evaluate them when data become available. 6.6.1 Sales force composite In this approach, forecasts for each outlet/branch/store of a company are generated by salespeople, and are then aggregated. This usually involves sales managers forecasting the demand for the outlet they manage. Salespeople are usually closest to the interaction between customers and products, and often develop an intuition about customer purchasing intentions. They bring this valuable experience and expertise to the forecast. However, having salespeople generate forecasts violates the key principle of segregating forecasters and users, which can create biases in many directions. 6.6.2 Executive opinion In contrast to the sales force composite, this approach involves staff at the top of the managerial structure generating aggregate forecasts. Such forecasts are usually generated in a group meeting, where executives contribute information from their own area of the company. Having executives from different functional areas of the company promotes great skill and knowledge diversity in the group. This process carries all of the advantages and disadvantages of a group meeting setting which we discussed earlier. In this setting, it is important to justify and document the forecasting process. That is, executives need to be held accountable in order to reduce the biases generated by the group meeting setting. There may also be scope to apply variations to a Delphi approach in this setting; for example, the estimate-talk-estimate process described earlier. 6.6.3 Customer intentions Customer intentions can be used to forecast the demand for a new product or for a variation on an existing product. Questionnaires are filled in by customers on their intentions to buy the product. A structured questionnaire is used, asking customers to rate the likelihood of them purchasing the product on a scale; for example, highly likely, likely, possible, unlikely, highly unlikely. Survey design challenges, such as collecting a representative sample, applying a time- and cost-effective method, and dealing with non-responses, need to be addressed.9 Furthermore, in this survey setting we must keep in mind the relationship between purchase intention and purchase behaviour. Customers do not always do what they say they will. Many studies have found a positive correlation between purchase intentions and purchase behaviour; however, the strength of these correlations varies substantially. The factors driving this variation include the timings of data collection and product launch, the definition of “new” for the product, and the type of industry. Also, the correlation between intention and behaviour is found to be stronger for variations on existing and familiar products than for completely new products. 6.7 Judgmental adjustments In this final section, we consider the situation where historical data are available and are used to generate statistical forecasts. And then practitioners attempt to apply judgmental adjustments to these forecasts. Judgemental forecasts provide an avenue for incorporating factors that may not be accounted for in the statistical model, such as promotions, large sporting events, holidays, or recent events that are not yet reflected in the data. However, these advantages come to fruition only when the right conditions are present. Judgmental adjustments, like judgmental forecasts, come with biases and limitations, and we must implement methodical strategies in order to minimise them. 6.7.1 Use adjustments sparingly Judgmental adjustments should not aim to correct for a systematic pattern in the data that is thought to have been missed by the statistical model. This has been proven to be ineffective, as forecasters tend to read non-existent patterns in noisy series. Statistical models are much better at taking account of data patterns, and judgmental adjustments only hinder accuracy. Judgmental adjustments are most effective when there is significant additional information at hand or strong evidence of the need for an adjustment. We should only adjust when we have important extra information which is not incorporated in the statistical model. Hence, adjustments seem to be most accurate when they are large in size. Small adjustments (especially in the positive direction promoting the illusion of optimism) have been found to hinder accuracy, and should be avoided. 6.7.2 Apply a structured approach Using a structured and systematic approach will improve the accuracy of judgmental adjustments. Following the key principles outlined in Section 6.2 is vital. In particular, having to document and justify adjustments will make it more challenging to override the statistical forecasts, and will guard against unnecessary adjustments. It is common for adjustments to be implemented by a panel (see the example that follows). Using a Delphi setting carries great advantages. 6.7.3 Example: Tourism Forecasting Committee (TFC) Tourism Australia publishes forecasts for all aspects of Australian tourism twice a year. The published forecasts are generated by the TFC, an independent body which comprises experts from various government and industry sectors; for example, the Australian Commonwealth Treasury, airline companies, consulting firms, banking sector companies, and tourism bodies. The forecasting methodology applied is an iterative process. First, model-based statistical forecasts are generated by the forecasting unit within Tourism Australia, then judgmental adjustments are made to these in two rounds. In the first round, the TFC Technical Committee (comprising senior researchers, economists and independent advisers) adjusts the model-based forecasts. In the second and final round, the TFC (comprising industry and government experts) makes final adjustments. In both rounds, adjustments are made by consensus. Figure 6.1: Long run annual forecasts for domestic visitor nights for Australia, comparing regression models, ETS(Exponetial Smoothing) and judgmental adjustsments by TFC. What can we learn from this example? Although the TFC clearly states in its methodology that it produces ‘forecasts’ rather than ‘targets’, could this be a case where these have been confused? Are the forecasters and users sufficiently well-segregated in this process? Could the iterative process itself be improved? Could the adjustment process in the meetings be improved? Could it be that the group meetings have promoted optimism ? "],
["time-series-regression-models.html", "Chapter 7 Time series regression models 7.1 The linear model 7.2 Least squares estimation 7.3 Evaluating a regression model 7.4 Some useful predictors 7.5 Selecting predictors 7.6 Forecasting with regression 7.7 Matrix formulation 7.8 Nonlinear regression 7.9 Correlation, causation and forecasting", " Chapter 7 Time series regression models library(tsibble) library(tsibbledata) library(fable) library(feasts) library(lubridate) library(patchwork) In this chapter we discuss regression models. The basic concept is that we forecast the time series of interest y assuming that it has a linear relationship with other time series \\(x\\). For example, we might wish to forecast monthly sales \\(y\\) using total advertising spend \\(x\\) as a predictor. Or we might forecast daily electricity demand \\(y\\) using temperature \\(x_1\\) and the day of week \\(x_2\\) as predictors. 7.1 The linear model 7.1.1 Simple linear regression In the simplest case, the regression model allows for a linear relationship between the forecast variable \\(y\\) and a single predictor variable \\(x\\): \\[ y_t = \\beta_0 + \\beta_1x_t + \\varepsilon_t \\] Use the US consumption data, us_change, to fit a simple linear model where Consumption is predicted against Income. First, plot these two time series us_change &lt;- fpp3::us_change us_change %&gt;% pivot_longer(c(Consumption, Income)) %&gt;% ggplot() + geom_line(aes(Quarter, value, color = name)) + labs(y = &quot;% change&quot;, color = &quot;Series&quot;) And then make a scatter plot: us_change %&gt;% ggplot(aes(Income, Consumption)) + ylab(&quot;Consumption (quarterly % change)&quot;) + xlab(&quot;Income (quarterly % change)&quot;) + geom_point() + geom_smooth(method=&quot;lm&quot;, se=FALSE) Fit a formal model: us_change_fit &lt;- lm(Consumption ~ Income, data = us_change) us_change_fit %&gt;% glance() #&gt; # A tibble: 1 x 12 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.147 0.143 0.591 33.8 2.40e-8 1 -176. 357. 367. #&gt; # ... with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; The simple linear model can be written as \\[ \\operatorname{Consumption} = 0.54 + 0.27(\\operatorname{Income}) + \\epsilon \\] TSLM() (time series regression model) is more compatible with the modelling workflow in fable, compared to the general method lm() us_change %&gt;% model(TSLM(Consumption ~ Income)) %&gt;% report() #&gt; Series: Consumption #&gt; Model: TSLM #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -2.58236 -0.27777 0.01862 0.32330 1.42229 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.54454 0.05403 10.079 &lt; 2e-16 *** #&gt; Income 0.27183 0.04673 5.817 2.4e-08 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.5905 on 196 degrees of freedom #&gt; Multiple R-squared: 0.1472, Adjusted R-squared: 0.1429 #&gt; F-statistic: 33.84 on 1 and 196 DF, p-value: 2.4022e-08 report() displays a object in a suitable format for reporting, here its result is identical to summary.lm(). TSLM() 7.1.2 Multiple linear regression \\[\\begin{equation} \\tag{7.1} y_t = \\beta_0 + \\beta_1x_{1t} + \\beta_2x_{2t} + \\dots + \\beta_kx_{kt} + \\varepsilon_t \\end{equation}\\] We could simply use more predictors in us_change to create a multiple linear regression model. This time, the last 4 columns are included in the model. Take a look at the rest 3 time series determined by Production, Savings and Unemployment us_change %&gt;% pivot_longer(4:6) %&gt;% ggplot() + geom_line(aes(Quarter, value, color = name)) + facet_wrap(vars(name), nrow = 3, scales = &quot;free_y&quot;) + scale_color_discrete(guide = FALSE) Below is a scatterplot matrix of five variables. The first column shows the relationships between the forecast variable (consumption) and each of the predictors. The scatterplots show positive relationships with income and industrial production, and negative relationships with savings and unemployment. GGally::ggpairs(us_change[, 2:6]) Figure 7.1: A scatterplot matrix of all 5 variables There may some concerns about multicolinearity, but VIF (Variance Inflation Factor) shows there is nothing to worry about: lm(Consumption ~ Income + Production + Savings + Unemployment, data = us_change) %&gt;% car::vif() #&gt; Income Production Savings Unemployment #&gt; 2.670685 2.537494 2.506434 2.519616 Fit a multiple linear model: us_change_mfit &lt;- us_change %&gt;% model(TSLM(Consumption ~ Income + Production + Savings + Unemployment)) us_change_mfit %&gt;% report() #&gt; Series: Consumption #&gt; Model: TSLM #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.90555 -0.15821 -0.03608 0.13618 1.15471 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.253105 0.034470 7.343 5.71e-12 *** #&gt; Income 0.740583 0.040115 18.461 &lt; 2e-16 *** #&gt; Production 0.047173 0.023142 2.038 0.0429 * #&gt; Savings -0.052890 0.002924 -18.088 &lt; 2e-16 *** #&gt; Unemployment -0.174685 0.095511 -1.829 0.0689 . #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.3102 on 193 degrees of freedom #&gt; Multiple R-squared: 0.7683, Adjusted R-squared: 0.7635 #&gt; F-statistic: 160 on 4 and 193 DF, p-value: &lt; 2.22e-16 \\[ \\operatorname{Consumption} = 0.25 + 0.74(\\operatorname{Income}) + 0.05(\\operatorname{Production}) - 0.05(\\operatorname{Savings}) - 0.17(\\operatorname{Unemployment}) + \\epsilon \\] 7.1.3 Assumptions When we use a linear regression model, we are implicitly making some assumptions about the variables in Equation (7.1): The forecast variable \\(y_t\\) and predictors \\({x_1, \\dots, x_k}\\) have a (approximate) linear relationship in reality Residuals \\(\\varepsilon_t\\) are independent (not autocorrelated in a time series linear model specifically) and have constant variance \\(\\sigma^2\\) and mean \\(0\\) . Otherwise the forecasts will be inefficient, as there is more information in the data that can be exploited. This can be expressed as \\[\\begin{equation} \\tag{7.2} \\begin{aligned} \\text{Cov}(\\varepsilon_i, \\varepsilon_j) &amp;= \\begin{cases} 0 &amp; i \\not=j \\\\ \\sigma^2 &amp; i = j \\end{cases} \\;\\;\\;i,j = 1, 2,\\dots,T \\\\ E(\\varepsilon_t) &amp;= 0 \\;\\;\\;t = 1,2,\\dots,T \\end{aligned} \\end{equation}\\] Equation (7.2) is also called a G-M (Gauss-Markov) condition. Residuals follow a approximate normal distribution, meaning: \\[ \\varepsilon_t \\sim N(0, \\sigma^2) \\;\\;\\; t = 1,2, \\dots,T \\] Another important assumption in the linear regression model is that each predictor \\(x\\) is not a random variable. If we were performing a controlled experiment in a laboratory, we could control the values of each \\(x\\) (so they would not be random) and observe the resulting values of \\(y\\). With observational data (including most data in business and economics), it is not possible to control the value of x, we simply observe it. Hence we make this an assumption. 7.2 Least squares estimation The least squares principle provides a way of choosing the coefficients effectively by minimising the sum of the squared errors. That is, we choose the values of \\(\\beta_0\\),\\(\\beta_1\\),…,\\(\\beta_k\\) that minimise : \\[ \\sum_{t=1}^{T}{\\varepsilon_t^2} = \\sum_{t=1}^{T}{(y_t -\\beta_0 - \\beta_1x_{t1} + \\beta_2x_{t2} - \\cdots- \\beta_kx_{tk})^2} \\] 7.2.1 Fitted values To get fitted values, use broom::augment(): us_change_mfit %&gt;% augment() %&gt;% pivot_longer(c(Consumption, .fitted)) %&gt;% ggplot() + geom_line(aes(Quarter, value, color = name)) + labs(color = &quot;&quot;, title = &quot;Percent change in US consumption expenditure&quot;) us_change_mfit %&gt;% augment() %&gt;% ggplot() + geom_point(aes(Consumption, .fitted)) + geom_abline(intercept = 0, slope = 1) + labs(title = &quot;Percent change in US consumption expenditure&quot;, y = &quot;Fitted (predicted values)&quot;, x = &quot;Data (actual values)&quot;) 7.2.2 Goodness of fit \\[ R^2 = \\frac{\\sum{(\\hat{y}_t - \\bar{y})^2}}{\\sum{(y_t - \\bar{y})^2}} \\] 7.2.3 Standard error of the regression Estimate residual standard deviation \\(\\hat{\\sigma}\\), which is often known as the “residual standard error”: \\[\\begin{equation} \\tag{7.3} \\hat{\\sigma} = \\sqrt{\\frac{1}{T-K-1} \\sum_{t=1}^T{e_t^2}} \\end{equation}\\] where \\(k\\) is the number of predictors in the model. Notice that we divide by \\(T− k − 1\\) because we have estimated \\(k + 1\\) parameters (the intercept and a coefficient for each predictor variable) in computing the residuals. The standard error is related to the size of the average error that the model produces. We can compare this error to the sample mean of \\(y\\) or with the standard deviation of \\(y\\) to gain some perspective on the accuracy of the model. 7.3 Evaluating a regression model gg_tsresiduals() and Ljung-Box test(\\(H_0\\) being the residuals are from a white noise series) introduced in Section 5.4 us_change_mfit %&gt;% gg_tsresiduals() us_change_mfit %&gt;% augment() %&gt;% features(.resid, ljung_box, lag = 10, dof = 5) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 TSLM(Consumption ~ Income + Production + Savings + Unemploy~ 18.9 0.00204 The time plot shows some changing variation over time, but is otherwise relatively unremarkable. This heteroscedasticity will potentially make the prediction interval coverage inaccurate. The histogram shows that the residuals seem to be slightly skewed, which may also affect the coverage probability of the prediction intervals. The autocorrelation plot shows a significant spike at lag \\(7\\), and a significant Ljung-Box test at the \\(5\\%\\) level. However, the autocorrelation is not particularly large, and at lag 7 it is unlikely to have any noticeable impact on the forecasts or the prediction intervals. In Chapter 11 we discuss dynamic regression models used for better capturing information left in the residuals. 7.3.1 Residual plots against predictors We would expect the residuals to be randomly scattered without showing any systematic patterns. A simple and quick way to check this is to examine scatterplots of the residuals against each of the predictor variables. If these scatterplots show a pattern, then the relationship may be nonlinear and the model will need to be modified accordingly. See Section 7.8 for a discussion of nonlinear regression. It is also necessary to plot the residuals against any predictors that are not in the model. If any of these show a pattern, then the corresponding predictor may need to be added to the model (possibly in a nonlinear form). 7.3.1.1 Example residuals()allow us to extract residuals from a fable object, without calling augment(). The residuals from the multiple regression model for forecasting US consumption plotted against each predictor seem to be randomly scattered. Therefore we are satisfied with these in this case. df &lt;- us_change %&gt;% left_join(us_change_mfit %&gt;% residuals(), by = &quot;Quarter&quot;) library(patchwork) p1 &lt;- ggplot(df, aes(Income, .resid)) + geom_point() + ylab(&quot;Residuals&quot;) p2 &lt;- ggplot(df, aes(Production, .resid)) + geom_point() + ylab(&quot;Residuals&quot;) p3 &lt;- ggplot(df, aes(Savings, .resid)) + geom_point() + ylab(&quot;Residuals&quot;) p4 &lt;- ggplot(df, aes(Unemployment, .resid)) + geom_point() + ylab(&quot;Residuals&quot;) p1 + p2 + p3 + p4 + plot_layout(nrow = 2) 7.3.2 Residual plots against fitted values A plot of the residuals against the fitted values should also show no pattern. If a pattern is observed, there may be “heteroscedasticity”, or non-constant variance. If this problem occurs, a transformation of the forecast variable such as a logarithm or square root may be required (see Section 3.1.) 7.3.2.1 Example The following plot shows the residuals plotted against the fitted values. The random scatter suggests the errors are homoscedastic. augment(us_change_mfit) %&gt;% ggplot()+ geom_point(aes(.fitted, .resid)) + labs(x = &quot;Fitted&quot;, y = &quot;Residuals&quot;) 7.3.3 Outliers and influential observations Observations that take extreme values compared to the majority of the data are called outliers. Observations that have a large influence on the estimated coefficients of a regression model are called influential observations. Usually, influential observations are also outliers that are extreme in the \\(x\\) direction. It is useful to distinguish outliers from anomalies. An outlier is mathematically stated as any observation point in given data-set that is more than 1.5 interquartile ranges (IQRs) below the first quartile or above the third quartile. Anomaly is items, events or observations which do not conform to an expected pattern (staistical distributions), simply anything “outside normal”. It can be noise, deviations and exceptions defined in application of particular system. The anomalize package provides tools in anomaly detection and visualization. For a formal detection of observation influence, the leverage of the t-th observation \\({x_{1t}, x_{2t}, \\dots, {x_{kt}}\\) is defined as the t-th diagonal element of the hat matrix \\(H = X(X^TX)^{-1}X^T\\), i.e. \\(h_{tt}\\). And the Cook distance of the i-th observation is defined as : \\[ C_t = \\frac{r_t^2}{k + 2} \\times \\frac{h_{tt}}{1 - h_{tt}} \\] where \\(k\\) is the number of predictors and \\(r_t\\) the i-th internally studentized residuals \\(r_t = \\frac{e_t}{\\hat{\\sigma}\\sqrt{1-h_{tt}}}\\) Finding influential observations in practice is not covered in the book. So I followed instructions from another book: An R Companion to Applied Regression, 3rd (Fox and Weisberg 2018). us_change_lm &lt;- lm(Consumption ~ Income + Production + Savings + Unemployment, data = us_change) us_change_lm %&gt;% car::influenceIndexPlot() 7.3.4 The performance package The performance package is dedicated to providing utilities for computing indices of model quality and goodness of fit. In the case of regression, performance provides many functions to check model assumptions, like check_collinearity(), check_normality() or check_heteroscedasticity(). To get a comprehensive check, use check_model() library(performance) check_model(us_change_lm) 7.3.5 Spurious regression Time series data are often “non-stationary”. That is, the values of the time series do not fluctuate around a constant mean or with a constant variance. We will come to the formal definition of stationarity in more detail in Section 9.1, but here we need to address the effect that non-stationary data can have on regression models. For example, consider the two variables plotted in below. These appear to be related simply because they both trend upwards in the same manner. However, air passenger traffic in Australia has nothing to do with rice production in Guinea. guinea_rice &lt;- fpp3::guinea_rice air_passengers &lt;- fpp3::aus_airpassengers p1 &lt;- guinea_rice %&gt;% autoplot() + ggtitle(&quot;Guinea rice production&quot;) p2 &lt;- air_passengers %&gt;% autoplot() + ggtitle(&quot;Australia air passengers&quot;) p3 &lt;- guinea_rice %&gt;% left_join(air_passengers) %&gt;% ggplot(aes(Production, Passengers)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + ggtitle(&quot;A well-fitted regression line&quot;) (p1 / p2) | p3 Regressing non-stationary time series can lead to spurious regressions. High \\(R^2\\) and high residual autocorrelation can be signs of spurious regression. Notice these features in the output below. We discuss the issues surrounding non-stationary data and spurious regressions in more details in Chapter 11. Cases of spurious regression might appear to give reasonable short-term forecasts, but they will generally not continue to work into the future. spurious_fit &lt;- guinea_rice %&gt;% left_join(air_passengers) %&gt;% lm(Passengers ~ Production, data = .) # high r^2 and sigma glance(spurious_fit) #&gt; # A tibble: 1 x 12 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.958 0.957 3.24 908. 4.08e-29 1 -108. 222. 227. #&gt; # ... with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Section 9.1.2 introduces the BG test, which is designed to detect autocorrelation among residuals of a regression model, small p-value suggests that residuals are highly correlated spurious_fit %&gt;% lmtest::bgtest() #&gt; #&gt; Breusch-Godfrey test for serial correlation of order up to 1 #&gt; #&gt; data: . #&gt; LM test = 23.309, df = 1, p-value = 0.00000138 7.4 Some useful predictors There are several useful predictors that occur frequently when using regression for time series data. 7.4.1 Trend It is common for time series data to be trending. A linear trend can be modelled by simply using \\(x_{1t} = t\\) as a predictor \\[ y_t = \\beta_0 + \\beta_1 t + \\varepsilon \\] where \\(t = 1, 2, \\dots, T\\) A trend variable can be specified in the TSLM() function using the trend() special. In Section 7.8 we discuss how we can also model a nonlinear trends. 7.4.2 Seasonal dummy variables If the time sereis data shows storng seasonality in some fashion, we tend to add seasonaly dummy variables to include this seasonality in our model. The TSLM() function will automatically handle this situation if you specify the special season(). For example, if we are modelling daily data (\\(period = 7\\)), 6 dummy variables will be created. 7.4.3 Example: Australian quarterly beer production Recall the Australian quarterly beer production data recent_production &lt;- aus_production %&gt;% filter(year(Quarter) &gt;= 1992) recent_production %&gt;% gg_tsdisplay(Beer) We want to forecast the value of future beer production. We can model this data using a regression model with a linear trend and quarterly dummy variables, \\[ y_t = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 d_{2t} + \\beta_3 d_{3t}+ \\beta_4 d_{4t} \\] where \\(d_{2t}\\), \\(d_{3t}\\) and \\(d_{4t}\\) are dummy variables representing 3 of all 4 seasons except the first. beer_fit &lt;- recent_production %&gt;% model(TSLM(Beer ~ trend() + season())) beer_fit %&gt;% report() #&gt; Series: Beer #&gt; Model: TSLM #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -42.9029 -7.5995 -0.4594 7.9908 21.7895 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 441.80044 3.73353 118.333 &lt; 0.0000000000000002 *** #&gt; trend() -0.34027 0.06657 -5.111 0.00000272965382 *** #&gt; season()year2 -34.65973 3.96832 -8.734 0.00000000000091 *** #&gt; season()year3 -17.82164 4.02249 -4.430 0.00003449674545 *** #&gt; season()year4 72.79641 4.02305 18.095 &lt; 0.0000000000000002 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 12.23 on 69 degrees of freedom #&gt; Multiple R-squared: 0.9243, Adjusted R-squared: 0.9199 #&gt; F-statistic: 210.7 on 4 and 69 DF, p-value: &lt; 0.000000000000000222 Note that trend() and season() are not standard functions; they are “special” functions that work within the TSLM() model formulae. There is an average downward trend of -0.34 megalitres per quarter. On average, the second quarter has production of 34.7 megalitres lower than the first quarter, the third quarter has production of 17.8 megalitres lower than the first quarter, and the fourth quarter has production of 72.8 megalitres higher than the first quarter. augment(beer_fit) %&gt;% ggplot(aes(x = Quarter)) + geom_line(aes(y = Beer, colour = &quot;Data&quot;)) + geom_line(aes(y = .fitted, colour = &quot;Fitted&quot;)) + labs(x = &quot;Year&quot;, y = &quot;Megalitres&quot;, title = &quot;Quarterly Beer Production&quot;) augment(beer_fit) %&gt;% ggplot(aes(x = Beer, y = .fitted, colour = factor(quarter(Quarter)))) + geom_point() + scale_colour_brewer(palette=&quot;Dark2&quot;, name=&quot;Quarter&quot;) + geom_abline(intercept = 0, slope = 1) + labs(y = &quot;Fitted&quot;, x = &quot;Actual values&quot;, title = &quot;Quarterly beer production&quot;) 7.4.4 Intervention variables It is often necessary to model interventions that may have affected the variable to be forecast. For example, competitor activity, advertising expenditure, industrial action, and so on, can all have an effect. When the effect lasts only for one period, we use a spike variable. This is a dummy variable that takes value one in the period of the intervention and zero elsewhere. A spike variable is equivalent to a dummy variable for handling an outlier. Other interventions have an immediate and permanent effect. If an intervention causes a level shift (i.e., the value of the series changes suddenly and permanently from the time of intervention), then we use a step variable. A step variable takes value zero before the intervention and one from the time of intervention onward. Another form of permanent effect is a change of slope. Here the intervention is handled using a piecewise linear trend; a trend that bends at the time of intervention and hence is nonlinear. We will discuss this in Section 7.8. 7.4.5 Trading days The number of trading days in a month can vary considerably and can have a substantial effect on sales data. To allow for this, the number of trading days in each month can be included as a predictor. An alternative that allows for the effects of different days of the week has the following predictors: \\[ \\begin{aligned} x_1 &amp;= \\text{number of Mondays in the month} \\\\ x_2 &amp;= \\text{number of Tuesdays in the month} \\\\ ... \\\\ x_7 &amp;= \\text{number of Sundays in the month} \\\\ \\end{aligned} \\] 7.4.6 Distributed lags It is often useful to include advertising expenditure as a predictor. However, since the effect of advertising can last beyond the actual campaign, we need to include lagged values of advertising expenditure. Thus, the following predictors may be used. \\[ \\begin{aligned} x_1 &amp;= \\text{expenditure in the last month} \\\\ x_2 &amp;= \\text{expenditure in the last 2 month} \\\\ ... \\\\ x_m &amp;= \\text{expenditure in the last m month} \\\\ \\end{aligned} \\] It is common to require the coefficients to decrease as the lag increases, although this is beyond the scope of the book. 7.4.7 Easter Easter differs from most holidays because it is not held on the same date each year, and its effect can last for several days. In this case, a dummy variable can be used with value one where the holiday falls in the particular time period and zero otherwise. With monthly data, if Easter falls in March then the dummy variable takes value 1 in March, and if it falls in April the dummy variable takes value 1 in April. When Easter starts in March and finishes in April, the dummy variable is split proportionally between months. 7.4.8 Fourier sereis An alternative to using seasonal dummy variables, especially for long seasonal periods, is to use Fourier terms, which, proved by French mathematician Jean-Baptiste Fourier in the 1800s, can approximate any periodic function. We can use them for seasonal patterns. If \\(m\\) is the seasonal period, then the first few Fourier terms (6 listed here) are given by: \\[ \\begin{aligned} x_{1t} &amp;= \\cos{\\frac{2 \\pi t}{m}} \\\\ x_{2t} &amp;= \\sin{\\frac{2 \\pi t}{m}} \\\\ x_{3t} &amp;= \\cos{\\frac{4 \\pi t}{m}} \\\\ x_{4t} &amp;= \\sin{\\frac{4 \\pi t}{m}} \\\\ x_{5t} &amp;= \\cos{\\frac{6 \\pi t}{m}} \\\\ x_{6t} &amp;= \\sin{\\frac{6 \\pi t}{m}} \\\\ \\end{aligned} \\] and so on. If we have monthly seasonality, and we use the first 11 of these predictor variables, then we will get exactly the same forecasts as using 11 dummy variables. With Fourier terms, we often need fewer predictors than with dummy variables, especially when m is large. This makes them useful for weekly data, for example, where \\(m \\approx 52\\). For short seasonal periods (e.g., quarterly data), there is little advantage in using Fourier terms over seasonal dummy variables. These Fourier terms are produced using the fourier(K) function. The K argument specifies the maximum order of Fourier terms (i.e., how many pairs of \\(\\sin\\) and \\(\\cos\\) terms to include). For example, the Australian beer data (quarterly, should include 3 terms so K = 2) can be modelled like this. fourier_beer &lt;- recent_production %&gt;% model(TSLM(Beer ~ trend() + fourier(K = 2))) fourier_beer %&gt;% report() #&gt; Series: Beer #&gt; Model: TSLM #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -42.9029 -7.5995 -0.4594 7.9908 21.7895 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 446.87920 2.87321 155.533 &lt; 0.0000000000000002 *** #&gt; trend() -0.34027 0.06657 -5.111 0.00000272965382379 *** #&gt; fourier(K = 2)C1_4 8.91082 2.01125 4.430 0.00003449674544834 *** #&gt; fourier(K = 2)S1_4 -53.72807 2.01125 -26.714 &lt; 0.0000000000000002 *** #&gt; fourier(K = 2)C2_4 -13.98958 1.42256 -9.834 0.00000000000000926 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 12.23 on 69 degrees of freedom #&gt; Multiple R-squared: 0.9243, Adjusted R-squared: 0.9199 #&gt; F-statistic: 210.7 on 4 and 69 DF, p-value: &lt; 0.000000000000000222 The maximum allowed is \\(K = m / 2\\) where \\(m\\) is the seasonal period. Because we have used the maximum here, the results are identical to those obtained when using seasonal dummy variables. If only the first two Fourier terms are used (\\(x_{1t}\\) and \\(x_{2t}\\)), the seasonal pattern will follow a simple sine wave. A regression model containing Fourier terms is often called a harmonic regression because the successive Fourier terms represent harmonics of the first two Fourier terms. 7.5 Selecting predictors When there are many possible predictors, we need some strategy for selecting the best predictors to use in a regression model. Here we use predictive accuracy. They can be shown using the glance() function, here applied to the model for us_change: glance(us_change_mfit) %&gt;% select(adj_r_squared, CV, AIC, AICc, BIC) #&gt; # A tibble: 1 x 5 #&gt; adj_r_squared CV AIC AICc BIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.763 0.104 -457. -456. -437. 7.5.1 Adjusst R square The determinant coefficient, \\(R^2\\), is defined by \\[ R^2 = \\frac{\\sum{(\\hat{y}_t - \\bar{y}_t)^2}}{\\sum{(y_t - \\bar{y}_t)^2}} \\] Since \\(R^2\\) favours (unjustly) models with more predictors (increases whatever predictor is added), it is common to use a penalized version, Adjusted \\(R^2\\) or \\(\\bar{R}^2\\) \\[ \\bar{R}^2 = 1 - (1 - R^2)\\frac{T - 1}{T - k -1} \\] All things being equal, adjusted \\(R^2\\) is generally smaller than \\(R^2\\), unless you are dealing with a null model so that \\(k = 0\\), since it aims to penalize models with too many predictors. Using this measure, the best model will be the one with the largest value of ¯R2. Maximising ¯R2 is equivalent to minimising the standard error \\(\\hat{\\sigma}\\) of \\(\\hat{\\varepsilon}\\) given in Equation (7.3). Maximising \\(\\bar{R}^2\\) (For the rest of regression measurements, we almost always want to minimize) works quite well as a method of selecting predictors, although it does tend to err on the side of selecting too many predictors. 7.5.2 Cross validation Time series cross-validation was introduced in Section 5.7 as a general tool for determining the predictive ability of a model. For regression models, it is also possible to use classical leave-one-out cross-validation to selection predictors. This is faster and makes more efficient use of the data. The procedure uses the following steps: Remove observation t from the data set, and fit the model using the remaining data. Then compute the error (\\(e^*_t=y_t−\\hat{y}_t\\)) for the omitted observation. Repeat step 1 for \\(t = 1, 2, \\dots, T\\) Compute \\(MSE = \\sum{{e^*_t}^2} / T\\) , we shall call it the CV. Although cross validation may look like time-consuming procedure, there are fast methods of calculating CV, so that it takes no longer than fitting one model to the full data set. The equation for computing CV efficiently is given in Section 7.7. Under this criterion, the best model is the one with the smallest value of CV. 7.5.3 Akaike’s Information Criterion A closely-related method is Akaike’s Information Criterion, which we define as \\[ \\text{AIC} = T \\log{\\frac{SSE}{T}} + 2(K + 2) \\] The k+2 part of the equation occurs because there are k+2 parameters in the model: the k coefficients for the predictors, the intercept and the variance of the residuals. The idea here is to penalise the fit of the model (SSE) with the number of parameters that need to be estimated. The model with the minimum value of the AIC is often the best model for forecasting. For large values of \\(T\\), minimising the AIC is equivalent to minimising the CV value. For small values of \\(T\\), the AIC tends to select too many predictors, and so a bias-corrected version of the AIC has been developed \\[ \\text{AIC}_c = \\text{AIC} + \\frac{2(k + 2)(k + 3)}{T - k - 3} \\] 7.5.4 Bayesian Information Criterion A related measure is Schwarz’s Bayesian Information Criterion (usually abbreviated to BIC, SBIC or SC): \\[ \\text{BIC} = T\\log{(\\frac{SSE}{T}) + (k + 2)\\log{(T)}} \\] As with the AIC, minimising the BIC is intended to give the best model. The model chosen by the BIC is either the same as that chosen by the AIC, or one with fewer terms. This is because the BIC penalises the number of parameters more heavily than the AIC. For large values of T, minimising BIC is similar to leave-v-out cross-validation when \\(v = T[1 - 1/\\log{(T)} -1]\\) 7.5.5 Which measure should we suse While \\(\\bar{R}^2\\) is widely used, and has been around longer than the other measures, its tendency to select too many predictor variables makes it less suitable for forecasting. Many statisticians like to use the BIC because it has the feature that if there is a true underlying model, the BIC will select that model given enough data. However, in reality, there is rarely, if ever, a true underlying model, and even if there was a true underlying model, selecting that model will not necessarily give the best forecasts (because the parameter estimates may not be accurate). Consequently, we recommend that one of the \\(\\text{AIC}_c\\), \\(\\text{AIC}\\), or \\(\\text{CV}\\) statistics be used, each of which has forecasting as their objective. If the value of \\(T\\) is large enough, they will all lead to the same model. In most of the examples in this book, we use the \\(\\text{AIC}_c\\) value to select the forecasting model. 7.5.6 Example: US consumption In us_change_mfit 4 predictors are specified, so there are \\(2^4 = 16\\) possible models The best model contains all four predictors according to \\(\\text{AIC}_c\\). The results from a backward selection using AIC follow suit: MASS::stepAIC(us_change_lm, direction = &quot;backward&quot;) #&gt; Start: AIC=-458.58 #&gt; Consumption ~ Income + Production + Savings + Unemployment #&gt; #&gt; Df Sum of Sq RSS AIC #&gt; &lt;none&gt; 18.573 -458.58 #&gt; - Unemployment 1 0.322 18.895 -457.18 #&gt; - Production 1 0.400 18.973 -456.36 #&gt; - Savings 1 31.483 50.056 -264.27 #&gt; - Income 1 32.799 51.371 -259.14 #&gt; #&gt; Call: #&gt; lm(formula = Consumption ~ Income + Production + Savings + Unemployment, #&gt; data = us_change) #&gt; #&gt; Coefficients: #&gt; (Intercept) Income Production Savings Unemployment #&gt; 0.25311 0.74058 0.04717 -0.05289 -0.17469 The best model contains all four predictors. However, a closer look at the results reveals some interesting features. There is clear separation between the models in the first four rows and the ones below. This indicates that Income and Savings are both more important variables than Production and Unemployment. Also, the first three rows have almost identical values of \\(\\text{CV}\\), \\(\\text{AIC}\\) and \\(\\text{AIC}_c\\). So we could possibly drop either the Production variable, or the Unemployment variable, and get similar forecasts. Note that Production and Unemployment are highly (negatively) correlated (\\(\\hat{r} = -0.768\\), see figure 7.1) 7.6 Forecasting with regression Recall the regression model Equation (7.1) \\[\\begin{equation} y_t = \\beta_0 + \\beta_1x_{1t} + \\beta_2x_{2t} + \\dots + \\beta_kx_{kt} + \\varepsilon_t \\end{equation}\\] While we can easily get fitted values \\({\\hat{y}_1, \\hat{y}_2, \\dots, \\hat{y}_T}\\), what we are interested in here, however, is forecasting future values of \\(y\\). 7.6.1 Ex-ante versus ex-post forecasts When using regression models for time series data, we need to distinguish between the different types of forecasts that can be produced, depending on whether future values of predictor variables are known in advance, or whether we need to forecast predcitor variables first. Ex-ante forecast is a forecast based solely on information available at the time of the forecast, whereas ex-post forecast is a forecast that uses information beyond the time at which the forecast is made. For example, ex-ante forecasts for the percentage change in US consumption will first requires forecasts of the predictors. To obtain these we can use one of the simple methods introduced in Section 5.2 (mean, naive, seasonal navie and drift method) or more sophisticated pure time series approaches that follow in Chapters 8 and 10. Alternatively, forecasts from some other source, such as a government agency, may be available and can be used. Ex-ante forecasts are genuine forecasting. On the other hand, ex-post forecasts are those that are made using later information on the predictors. For example, ex-post forecasts of consumption may use the actual observations of the predictors, once these have been observed. These are not genuine forecasts, but are useful for studying the behaviour of forecasting models. 7.6.2 Example: Australian quarterly beer production Normally, we cannot use actual future values of the predictor variables when producing ex-ante forecasts because their values will not be known in advance. However, the special predictors introduced in Section 7.4 are all known in advance (a trend variable and 3 seasonal dummy variable), as they are based on calendar variables (e.g., seasonal dummy variables or public holiday indicators) or deterministic functions of time (e.g. time trend). In such cases, there is no difference between ex-ante and ex-post forecasts. # `beer_fit` uses only trend variabe and seasonal dummy variables. In such cases, there is no difference between ex-ante and ex-post forecasts. beer_fit %&gt;% forecast(h = &quot;3 years&quot;) %&gt;% autoplot(recent_production, level = 90) + labs(x = &quot;Year&quot;, y = &quot;megalitres&quot;, title = &quot;Forecasts of beer production using regression&quot;) 7.6.3 Scenario based forecasting In this setting, the forecaster assumes possible scenarios for the predictor variables that are of interest. For example, a US policy maker may be interested in comparing the predicted change in consumption when there is a constant growth of \\(1\\%\\) and \\(0.5\\%\\) respectively for income and savings with no change in production and the employment rate, versus a respective decline of \\(1\\%\\) and \\(0.5\\%\\), for each of the four quarters following the end of the sample. We should note that prediction intervals for scenario based forecasts do not include the uncertainty associated with the future values of the predictor variables. They assume that the values of the predictors are known in advance, (i.e, ex-post forecasts). # new_data(data, n) creates n rows of time index of each series to new_data(us_change, 4) #&gt; # A tsibble: 4 x 1 [1Q] #&gt; Quarter #&gt; &lt;qtr&gt; #&gt; 1 2019 Q3 #&gt; 2 2019 Q4 #&gt; 3 2020 Q1 #&gt; 4 2020 Q2 # define a function producing values based on increase / decrease rate new_obs &lt;- function(value, length, rate) { vec &lt;- rep(value, length) imap_dbl(vec, ~ .x * (1 + rate) ^ .y) } up_future &lt;- us_change %&gt;% slice(n()) %&gt;% as_tibble() %&gt;% select(3:6) %&gt;% pivot_longer(everything()) %&gt;% select(-name) %&gt;% mutate(length = 4, rate = c(0.01, 0, 0.005, 0)) %&gt;% pmap(new_obs) %&gt;% set_names(c(&quot;Income&quot;, &quot;Production&quot;, &quot;Savings&quot;, &quot;Unemployment&quot;)) %&gt;% as_tibble() %&gt;% bind_cols(new_data(us_change, 4), .) up_future #&gt; # A tsibble: 4 x 5 [1Q] #&gt; Quarter Income Production Savings Unemployment #&gt; &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2019 Q3 0.599 -0.540 -4.29 -0.100 #&gt; 2 2019 Q4 0.605 -0.540 -4.31 -0.100 #&gt; 3 2020 Q1 0.611 -0.540 -4.33 -0.100 #&gt; 4 2020 Q2 0.617 -0.540 -4.35 -0.100 down_future &lt;- us_change %&gt;% slice(n()) %&gt;% as_tibble() %&gt;% select(3:6) %&gt;% pivot_longer(everything()) %&gt;% select(-name) %&gt;% mutate(length = 4, rate = c(-0.01, 0, -0.005, 0)) %&gt;% pmap(new_obs) %&gt;% set_names(c(&quot;Income&quot;, &quot;Production&quot;, &quot;Savings&quot;, &quot;Unemployment&quot;)) %&gt;% as_tibble() %&gt;% bind_cols(new_data(us_change, 4), .) down_future #&gt; # A tsibble: 4 x 5 [1Q] #&gt; Quarter Income Production Savings Unemployment #&gt; &lt;qtr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2019 Q3 0.587 -0.540 -4.24 -0.100 #&gt; 2 2019 Q4 0.582 -0.540 -4.22 -0.100 #&gt; 3 2020 Q1 0.576 -0.540 -4.20 -0.100 #&gt; 4 2020 Q2 0.570 -0.540 -4.18 -0.100 Then we could produce forecast for each of the two scenarios : up_future_fc &lt;- forecast(us_change_mfit, new_data = up_future) %&gt;% mutate(Scenario = &quot;Increase&quot;) down_future_fc &lt;- forecast(us_change_mfit, new_data = down_future) %&gt;% mutate(Scenario = &quot;Decrease&quot;) %&gt;% as_tibble() us_change %&gt;% ggplot(aes(Quarter, Consumption)) + geom_line() + geom_line(aes(y = .mean), data = up_future_fc, color = &quot;red&quot;) + geom_line(aes(y = .mean), data = down_future_fc, color = &quot;blue&quot;) + coord_cartesian(xlim = c(ymd(&quot;2015-01-01&quot;, NA)), ylim = c(0, 1.2)) 7.6.4 Prediction intervals The general formulation of how to calculate prediction intervals for multiple regression models is presented in Section 7.7. As this involves some advanced matrix algebra we present here the case for calculating prediction intervals for a simple regression, where a forecast can be generated using the equation \\[ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\] Assuming that the regression errors are normally distributed, an approximate \\(1 - \\alpha\\) prediction interval associated with this forecast is given by \\[\\begin{equation} \\tag{7.4} \\hat{y} + Z_{\\alpha}\\hat{\\sigma} \\sqrt{1 + \\frac{1}{T} + \\frac{(x - \\bar{x})^2}{(T - 1)s_x^2}} \\end{equation}\\] where \\(\\hat{\\sigma}\\) is the residual standard deviation given by Equation (7.3), and \\(s_x\\) the standard deviation of predictor \\(x\\). Equation (eq:prediction-interval) shows that the prediction interval is wider when \\(x\\) is far from \\(\\bar{x}\\). That is, we are more certain about our forecasts when considering values of the predictor variable close to its sample mean. 7.6.5 Building a predictive regression model A major challenge however in regression, is that in order to generate ex-ante forecasts, the model requires future values of each predictor. If scenario based forecasting is of interest then these models are extremely useful. However, if ex-ante forecasting is the main focus, obtaining forecasts of the predictors can be challenging (in many cases generating forecasts for the predictor variables can be more challenging than forecasting directly the forecast variable without using predictors). An alternative formulation is to use as predictors their lagged values. Assuming that we are interested in generating a \\(h\\)-step ahead forecast we write \\[ y_{t + h} = \\beta_0 + \\beta_1 x_{1t} + \\beta_2 x_{2t} + \\cdots + \\beta_k x_{kt} \\] for \\(h = 1, 2, \\dots\\). The predictor set is formed by values of the \\(x\\)s that are observed h time periods prior to observing y. Therefore when the estimated model is projected into the future, i.e., beyond the end of the sample \\(T\\), all predictor values are available (unless the step of forecast is larger than lag \\(h\\)). Including lagged values of the predictors does not only make the model operational for easily generating forecasts, it also makes it intuitively appealing. For example, the effect of a policy change with the aim of increasing production may not have an instantaneous effect on consumption expenditure. It is most likely that this will happen with a lagging effect. We touched upon this in Section 7.4 when briefly introducing distributed lags as predictors. Several directions for generalising regression models to better incorporate the rich dynamics observed in time series are discussed in Chapter 11. 7.7 Matrix formulation A linear regression model can be expressed in matrix forms as such: \\[ \\boldsymbol{y} = \\boldsymbol{X} \\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\] where \\(\\boldsymbol{y} = [y_1, y_2, \\dots, y_T]\\), \\(\\boldsymbol{X}\\) being the design matrix and \\(\\boldsymbol{\\varepsilon} = [\\varepsilon_1, \\varepsilon_2, \\dots, \\varepsilon_T]\\) thus have mean \\(\\boldsymbol{0}\\) and variance-covariance matrix \\(\\sigma^2\\boldsymbol{I}\\) Least square estimation uses a projection matrix \\(H = \\boldsymbol{X(X^TX)^{-1}X^T}\\) (also called “hat matrix”) so that \\(\\boldsymbol{Hy = \\hat{y} = X\\hat{\\beta}}\\). We can derive that \\[ \\boldsymbol{\\beta} = \\boldsymbol{(X^TX)^{-1}X^Ty} \\] The residual variance \\(\\hat{\\sigma}\\)is estimated using : \\[ \\boldsymbol{\\hat{\\sigma}} = \\frac{1}{T-k-1}\\boldsymbol{(y - X\\hat{\\beta})^T(y - X\\hat{\\beta})} \\] If the diagonal value of \\(\\boldsymbol{P}\\) is denoted by \\(h_1, h_2 , \\dots, h_T\\) then the cross-validation statistic can be computed using : \\[ \\text{CV} = \\frac{1}{T}\\sum{[e_t / (1 - h_t)]^2} \\] For any given \\(\\boldsymbol{x^*} = [1, x_{1t}, \\dots, x_{kt}]\\), the fitted value is \\(\\hat{y} = \\boldsymbol{x^*}\\boldsymbol{\\hat{\\beta}}\\) and its estimated variance given by: \\[ \\hat{\\sigma}[1 + \\boldsymbol{x^*}(\\boldsymbol{X^T X})^{-1}\\boldsymbol{(x^*)^T}] \\] A \\(1 - \\alpha\\) prediction interval (assuming normally distributed errors) can be calculated as \\[ \\hat{y} + Z_\\alpha \\hat{\\sigma} \\sqrt{[1 + \\boldsymbol{x^*}(\\boldsymbol{X^T X})^{-1}\\boldsymbol{(x^*)^T}]} \\] This takes into account the uncertainty due to the error term \\(\\boldsymbol{\\varepsilon}\\) and the uncertainty in the coefficient estimates. However, it ignores any errors in \\(\\boldsymbol{x^*}\\). Thus, if the future values of the predictors are uncertain, then the prediction interval calculated using this expression will be too narrow. 7.8 Nonlinear regression Although the linear relationship assumed so far in this chapter is often adequate, there are many cases in which a nonlinear functional form is more suitable. To keep things simple in this section we assume that we only have one predictor \\(x\\). The simplest way of modelling a nonlinear relationship is to transform the forecast variable y and/or the predictor variable x before estimating a regression model. While this provides a non-linear functional form, the model is still linear in the parameters. The most commonly used transformation is the (natural) logarithm (see Section 3.1). A log-log functional form is specified as \\[ \\log{y} = \\beta_0 + \\beta_1 \\log{x} + \\varepsilon \\] Other useful forms can also be specified. The log-linear form is specified by only transforming the forecast variable and the linear-log form is obtained by transforming the predictor. Recall that in order to perform a logarithmic transformation to a variable, all of its observed values must be greater than zero. In the case that variable \\(x\\) contains zeros, we use the transformation \\(\\log{(x+1)}\\); i.e., we add one to the value of the variable and then take logarithms. This has a similar effect to taking logarithms but avoids the problem of zeros. It also has the neat side-effect of zeros on the original scale remaining zeros on the transformed scale. Also, box-cox transformation as a family of both power transformation and log transformation is given by Equation (3.1) There are cases for which simply transforming the data will not be adequate and a more general specification may be required. Then the model we use is \\[ y = f(x) + \\varepsilon \\] where \\(f(x)\\) could be of any form. In the specification of nonlinear regression that follows, we allow f to be a more flexible nonlinear function of \\(x\\), compared to simply a logarithmic or other transformation. One of the simplest specifications is to make \\(f\\) piecewise linear. That is, we introduce points where the slope of \\(f\\) can change. These points are called knots. This can be achieved by letting \\(x_{1t} = x\\) and introducing variable \\(x_{2t}\\) such that \\[ x_{2t} = (x - c)_+ = \\begin{cases} 0 &amp; x &lt; c\\\\ (x - c) &amp; x \\ge c \\end{cases} \\] The notation \\((x - c)_+\\) means that the value \\(x - c\\) if it is positive and \\(0\\) ohterwise, which can be achieved by introducing a dummy variable \\(D\\) that take 0 if \\(x &lt; 0\\) and 1 if \\(x \\ge 0\\) and then include term \\((x - c)\\times D\\). This forces the slope to bend at point \\(c\\). Additional bends can be included in the relationship by adding further variables of the above form. Piecewise linear relationships constructed in this way are a special case of regression splines. In general, a linear regression spline is obtained using \\[ x_1 = x \\;\\;\\; x_2 = (x - c_1)_+ \\;\\;\\; \\cdots \\;\\;\\; x_k = (x - c_{k-1})_+ \\] where \\(c_1, \\dots, c_k-1\\) are knots. Selecting the number of knots (\\(k−1\\)) and where they should be positioned can be difficult and somewhat arbitrary. 7.8.1 Forecasting with a nonlinear trend In section 7.4 we introduce the trend variable \\(t\\). The simplest way of fitting a nonlinear trend is using quadratic or higher order trends obtained by specifying \\[ x_{1t} = t, \\;\\;\\;x_{2t} = t^2, \\;\\;\\;\\dots \\] In practice, higher order (\\(&gt;3\\)) or even quadratic terms are not recommended in forecasting. When they are extrapolated, the resulting forecasts are often unrealistic. A better approach is to use the piecewise specification introduced above and fit a piecewise linear trend which bends at some point in time. We can think of this as a nonlinear trend constructed of linear pieces. If the trend bends at time \\(\\tau\\), then it can be specified by simply replacing \\(x=t\\) and \\(c=τ\\) above such that we include the predictors \\[ \\begin{aligned} x_{1t} &amp;= t \\\\ x_{2t} = (t - \\tau)_+ &amp;= \\begin{cases} 0 &amp; t &lt; \\tau \\\\ (t - \\tau) &amp; t \\ge \\tau \\end{cases} \\end{aligned} \\] in the model. 7.8.2 Example: Boston marathon winning times We will fit some trend models to the Boston marathon winning times for men since the event started in 1897. boston_men &lt;- fpp3::boston_marathon %&gt;% filter(Event == &quot;Men&#39;s open division&quot;) %&gt;% mutate(Minutes = as.numeric(Time) / 60) boston_lm &lt;- boston_men %&gt;% model(TSLM(Minutes ~ trend())) boston_men %&gt;% ggplot(aes(Year, Minutes)) + geom_line() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + geom_vline(xintercept = c(1940, 1980), size = 1.5) + labs(title = &quot;Fitting a linear line&quot;) boston_lm %&gt;% augment() %&gt;% ggplot(aes(Year, .resid)) + geom_point() + geom_hline(yintercept = 0, size = 1.5) + labs(title = &quot;Residual across trend&quot;) boston_lm %&gt;% glance() #&gt; # A tibble: 1 x 16 #&gt; Event .model r_squared adj_r_squared sigma2 statistic p_value df log_lik #&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 Men&#39;~ TSLM(~ 0.728 0.726 38.2 324. 4.82e-36 2 -398. #&gt; # ... with 7 more variables: AIC &lt;dbl&gt;, AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, CV &lt;dbl&gt;, #&gt; # deviance &lt;dbl&gt;, df.residual &lt;int&gt;, rank &lt;int&gt; There seems to be a (quadratic) pattern in our residual plot, and our simple linear model isn’t fitting very well. Alternatively, fitting an exponential trend (equivalent to a log-linear regression) to the data can be achieved by transforming the \\(y\\) variable so that the model to be fitted is, \\[ \\log{y}_t = \\beta_0 + \\beta_1t + \\varepsilon_t \\] The plot of winning times reveals three different periods. There is a lot of volatility in the winning times up to about 1940, with the winning times decreasing overall but with significant increases during the 1920s. After 1940 there is a near-linear decrease in times, followed by a flattening out after the 1980s, with the suggestion of an upturn towards the end of the sample. To account for these changes, we specify the years 1940 and 1980 as knots. We should warn here that subjective identification of knots can lead to over-fitting, which can be detrimental to the forecast performance of a model, and should be performed with caution. A piecewise regression (bends at certain time \\(t\\)) is specified using the knots argument in trend(): boston_piece_fit &lt;- boston_men %&gt;% model( linear = TSLM(Minutes ~ trend()), exponential = TSLM(log(Minutes) ~ trend()), piecewise = TSLM(Minutes ~ trend(knots = c(1940, 1980))) ) boston_piece_fc &lt;- boston_piece_fit %&gt;% forecast(h = 10) boston_piece_fc %&gt;% autoplot(boston_men, level = NULL) + labs(title = &quot;Boston Marathon&quot;, x = &quot;Year&quot;, y = &quot;Winning times in minutes&quot;, color = &quot;Model&quot;) 7.9 Correlation, causation and forecasting "],
["exponential-smoothing.html", "Chapter 8 Exponential smoothing 8.1 Simple exponential smoothing 8.2 Methods with trend and seasonality 8.3 A taxonomy of exponential smoothing methods 8.4 Innovations state space models for exponential smoothing 8.5 Estimation and model selection 8.6 Forecasting with ETS models", " Chapter 8 Exponential smoothing Exponential smoothing was proposed in the late 1950s ((Brown 1959; Holt 1957; Winters 1960)), and has motivated some of the most successful forecasting methods. Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older. In other words, the more recent the observation the higher the associated weight. This framework generates reliable forecasts quickly and for a wide range of time series, which is a great advantage and of major importance to applications in industry. library(tsibble) library(tsibbledata) library(feasts) library(fable) library(lubridate) 8.1 Simple exponential smoothing The simplest of the exponentially smoothing methods is naturally called simple exponential smoothing (SES). This method is suitable for forecasting data with no clear trend or seasonal pattern. For example, algeria_economy below do not display any clear trending behaviour or any seasonality. (There is a decline in the last few years, which might suggest a trend. We will consider whether a trended method would be better for this series later in this chapter.) algeria_economy &lt;- tsibbledata::global_economy %&gt;% filter(Country == &quot;Algeria&quot;) algeria_economy %&gt;% autoplot(Exports) + labs(y = &quot;Exports (% of GDP)&quot;, x = &quot;Year&quot;) While the naïve method and average method can be considered as two extremes: all weight given to the last observation and equal weight given to all of the observations, we often want something in between. This is the idea behind the exponential smoothing method. Forecasts are calculated using weighted averages, where the weights decrease exponentially as observations come from further in the past — the smallest weights are associated with the oldest observations: \\[\\begin{equation} \\tag{8.1} \\hat{y}_{T+1|T} = \\alpha y_T + \\alpha(1 - \\alpha)y_{T-1} + \\alpha(1-\\alpha)^2y_{T-2} + \\cdots \\end{equation}\\] where \\(0 &lt; \\alpha &lt; 1\\) is called a smoothing parameter, controlly the rate at which the weights decrease. A larger \\(\\alpha\\) means more weight is given to recent observations (large weight first and decrease quickly), and a smaller \\(\\alpha\\) means more weight is given to observations from the more distant past (small weight first but decrease slowly). We present two equivalent forms of simple exponential smoothing, each of which leads to the forecast Equation (8.1) 8.1.1 Weighted average form \\[ \\begin{aligned} \\hat{y}_{T+1|T} &amp;= \\alpha y_T + (1 - \\alpha) \\hat{y}_{T|T-1} \\\\ \\hat{y}_{T|T-1} &amp;= \\alpha y_{T-1} + (1 - \\alpha) \\hat{y}_{T-1|T-2} \\\\ \\vdots \\\\ \\hat{y}_{4|3} &amp;= \\alpha y_3 + (1 - \\alpha) \\hat{y}_{3|2} \\\\ \\hat{y}_{3|2} &amp;= \\alpha y_2 + (1 - \\alpha) \\hat{y}_{2|1} \\\\ \\hat{y}_{2|1} &amp;= \\alpha y_1 + (1-\\alpha) l_0 \\end{aligned} \\] Note we denote \\(\\hat{y}_1\\) with \\(\\ell_0\\), which we will have to estimate. Substituting upwards, we get : \\[ \\begin{aligned} \\hat{y}_{2|1} &amp;= \\alpha y_1 + (1-\\alpha) \\ell_0 \\\\ \\hat{y}_{3|2} &amp;= \\alpha y_2 + (1 - \\alpha) (\\alpha y_1 + (1-\\alpha) \\ell_0) = \\alpha y_2 + \\alpha(1 - \\alpha)y_1 + (1 - \\alpha)^2 \\ell_0 \\\\ \\hat{y}_{4|3} &amp;= \\alpha{y}_3 + (1 - \\alpha)(\\alpha y_2 + \\alpha(1 - \\alpha)y_1 + (1 - \\alpha)^2 \\ell_0) = \\alpha{y}_3 + \\alpha(1- \\alpha)y_2 + \\alpha(1 - \\alpha)^2y_1 + (1 -\\alpha)^3\\ell_0 \\\\ \\vdots \\\\ \\hat{y}_{T + 1|T} &amp;= \\sum_{j = 0}^{T-1}{\\alpha(1 - \\alpha)^jy_{T -j}} + (1 - \\alpha)^T \\ell_0 \\end{aligned} \\] When \\(T\\) is large, \\((1 - \\alpha)^T \\ell_0\\) can be ignored. So the least average form approximate the same forecast Equation (8.1). 8.1.2 Component form An alternative representation is the component form. For simple exponential smoothing, the only component included is the level, \\(\\ell\\).1 Component form representations of exponential smoothing methods comprise a forecast equation and a smoothing equation for each of the components included in the method. For \\(h = 1, 2, \\dots\\) (any step of forecast), we have \\[ \\begin{aligned} \\text{Forecast equation} \\;\\;\\;\\; \\hat{y}_{t+h|t} &amp;= \\ell_t \\\\ \\text{Smoothing equation} \\;\\;\\;\\;\\;\\;\\;\\; \\ell_t &amp;= \\alpha y_t + (1 - \\alpha) \\ell_{t-1} \\end{aligned} \\] where \\(\\ell_t\\) is the level (or the smoothed value) of the series at time \\(t\\). Setting \\(h=1\\) gives the fitted values, while setting \\(t=T\\) gives the true forecasts beyond the training data. The forecast equation shows that the forecast value at time \\(t+1\\) is the level at time t, which is essentialy an weighted average of \\(y_t, y_{t-1}, \\dots, y_1\\). For now the component form seems nothing but a change of notations, yet it will be in the foreground once we start to add more components and build a formal statistical model. 8.1.3 Flat forecast Simple exponential smoothing has a “flat” forecast function (recall the component form, change \\(h\\) does not affect the equation: \\[ \\hat{y}_{T + h | T} = \\hat{y}_{T + 1 | T} = \\hat{y}_{T + 2 | T} = \\dots = \\ell_t \\;\\;\\;\\;\\; h = 1, 2, \\dots \\] 8.1.4 Estimation The application of every exponential smoothing method requires the smoothing parameters and the initial values to be chosen. In particular, for simple exponential smoothing, we need to select the values of \\(\\alpha\\) and \\(\\ell_0\\). All forecasts can be computed from the data once we know those values. For the methods that follow there is usually more than one smoothing parameter and more than one initial component to be chosen. In some cases, the smoothing parameters may be chosen in a subjective manner — the forecaster specifies the value of the smoothing parameters based on previous experience. However, a more reliable and objective way to obtain values for the unknown parameters is to estimate them from the observed data. We find the values of the unknown parameters and the initial values that minimise \\[ \\text{SSE} = \\sum_{t = 1}^T{y_t - \\hat{y}_{t |t-1 }} = \\sum_{t = 1}^T{e_t^2} \\] An alternative to estimating the parameters by minimising the sum of squared errors is the maximum likelihood estimation. This method requires the probability distribution on the part of the response variable \\(y\\), which follows a normal distribution assuming normally distributed errors. This is also discussed in Section 8.5. 8.1.5 Example: Algerian exports In this example, simple exponential smoothing is applied to forecast exports of goods and services from Algeria. # estimate parameters # default estimation: opt_crit = &quot;lik&quot; algeria_fit &lt;- algeria_economy %&gt;% model(ETS(Exports ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;), opt_crit = &quot;mse&quot;)) algeria_fit %&gt;% tidy() #&gt; # A tibble: 2 x 4 #&gt; Country .model term estimate #&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 Algeria &quot;ETS(Exports ~ error(\\&quot;A\\&quot;) + trend(\\&quot;N\\&quot;) + season(\\&quot;~ alpha 0.840 #&gt; 2 Algeria &quot;ETS(Exports ~ error(\\&quot;A\\&quot;) + trend(\\&quot;N\\&quot;) + season(\\&quot;~ l 39.5 algeria_fit %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(algeria_economy, level = 95) + geom_line(aes(y = .fitted, color = &quot;Fitted&quot;), data = augment(algeria_fit)) + scale_color_discrete(name = &quot;&quot;) algeria_fit &lt;- algeria_economy %&gt;% model(ETS(Exports ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;), opt_crit = &quot;mse&quot;)) algeria_fit %&gt;% report() #&gt; Series: Exports #&gt; Model: ETS(A,N,N) #&gt; Smoothing parameters: #&gt; alpha = 0.839812 #&gt; #&gt; Initial states: #&gt; l #&gt; 39.54013 #&gt; #&gt; sigma^2: 35.6301 #&gt; #&gt; AIC AICc BIC #&gt; 446.7154 447.1599 452.8968 This gives parameter estimates \\(\\alpha = 0.84\\) and \\(\\ell_0 = 39.5\\), obtained by minimising SSE over periods \\(t = 1, 2, \\dots, 58\\), subject to the restriction that \\(0 \\le \\alpha \\le 1\\). The large value of \\(\\alpha\\) in this example is reflected in the large adjustment that takes place in the estimated level \\(\\ell_t\\) at each time. A smaller value of α would lead to smaller changes over time, and so the series of fitted values would be smoother. algeria_fit %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(algeria_economy, level = 95) + geom_line(aes(y = .fitted, color = &quot;Fitted&quot;), data = augment(algeria_fit)) + scale_color_discrete(name = &quot;&quot;) 8.2 Methods with trend and seasonality 8.2.1 Holt’s linear trend method Holt (1957) extended simple exponential smoothing to allow the forecasting of data with a trend. This method involves a forecast equation and two smoothing equations (one for the level and one for the trend): \\[ \\begin{aligned} \\text{Forecast equation} \\;\\;\\; \\hat{y}_{t+h|t} &amp;= \\ell_t + hb_t \\\\ \\text{Level equation} \\;\\;\\;\\;\\;\\;\\;\\; \\ell_t &amp;= \\alpha y_t + (1 - \\alpha) (\\ell_{t-1} + b_{t-1}) \\\\ \\text{Trend equation} \\;\\;\\;\\;\\;\\;\\; b_t &amp;= \\beta^*(\\ell_t - \\ell_{t - 1}) + (1 - \\beta^*)b_{t-1} \\end{aligned} \\] \\(b_t\\) denotes the estimated slope of the series at time \\(t\\), and \\(\\beta^*\\) is a smoothing parameter for the trend \\(0\\le \\beta^* \\le 1\\). For \\(b_t\\) is an essentially weighted average of slope at \\(t=1, t = 2, \\cdots, t = t - 1\\). The following equation shows thta \\(\\beta_0^*(\\ell_t - \\ell_{t - 1})\\) is weight \\(\\beta_0^*\\) attatched to the estimated slope at time \\(t\\) \\[ \\begin{split} \\ell_t - \\ell_{t - 1} &amp;= [(\\hat{y}_{t+1|t} - b_t) - (\\hat{y}_{t|t-1} - b_{t-1})] \\\\ &amp;= \\hat{y}_{t+1|t} - \\hat{y}_{t|t-1} - (b_t - b_{t-1}) \\\\ &amp;= \\frac{(\\hat{y}_{t+1|t} - \\hat{y}_{t|t-1})}{1} + \\frac{(b_t - b_{t-1})}{1} \\end{split} \\] In Holt’s linear trend, the level equation here shows that \\(\\ell_t\\) is a weighted average of observation \\(y_t\\) and the one-step-ahead training forecast for time \\(t\\), here given by the level \\(\\ell_t\\) at time plus a rise after one observation unit \\(b_t \\times 1\\). The trend equation shows that \\(b_t\\) is a weighted average of the estimated trend at time \\(t\\) based on \\(\\ell_t - \\ell_{t-1}\\) and \\(b_{t−1}\\), the previous estimate of the trend. With the introduction of the trend component, now there are 4 parameters that have to be estimated. Two smoothing parameters \\(\\alpha\\), \\(\\beta^*\\) and two initials \\(\\ell_0\\), \\(b_0\\) The forecast function is no longer flat but trending. The \\(h\\) -step-ahead forecast is equal to the last estimated level plus \\(h\\) times the last estimated trend value. Hence the forecasts are a linear function of \\(h\\). 8.2.2 Example: Australian population aus_economy &lt;- global_economy %&gt;% filter(Code == &quot;AUS&quot;) %&gt;% mutate(Pop = Population / 1e6) pop_fit &lt;- aus_economy %&gt;% model(ETS(Pop ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;), opt_crit = &quot;mse&quot;)) pop_fit %&gt;% report() #&gt; Series: Pop #&gt; Model: ETS(A,A,N) #&gt; Smoothing parameters: #&gt; alpha = 0.9998998 #&gt; beta = 0.3267372 #&gt; #&gt; Initial states: #&gt; l b #&gt; 10.05413 0.2225002 #&gt; #&gt; sigma^2: 0.0041 #&gt; #&gt; AIC AICc BIC #&gt; -76.98568 -75.83184 -66.68347 The estimated smoothing coefficient for the level is \\(\\alpha = 1\\). The very high value shows that the level changes rapidly in order to capture the highly trended series. The estimated smoothing coefficient for the slope is \\(\\beta^*= \\alpha \\beta= 1 \\times0.33 = 0.33\\) (See ETS(A, A, N) in Section 8.4). pop_fit %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(aus_economy) + geom_line(aes(y = .fitted, color = &quot;.fitted&quot;), data = augment(pop_fit)) + scale_color_discrete(name = &quot;&quot;) 8.2.3 Damped trend methods The forecasts generated by Holt’s linear method display a constant trend (increasing or decreasing) indefinitely into the future. Empirical evidence indicates that these methods tend to over-forecast, especially for longer forecast horizons. Motivated by this observation, Gardner &amp; McKenzie {-gardner1985forecasting} introduced a parameter that “dampens” the trend to a flat line some time in the future. Methods that include a damped trend have proven to be very successful, and are arguably the most popular individual methods when forecasts are required automatically for many series. In conjunction with the smoothing parameters \\(\\alpha\\) and \\(\\beta^*\\) (with values between 0 and 1 as in Holt’s method), this method also includes a damping parameter \\(0 \\lt \\phi &lt; 1\\): \\[ \\begin{aligned} \\text{Forecast equation} \\;\\;\\; \\hat{y}_{t+h|t} &amp;= \\ell_t + (\\phi + \\phi^2 + \\cdots + \\phi^h)b_t \\\\ \\text{Level equation} \\;\\;\\;\\;\\;\\;\\;\\; \\ell_t &amp;= \\alpha y_t + (1 - \\alpha) (\\ell_{t-1} + \\phi b_{t-1})\\\\ \\text{Trend equation} \\;\\;\\;\\;\\;\\;\\; b_t &amp;= \\beta^*(\\ell_t - \\ell_{t - 1}) + (1 - \\beta^*)b_{t-1} \\end{aligned} \\] if \\(\\phi = 1\\), the method is identical to Holt’s linear method. For values between 0 and 1, \\(\\phi\\) dampens the trend so that it approaches a constant some time in the future. To be precise, short-run forecasts are trended while long-run forecasts are constant. 8.2.4 Example: Australian Population (continued) aus_economy %&gt;% model( `Holt&#39;s method` = ETS(Pop ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;)), `Damped Holt&#39;s method` = ETS(Pop ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;, phi = 0.9) + season(&quot;N&quot;)) ) %&gt;% forecast(h = 15) %&gt;% autoplot(aus_economy, level = NULL) + labs(title = &quot;Forecasts from Holt&#39;s method&quot;, x = &quot;Year&quot;, y = &quot;Population of Australia (millions)&quot;) + guides(colour = guide_legend(title = &quot;Forecast&quot;)) We have set the damping parameter to a relatively low number (\\(\\phi = 0.90\\)) to exaggerate the effect of damping for comparison. Usually, we would estimate \\(\\phi\\) (simply trend(\"Ad\")) along with the other parameters. We have also used a rather large forecast horizon (\\(h = 15\\)) to highlight the difference between a damped trend and a linear trend. 8.2.5 Example: Internet usage In this example, we compare the forecasting performance of the three exponential smoothing methods that we have considered so far in forecasting the number of users connected to the internet via a server. www_usage &lt;- as_tsibble(WWWusage) www_usage %&gt;% autoplot(value) + xlab(&quot;Minute&quot;) + ylab(&quot;Number of users&quot;) We will use time series cross-validation to compare the one-step forecast accuracy of the three methods. www_usage %&gt;% stretch_tsibble(.init = 10, .step = 1) %&gt;% model( SES = ETS(value ~ error(&quot;A&quot;) + trend(&quot;N&quot;) + season(&quot;N&quot;)), Holt = ETS(value ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;N&quot;)), Damped = ETS(value ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;)) ) %&gt;% forecast(h = 1) %&gt;% accuracy(www_usage) #&gt; # A tibble: 3 x 9 #&gt; .model .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 Damped Test 0.288 3.69 3.00 0.347 2.26 0.663 0.336 #&gt; 2 Holt Test 0.0610 3.87 3.17 0.244 2.38 0.701 0.296 #&gt; 3 SES Test 1.46 6.05 4.81 0.904 3.55 1.06 0.803 Damped Holt’s method is best whether you compare MAE or RMSE values. So we will proceed with using the damped Holt’s method and apply it to the whole data set to get forecasts for future years. usage_fit &lt;- www_usage %&gt;% model(ETS(value ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;N&quot;))) usage_fit %&gt;% tidy() #&gt; # A tibble: 5 x 3 #&gt; .model term estimate #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 &quot;ETS(value ~ error(\\&quot;A\\&quot;) + trend(\\&quot;Ad\\&quot;) + season(\\&quot;N\\&quot;))&quot; alpha 1.00 #&gt; 2 &quot;ETS(value ~ error(\\&quot;A\\&quot;) + trend(\\&quot;Ad\\&quot;) + season(\\&quot;N\\&quot;))&quot; beta 0.997 #&gt; 3 &quot;ETS(value ~ error(\\&quot;A\\&quot;) + trend(\\&quot;Ad\\&quot;) + season(\\&quot;N\\&quot;))&quot; phi 0.815 #&gt; 4 &quot;ETS(value ~ error(\\&quot;A\\&quot;) + trend(\\&quot;Ad\\&quot;) + season(\\&quot;N\\&quot;))&quot; l 90.4 #&gt; 5 &quot;ETS(value ~ error(\\&quot;A\\&quot;) + trend(\\&quot;Ad\\&quot;) + season(\\&quot;N\\&quot;))&quot; b -0.0173 usage_fit %&gt;% report() #&gt; Series: value #&gt; Model: ETS(A,Ad,N) #&gt; Smoothing parameters: #&gt; alpha = 0.9999 #&gt; beta = 0.9966439 #&gt; phi = 0.814958 #&gt; #&gt; Initial states: #&gt; l b #&gt; 90.35177 -0.01728234 #&gt; #&gt; sigma^2: 12.2244 #&gt; #&gt; AIC AICc BIC #&gt; 717.7310 718.6342 733.3620 The smoothing parameter for the slope is estimated to be almost one, indicating that the trend changes to mostly reflect the slope between the last two minutes of internet usage. The decline in the last few years is captured by large \\(\\beta^*\\), so that \\(b_{T+1}, b_{T+2}, \\dots, b_{T+10}\\) is all negative. \\(\\alpha\\) is very close to one, showing that the level reacts strongly to each new observation. usage_fit %&gt;% forecast(h = 10) %&gt;% autoplot(www_usage) 8.2.6 Holt-Winters’ additive method Holt (1957)and Winters (1960) extended Holt’s method to capture seasonality. The Holt-Winters seasonal method comprises the forecast equation and three smoothing equations — one for the level \\(\\ell_t\\), one for the trend \\(b_t\\), and one for the seasonal component \\(s_t\\), with corresponding smoothing parameters \\(\\alpha\\), \\(\\beta^*\\) and \\(\\gamma\\). We use \\(m\\) to denote the frequency of the seasonality, i.e., the number of seasons in a year. For example, for quarterly data \\(m = 4\\), and for monthly data \\(m = 12\\). There are two variations to this method that differ in the nature of the seasonal component. The additive method is preferred when the seasonal variations are roughly constant through the series, while the multiplicative method is preferred when the seasonal variations are changing proportional to the level of the series. \\[ \\begin{aligned} \\hat{y}_{t + h | t} &amp;= \\ell_t + hb_t + s_{t + h -m(k + 1)} \\\\ \\ell_t &amp;= \\alpha(y_t - s_{t - m}) + (1 - \\alpha)(\\ell_{t - 1} + b_{t - 1}) \\\\ b_t &amp;= \\beta^*(\\ell_t - \\ell_{t - 1}) + (1 - \\beta^*)b_{t-1} \\\\ s_t &amp;= \\gamma(y_t - \\ell_{t-1} - b_{t - 1}) + (1 - \\gamma)s_{t-m} \\\\ \\end{aligned} \\] where \\(k\\) is the integer part of \\((h − 1) / m\\), which ensures that the estimates of the seasonal indices used for forecasting come from the final year of the sample. The level equation shows a weighted average between the seasonally adjusted observation (\\(y_t−s_{t−m}\\)) and the non-seasonal forecast (\\(\\ell_{t−1}+b_{t−1}\\)) for time t. The trend equation is identical to Holt’s linear method. The seasonal equation shows a weighted average between the current seasonal index, (\\(y_t - \\ell_{t-1} - b_{t - 1}\\)), and the seasonal index of the same season last year (i.e., \\(m\\) time periods ago). This means we have \\(m\\) more inital values to estimate \\(s_1, s_2, \\dots, s_m\\) in terms of the seasonal component. The equation for the seasonal component can be also expressed as \\[ s_t = \\gamma^*(y_t - \\ell_t) + (1 - \\gamma^*)s_{t-m} \\] This is the case when we substitute the level equation for \\(\\ell_t\\): \\[ \\begin{split} s_t &amp;= \\gamma^*(y_t - \\ell_t) + (1 - \\gamma^*)s_{t-m} \\\\ &amp;= \\gamma^*[y_t - \\alpha(y_t - s_{t - m}) - (1 - \\alpha)(\\ell_{t - 1} + b_{t - 1})] + (1 - \\gamma^*)s_{t-m} \\\\ &amp;= \\gamma^*y_t - \\gamma^*y_t\\alpha + \\gamma^* \\alpha s_{t - m} - \\gamma^*(1 - \\alpha)(\\ell_{t - 1} + b_{t - 1}) + (1 - \\gamma^*)s_{t-m} \\\\ &amp;= \\gamma^*(1 - \\alpha)y_t - \\gamma^*(1 - \\alpha)(\\ell_{t - 1} + b_{t - 1}) + \\gamma^* \\alpha s_{t - m} + [1 + y^*(\\alpha - 1)]s_{t-m} \\\\ &amp;= \\gamma^*(1 - \\alpha)(y_t - \\ell_{t-1} - b_{t-1}) + [1 - y^*(1 - \\alpha)]s_{t-m} \\end{split} \\] which is identical to the smoothing equation for the seasonal component we specify here, with \\(\\gamma = \\gamma^*(1 - \\alpha)\\). The usual parameter restriction is \\(0 \\le \\gamma^* \\le 1\\), which translates to \\(0 \\le \\gamma \\le 1− \\alpha\\). 8.2.7 Holt-Winters’ multiplicative method The component form for the multiplicative method is: \\[ \\begin{aligned} \\hat{y}_{t + h | t} &amp;= (\\ell_t + hb_t)s_{t + h -m(k + 1)} \\\\ \\ell_t &amp;= \\alpha(y_t / s_{t - m}) + (1 - \\alpha)(\\ell_{t - 1} + b_{t - 1}) \\\\ b_t &amp;= \\beta^*(\\ell_t - \\ell_{t - 1}) + (1 - \\beta^*)b_{t-1} \\\\ s_t &amp;= \\gamma[y_t / (\\ell_{t-1} + b_{t - 1})] + (1 - \\gamma)s_{t-m} \\\\ \\end{aligned} \\] 8.2.8 Example: Domestic overnight trips in Australia Here we use cross validation to compare the forecast of additive seasonality with that of multiplicative seasonality. aus_holidays &lt;- tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarize(Trips = sum(Trips)) holidays_fit &lt;- aus_holidays %&gt;% model( additive = ETS(Trips ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;A&quot;)), multiplicative = ETS(Trips ~ error(&quot;A&quot;) + trend(&quot;A&quot;) + season(&quot;M&quot;)) ) holidays_fit %&gt;% glance() #&gt; # A tibble: 2 x 9 #&gt; .model sigma2 log_lik AIC AICc BIC MSE AMSE MAE #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 additive 189416. -657. 1332. 1335. 1354. 170475. 180856. 315. #&gt; 2 multiplicative 187599. -657. 1331. 1334. 1353. 168839. 179731. 307. holidays_fit %&gt;% forecast(h = &quot;3 years&quot;) %&gt;% autoplot(aus_holidays, level = NULL) Because both methods have exactly the same number of parameters to estimate, we can compare the training RMSE from both models. In this case, the method with multiplicative seasonality fits the data best. This was to be expected, as the time plot shows that the seasonal variation in the data increases as the level of the series increases. This is also reflected in the two sets of forecasts; the forecasts generated by the method with the multiplicative seasonality display larger and increasing seasonal variation as the level of the forecasts increases compared to the forecasts generated by the method with additive seasonality. 8.2.9 Holt-Winters’ damped method Damping is possible with both additive and multiplicative Holt-Winters’ methods. A method that often provides accurate and robust forecasts for seasonal data is the Holt-Winters method with a damped trend and multiplicative seasonality: \\[ \\begin{aligned} \\hat{y}_{t + h | t} &amp;= [\\ell_t + (\\phi + \\phi^2 + \\dots + \\phi^h)b_t]s_{t + h -m(k + 1)} \\\\ \\ell_t &amp;= \\alpha(y_t / s_{t - m}) + (1 - \\alpha)(\\ell_{t - 1} + \\phi b_{t - 1}) \\\\ b_t &amp;= \\beta^*(\\ell_t - \\ell_{t - 1}) + (1 - \\beta^*)\\phi b_{t-1} \\\\ s_t &amp;= \\gamma[y_t / (\\ell_{t-1} + \\phi b_{t - 1})] + (1 - \\gamma)s_{t-m} \\\\ \\end{aligned} \\] 8.2.10 Example: Holt-Winters method with daily data The Holt-Winters method can also be used for daily type of data, where the seasonal period is \\(m = 7\\). Here we forecast pedestrian traffic at a busy Melbourne train station in July 2016. pedestrian_per_day &lt;- pedestrian %&gt;% filter(Sensor == &quot;Southern Cross Station&quot;, yearmonth(Date) == yearmonth(&quot;2016 July&quot;)) %&gt;% index_by(Date) %&gt;% summarise(Count = sum(Count)) pedestrian_fit &lt;- pedestrian_per_day %&gt;% model(ETS(Count ~ error(&quot;A&quot;) + trend(&quot;Ad&quot;) + season(&quot;M&quot;))) pedestrian_fit %&gt;% report() #&gt; Series: Count #&gt; Model: ETS(A,Ad,M) #&gt; Smoothing parameters: #&gt; alpha = 0.1901066 #&gt; beta = 0.002178528 #&gt; gamma = 0.0009006456 #&gt; phi = 0.9729213 #&gt; #&gt; Initial states: #&gt; l b s1 s2 s3 s4 s5 s6 #&gt; 12372.2 94.66631 1.34518 1.322323 1.320987 1.314923 0.144459 0.2077794 #&gt; s7 #&gt; 1.344349 #&gt; #&gt; sigma^2: 184620 #&gt; #&gt; AIC AICc BIC #&gt; 493.1853 514.5971 511.8271 Here we estimate 9 inital values, 1 for level, 1 for slope, and 7 for seasonal index. pedestrian_fit %&gt;% forecast(h = &quot;2 weeks&quot;) %&gt;% autoplot(pedestrian_per_day) 8.3 A taxonomy of exponential smoothing methods By considering variations in the combinations of the trend(\\(N\\), \\(A\\) and \\(A_d\\)) and seasonal components(\\(N\\), \\(A\\), and \\(M\\)), nine exponential smoothing methods are possible, listed in below Multiplicative trend methods are not included as they tend to produce poor forecasts. See R. Hyndman et al. (2008) for a more thorough discussion of all exponential smoothing methods. The following table gives the recursive formulas for applying the nine exponential smoothing methods. Each cell includes the forecast equation for generating h-step-ahead forecasts, and the smoothing equations for applying the method. 8.4 Innovations state space models for exponential smoothing Now we study the statistical models that underlie the exponential smoothing methods we have considered so far. All the exponential smoothing methods presented so far are algorithms which generate point forecasts, instead of a statistical model. The statistical models in this section generate the same point forecasts, but can also generate prediction (or forecast) intervals. A statistical model is a stochastic (or random) data generating process that can produce an entire forecast distribution. Each model consists of a measurement equation that describes the observed data, and some state equations that describe how the unobserved components or states (level, trend, seasonal) change over time. Hence, these are referred to as state space models. For each method there exist two models: one with additive errors and one with multiplicative errors. The point forecasts produced by the models are identical if they use the same smoothing parameter values. They will, however, generate different prediction intervals. Notations : 8.4.1 ETS(A,N,N): simple exponential smoothing with additive errors Recall the simple exponential smoothing Equation (component form, 1-step forecast) : \\[ \\begin{aligned} \\text{Forecast equation} \\;\\;\\;\\; \\hat{y}_{t+1|t} &amp;= \\ell_t \\\\ \\text{Smoothing equation} \\;\\;\\;\\;\\;\\;\\;\\; \\ell_t &amp;= \\alpha y_t + (1 - \\alpha) \\ell_{t-1} \\end{aligned} \\] Let \\(e_t = y_{t} -\\hat{y}_{t|t-1} = y_{t} - \\ell_{t-1}\\), some then substitute \\(e_t + \\ell_{t - 1}\\) for \\(y_{t}\\). We get \\[\\begin{equation} y_t = \\ell_{t - 1} + e_t \\tag{8.2} \\end{equation}\\] \\[\\begin{equation} \\ell_t = \\ell_{t-1} + \\alpha e_t \\tag{8.3} \\end{equation}\\] We refer to Equation (8.2) as the measurement (or observation) equation and Equation (8.3) as the state (or transition) equation. These two equations, together with the statistical distribution of the errors, form a fully specified statistical model. Specifically, these constitute an innovations state space model underlying simple exponential smoothing. The term “innovations” comes from the fact that all equations use the same random error process, \\(\\varepsilon_t\\). For the same reason, this formulation is also referred to as a “single source of error” model. There are alternative multiple source of error formulations that is not presented here. The state equation shows the evolution of the state through time. The influence of the smoothing parameter \\(\\alpha\\) is the same as for the methods discussed earlier. For example, \\(\\alpha\\) governs the amount of change in successive levels: high values of α allow rapid changes in the level; low values of α lead to smooth changes. If \\(\\alpha = 0\\), the level of the series does not change over time; if \\(\\alpha = 1\\), the model reduces to a random walk model, \\(y_t = \\ell_{t-1} + \\varvarepsilon_t = y_{t−1} + \\varepsilon_t\\). (See Section 9.1 for a discussion of this model.) 8.4.2 ETS(M,N,N): simple exponential smoothing with multiplicative errors A multiplicative error is defined as: \\[ \\varepsilon_t = \\frac{y_{t} - \\hat{y}_{t|t-1}}{\\hat{y}_{t|t-1}} \\] where \\(\\varepsilon_t \\sim N(0, \\sigma^2)\\). From the above equaiton we know \\(y_t = \\ell_{t-1}(1 + \\varepsilon_t)\\), so that \\[ \\begin{split} \\ell_t &amp;= \\alpha y_t + (1 - \\alpha) \\ell_{t-1} \\\\ &amp;= \\alpha(1 + \\varepsilon_t)\\ell_{t-1} + (1-\\alpha)\\ell_{t-1} \\\\ &amp;=\\ell_{t-1} (1 + \\alpha\\varepsilon_t) \\end{split} \\] Then we can write the multiplicative form of the state space model as \\[ \\begin{aligned} y_t &amp;= \\ell_{t-1} (1 + \\varepsilon_t)\\\\ l_t &amp;= \\ell_{t-1}(1 + \\alpha\\varepsilon_t) \\end{aligned} \\] 8.4.3 ETS(A,A,N): Holt’s linear method with additive errors Recall in Holt’s linear trend method, we have: \\[ \\begin{aligned} \\hat{y}_{t + h | t} &amp;= \\ell_t + hb_t \\\\ \\ell_t &amp;= \\alpha y_t + (1 - \\alpha)(\\ell_{t - 1} + b_{t - 1}) \\\\ b_t &amp;= \\beta^*(\\ell_t - \\ell_{t - 1}) + (1 - \\beta^*)b_{t-1} \\\\ \\end{aligned} \\] In the second equation, we have \\[ \\begin{split} \\ell_t &amp;= \\alpha (\\ell_{t -1} + b_{t-1} + e_t) + (1 - \\alpha)(\\ell_{t - 1} + b_{t - 1}) \\\\ &amp;= \\ell_{t-1} + b_{t-1} + \\alpha e_t \\end{split} \\] and in the third (from \\(\\ell_t - \\ell_{t-1} = b_{t-1} + \\alpha e_t\\) we just derived) \\[ \\begin{split} b_t &amp;= \\beta^*(b_{t-1} + \\alpha e_t) + (1 - \\beta^*)b_{t-1} \\\\ &amp;= b_{t-1} + \\alpha \\beta^*e_t \\end{split} \\] Finally, assuiming NID errors \\(\\varepsilon_t = e_t \\sim (0, \\sigma^2)\\) and let \\(\\beta = \\alpha\\beta^*\\), we get \\[ \\begin{aligned} y_{t} &amp;= \\ell_{t- 1} + b_{t-1} + \\varepsilon_t \\\\ \\ell_t &amp;= \\ell_{t-1} + b_{t-1} + \\alpha \\varepsilon_t \\\\ b_t &amp;= b_{t-1} + \\beta \\varepsilon_t \\end{aligned} \\] 8.4.4 ETS(M,A,N): Holt’s linear method with multiplicative errors Specifying one-step-ahead training errors as relative errors such that \\[ \\varepsilon_t = \\frac{y_t - (\\ell_{t-1} + b_{t-1})}{(\\ell_{t-1} + b_{t-1})} \\] and that \\(y_t = (1 + \\varepsilon_t)(\\ell_{t-1} + b_{t-1})\\), so \\[ \\begin{split} \\ell_t &amp;= \\alpha(1 + \\varepsilon_t)(\\ell_{t-1} + b_{t-1}) + (1 - \\alpha)(\\ell_{t-1} + b_{t-1}) \\\\ &amp;= (1 + \\alpha \\varepsilon_t) \\ell_{t-1} + (1 + \\alpha \\varepsilon_t)b_{t-1} \\\\ &amp;= (\\ell_{t-1} + b_{t-1})(1 + \\alpha \\varepsilon_t) \\end{split} \\] and \\[ \\begin{split} b_t &amp;= \\beta^*(\\ell_t - \\ell_{t-1}) + (1 - \\beta^*)b_{t-1} \\\\ &amp;= \\beta^* \\ell_t + b_{t-1} - \\beta^* (\\ell_{t-1} + b_{t-1}) \\\\ &amp;= \\beta^* (\\ell_{t-1} + b_{t-1})(1 + \\alpha \\varepsilon_t) + b_{t-1} - \\beta^* (\\ell_{t-1} + b_{t-1}) \\\\ &amp;= \\alpha\\beta^*(\\ell_{t-1} + b_{t-1})\\varepsilon_t + b_{t-1} \\\\ &amp;= b_{t-1} + \\beta(\\ell_{t-1} + b_{t-1})\\varepsilon_t \\end{split} \\] And our final state space model is: \\[ \\begin{aligned} y_t &amp;= (\\ell_{t-1} + b_{t-1})(1 + \\varepsilon_t) \\\\ \\ell_t &amp;= (\\ell_{t-1} + b_{t-1})(1 + \\alpha \\varepsilon_t)\\\\ b_t &amp;= b_{t-1} + \\beta(\\ell_{t-1} + b_{t-1})\\varepsilon_t \\end{aligned} \\] 8.4.5 Other ETS models In a similar fashion, we can write an innovations state space model for each of the exponential smoothing methods in the following table 8.5 Estimation and model selection 8.5.1 Estimating ETS models In Section 8.1.4 we use opt_crit = \"mse\" to estimate smoothing parameters and initial values. However, the default method is maximum likelihood estimation. In this section, we will estimate the smoothing parameters \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\) and \\(\\phi\\), and the initial states \\(\\ell_0\\), \\(b_9\\), \\(s_0\\),\\(s_1\\), \\(\\dots\\), \\(s_{m-1}\\), by maximising the likelihood. The possible values that the smoothing parameters can take are restricted. Traditionally, the parameters have been constrained to lie between 0 and 1 so that the equations can be interpreted as weighted averages. That is, \\(0 &lt;\\alpha,\\beta^*,\\gamma^*,\\phi &lt; 1\\)(Why \\(\\gamma^*\\) instead of \\(\\gamma\\)?, 8.2.6). For the state space models, we have set \\(\\beta = \\alpha \\beta^∗\\) and \\(\\gamma = (1− \\alpha)\\gamma^*\\). Therefore, the traditional restrictions translate to \\[ \\begin{aligned} 1 \\lt &amp;\\alpha \\lt 1 \\\\ 0 \\lt &amp;\\beta \\lt \\alpha \\\\ 0 \\lt &amp;\\gamma \\lt 1- \\alpha \\end{aligned} \\] In practice, the damping parameter \\(\\phi\\) is usually constrained further to prevent numerical difficulties in estimating the model. In R, it is restricted so that \\(0.8 &lt; \\phi &lt; 0.98\\). Another way to view the parameters is through a consideration of the mathematical properties of the state space models. The parameters are constrained in order to prevent observations in the distant past having a continuing effect on current forecasts. From this standing points, restrictions are usually (but not always) looser. For example, for the ETS(A, N, N) model, the traditional parameter region is \\(0 &lt; \\alpha &lt; 1\\) but the admissible region is \\(0 &lt; \\alpha &lt; 1\\). For the ETS(A, A, N) model, the traditional parameter region is \\(0 &lt; \\alpha &lt; 1\\) and \\(0 &lt; \\beta &lt;\\alpha\\) but the admissible region is \\(0 &lt; \\alpha &lt;2\\) and \\(0&lt; \\beta &lt; 4−2\\alpha\\). 8.5.2 Model selection criteria \\(\\text{AIC}\\), \\(\\text{AIC}_c\\)and \\(\\text{BIC}\\), introduced in Section 7.5, can be used here to determine which of the ETS models is most appropriate for a given time series. For ETS models, Akaike’s Information Criterion (AIC) is defined as \\[ \\text{AIC} = -2\\log{(L)} + 2k \\] where \\(L\\) is the likelihood of the model and \\(k\\) is the total number of parameters and initial states that have been estimated (including the residual variance). The AIC corrected for small sample bias (\\(\\text{AIC}_c\\)) is defined as \\[ \\text{AIC}_c = \\text{AIC} + \\frac{2k(k + 1)}{T - k - 1} \\] and the Bayesian Information Criterion (BIC) is \\[ \\text{BIC} = \\text{AIC} + k[\\log{(T)} - 2] \\] Three of the combinations of (Error, Trend, Seasonal) can lead to numerical difficulties. Specifically, the models that can cause such instabilities are multiplicative seasonality and additive error, ETS(A, N, M), ETS(A, A, M), and ETS(A, Ad, M), due to division by values potentially close to zero in the state equations. We normally do not consider these particular combinations when selecting a model. Models with multiplicative errors are useful when the data are strictly positive, but are not numerically stable when the data contain zeros or negative values. Therefore, multiplicative error models will not be considered if the time series is not strictly positive. In that case, only the six fully additive models will be applied. 8.5.3 Example: Domestic holiday tourist visitor nights in Australia If not explicitly set error(), trend() or season(), ETS() use MLE to estimtate the corresponding parameters. aus_holidays &lt;- tourism %&gt;% filter(Purpose == &quot;Holiday&quot;) %&gt;% summarise(Trips = sum(Trips)) holidays_fit &lt;- aus_holidays %&gt;% model(ETS(Trips)) holidays_fit %&gt;% report() #&gt; Series: Trips #&gt; Model: ETS(M,N,M) #&gt; Smoothing parameters: #&gt; alpha = 0.3578226 #&gt; gamma = 0.0009685565 #&gt; #&gt; Initial states: #&gt; l s1 s2 s3 s4 #&gt; 9666.501 0.9430367 0.9268433 0.968352 1.161768 #&gt; #&gt; sigma^2: 0.0022 #&gt; #&gt; AIC AICc BIC #&gt; 1331.372 1332.928 1348.046 The model selected is ETS(M, N, M) (Trips are strictly positive): \\[ \\begin{aligned} y_t &amp;= \\ell_{t-1}s_{t-m}(1 + \\varepsilon_t) \\\\ \\ell_t &amp;= \\ell_{t-1}(1 + \\alpha \\varepsilon_ t) \\\\ s_t &amp;= s_{t-1}(1 + \\gamma \\varepsilon_t) \\end{aligned} \\] holidays_fit %&gt;% components() %&gt;% autoplot() Because this model has multiplicative errors, the residuals are not equivalent to the one-step training errors. The residuals are given by \\(\\hat{\\varepsilon}_t\\), while the one-step training errors are defined as \\(y_t − \\hat{y}_{t|t−1}\\). residuals(holidays_fit) %&gt;% autoplot() + ggtitle(&quot;Innovation errors&quot;) residuals(holidays_fit, type = &quot;response&quot;) %&gt;% autoplot() + ggtitle(&quot;Response errors&quot;) 8.6 Forecasting with ETS models For model ETS(M, A, N), we have \\(y_{T+1} = (l_t + b_t)(1 + \\varepsilon_t)\\). Therefore \\(\\hat{y}_{T+1} = l_t + b_t\\). Similarly (check Section 8.4.4 for formula of \\(\\ell_{T+1}\\) and \\(b_{T+1}\\)) \\[ \\begin{split} y_{T+2} &amp;= (\\ell_{T + 1} + b_{T + 1})(1 + \\varepsilon_{T+1}) \\\\ &amp;= [(\\ell_{T} + b_T)(1 + \\alpha \\varepsilon_t) + b_T + \\beta(\\ell_T + b_T)\\varepsilon_t] (1 + \\varepsilon_{T+1}) \\end{split} \\] By setting \\(\\varepsilon_{T+1} = 0\\), we get \\(y_{T+2} =\\ell_T + 2b_T\\), which is the same to Holt’s linear methodin Section 8.2.1, where innovation state space model is not formally introduced. Thus, the point forecasts obtained from the method and from the two models that underlie the method are identical (assuming that the same parameter values are used). ETS point forecasts are equal to the medians of the forecast distributions. For models with only additive components, the forecast distributions are normal, so the medians and means are equal. For ETS models with multiplicative errors, or with multiplicative seasonality, the point forecasts will not be equal to the means of the forecast distributions. holidays_fit %&gt;% forecast(h = 8) %&gt;% autoplot(aus_holidays, level = 95) 8.6.1 Example: Australia gas production gas_fit &lt;- aus_production %&gt;% model(ETS(Gas)) gas_fit %&gt;% report() #&gt; Series: Gas #&gt; Model: ETS(M,A,M) #&gt; Smoothing parameters: #&gt; alpha = 0.6528545 #&gt; beta = 0.1441675 #&gt; gamma = 0.09784922 #&gt; #&gt; Initial states: #&gt; l b s1 s2 s3 s4 #&gt; 5.945592 0.07062881 0.9309236 1.177883 1.074851 0.8163427 #&gt; #&gt; sigma^2: 0.0032 #&gt; #&gt; AIC AICc BIC #&gt; 1680.929 1681.794 1711.389 Why is multiplicative seasonality necessary here? A model ETS(M, A, M) gas_fit %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(aus_production, level = 95) Experiment with making the trend damped, not improving \\(\\text{AIC}_c\\) gas_fit_damped &lt;- aus_production %&gt;% model(ETS(Gas ~ trend(&quot;Ad&quot;))) gas_fit_damped %&gt;% report() #&gt; Series: Gas #&gt; Model: ETS(M,Ad,M) #&gt; Smoothing parameters: #&gt; alpha = 0.6489044 #&gt; beta = 0.1551275 #&gt; gamma = 0.09369372 #&gt; phi = 0.98 #&gt; #&gt; Initial states: #&gt; l b s1 s2 s3 s4 #&gt; 5.858941 0.09944006 0.9281912 1.177903 1.07678 0.8171255 #&gt; #&gt; sigma^2: 0.0033 #&gt; #&gt; AIC AICc BIC #&gt; 1684.028 1685.091 1717.873 8.6.2 Prediction intervals Since \\(y_{T + h| T} = \\hat{y}_{T+h|h} + \\varepsilon_{T+h}\\), and h-step residuals are assumed to be normally distributed and have standard deviation \\(\\hat{\\sigma}_h\\), mean 0, we have \\[ y_{T + h| T} \\sim N(\\hat{y}_{T+h|h},\\hat{\\sigma}_h^2) \\] (For the purpose of reviewing, in section 5.5 we conclude that when forecasting is one step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals. And for multi-step forecast, \\(\\sigma_h\\) usually increases with \\(h\\), and some more complex estimate methods may be required). For most ETS models, a prediction interval can be written as: \\[ \\hat{y}_{T+h|h} \\pm c\\hat{\\sigma}_h \\] where \\(c\\) depends on the coverage probability. For ETS models, formulas for \\(\\sigma_h\\) can be complicated; the details are given in Chapter 6 of https://robjhyndman.com/expsmooth/. In the following table we give the formulas for the additive ETS models, which are the simplest. Forecast variance expressions for each additive state space model, where \\(\\sigma_h^2\\) is the residual variance of a h-step forecast, \\(m\\) is the seasonal period, and \\(k\\) is the integer part of \\((h−1)/m\\) (i.e., the number of complete years in the forecast period prior to time \\(T+h\\)) \\(\\ell\\) is just styled \\(l\\), \\ell in latex↩︎ "],
["univariate-stationary-processes.html", "Chapter 9 Univariate stationary processes 9.1 Stationarity 9.2 Backshift notation 9.3 Autoregressive models 9.4 Moving average models 9.5 ARMA models", " Chapter 9 Univariate stationary processes library(tsibble) library(tsibbledata) library(fable) library(feasts) library(lubridate) library(pins) library(slider) library(patchwork) 9.1 Stationarity Given a stochastic process (a data generating process), \\(y_1, y_2, \\dots, y_T\\), the real numbers called time series \\(y_1^{(1)}, y_2^{(1)}, \\cdots, y_T^{(1)}\\) is just an set of observed value, or a realization of the process.It is obvious, however, that there is not just one realisation of such a process, but, in principle, an arbitrary number of realisations which all have the same statistical properties as they all result from the same data generating process. In the following, a time series is considered as one realisation of the underlying stochastic process. We can also regard the stochastic process as the entirety of all of its possible realisations. To make the notation as simple as possible, we will not distinguish between the process itself and itsrealisation. This can be taken out of the context. We can define a strict-sense stationarity of a time series to be : \\[ \\begin{aligned} F_Y(y_{t_1+\\alpha}, y_{t_2+\\alpha}, \\dots, y_{t_n+\\alpha}) = F_Y(y_{t_1+}, y_{t_2}, \\dots, y_{t_n}) \\end{aligned} \\] Where \\(F_Y(*)\\) stands for the multivariate distribution of a specific window of that time series. This basically means the distribution stays the same stays same when you pick a window on that time series and then make a shift \\(\\alpha\\).In the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time. However, strict-sense stationarity are rarely practical or helpful in most cases. For this reason we place more emphasis on weak or wide-sense stationarity. A stochastic process is weak stationary if \\(\\forall k\\) we have: \\[ \\begin{aligned} \\text{mean stationarity}: \\text{E}(y_t) &amp;= \\mu_t = \\mu\\\\ \\text{variance stationarity}:\\text{Var}(y_t) &amp;= \\sigma_t^2 = \\sigma^2 \\\\ \\text{covariance stationarity}: \\text{Cov}(y_t, y_s) &amp;= \\text{E}(y_t - \\mu)(y_s - \\mu) = f(|s-t|) \\end{aligned} \\] (covariance stationarity means that covariance is only a function of the distance \\(|s-t|\\), but not related to point \\(t\\)). Because we only assume this kind of stationarity in the following, we will mostly drop the adjective weak. As variance stationarity immediately results from covariance stationarity for \\(s = t\\), a stochastic process is weakly stationary when it is mean and covariance stationary. We can also refer to a time series to be stationary if the underlying stochastic process is stationary. So for a stationary time series, what do we expect to see? No trend and seasonality (constant mean, local compared to global) No sharp rise and fall (constant variance, local compared to global) Why does stationarity matter Why is this important? First, because stationary processes are easier to analyze. Without a formal definition for processes generating time series data, it is already clear that stationary processes are a sub-class of a wider family of possible models of reality. This sub-class is much easier to model and investigate. The above informal definition also hints that such processes should be possible to predict, as the way they change is predictable. Although it sounds a bit streetlight effect-ish that simpler theories or models should become more prominent, it is actually quite a common pattern in science, and for good reason. In many cases simple models can be surprisingly useful, either as building blocks in constructing more elaborate ones, or as helpful approximations to complex phenomena. As it turns out, this also true for stationary processes. Due to these properties, stationarity has become a common assumption for many practices and tools in time series analysis. These include trend estimation, forecasting and causal inference, among others. 9.1.1 White noise White noise is first introduced in Section 5.4.1. As it turns out, the stochastic process behind a white noise is called a pure random process or simply white noise process. Such a process satisfies \\(\\text{E}(y_t) = 0\\), \\(\\text{Var}(y_t) = \\sigma^2\\) and \\(\\forall k \\not= 0, \\text{Cov}(y_t, y_{t+k}) = 0\\). Apparently, white noise is generated by a special case of stationary process. Notably it should have contant mean 0, constant variance \\(\\sigma^2\\) and no autocorrelation. 9.1.2 Tests for autocorrelation and normality In Section 5.4.3 we mentioned statistical tests to decide whether there is no significant correlation among residuals. And here are some details. First, it is important to know that white noise, like all stationary time series, has some nice consistent estimators to estimate its mean, variance and covariance: \\[ \\begin{aligned} \\hat{\\mu} &amp;= \\frac{1}{T}\\sum_{t=1}^{T}y_t \\\\ \\hat{\\gamma}(0) = \\hat{\\sigma^2} &amp;= \\frac{1}{T}\\sum_{t=1}^{T}(y_t - \\hat{\\mu})^2 \\\\ \\hat{\\gamma}(k) = \\hat{\\text{Cov}}(y_t, y_{t+k}) &amp;= \\frac{1}{T}\\sum_{t=1}^{T-k}{(y_t - \\hat{\\mu})(y_{t+k} - \\hat{\\mu})} \\quad k = 1, 2, 3, \\dots \\end{aligned} \\] We can also get the consistent estimator of autocorrelation coefficient: \\[ r_k = \\frac{\\sum_{t=1}^{T-k} = {(y_t - \\hat{\\mu})(y_{t+k} - \\hat{\\mu})}}{\\sum_{t=1}^{T}(y_t - \\hat{\\mu})^2} = \\frac{\\hat{\\gamma}(k)}{\\hat{\\gamma}(0)} \\] For white noise processes, its variance can be approximated by \\(1/T\\) and is asymptotically normally distributed. Due to this, pointwise \\(95\\) percent confidence intervals of \\(\\pm1.96 /\\sqrt{T}\\) ? are often indicated for the estimated autocorrelation coefficients. In order to evaluate estimated time series models, it is important to know whether the residuals of the model really have the properties of a pure random process, in particular, whether they are uncorrelated. Thus, the null hypothesis to be tested is the first \\(h\\) \\(r_k\\) is zero: \\[ H_0: r_1 = r_2 = \\cdots = r_h, h&lt;T \\] The first possibility to check this is to apply the 95 percent confidence limits \\(\\pm2/\\sqrt{T}\\) valid under the null hypothesis to every estimated correlation coefficient. Under \\(H_0\\) at most 5 percent of autocorrelation coefficients may lie outside these limits. To make a global statement, i.e. to test the common hypothesis whether a given number of \\(h\\) autocorrelation coefficients are zero altogether, GEORGE E.P. BOX and DAVID A. PIERCE (1970) have developed the following test statistic: \\[ Q = T \\sum_{k = 1}^{h}{r^2_k} \\] Under the null hypothesis it is asymptotically \\(\\chi^2\\) distributed with \\(h - K\\) degrees of freedom, \\(K\\) being the number of estimated parameters. It is suggested that use \\(h = 10\\) for non-seasonal data and \\(h = 2m\\) for seasonal data, where \\(m\\) is the period of seasonality. As – strictly applied – the distribution of this test statistic holds only asymptotically (under large sample), GRETA M. LJUNG and GEORGE E.P. BOX (1978) proposed the following modification for small samples, known as the ljung-box test \\[ Q^* = T(T + 2)\\sum_{k = 1}^{h}{(T-K)^{-1}}r_k^2 \\] \\(Q^*\\) is also asymptotically \\(\\chi^2\\) distributed with \\(m - k\\) degrees of freedom, An alternative to these testing procedures is the Breusch-Godfrey test (BG test), also known asa the Lagrange-Multiplier Test (LM Test) developed by TREVOR S. BREUSCH (1978) and LESLIE G. GODFREY (1978). Like for the \\(Q\\) (\\(Q^*\\)) test the null hypothesis is which is tested against the alternative that the residuals follow an autoregressive or a moving average process of order h. The test can be performed with an auxiliary regression. The estimated residuals are regressed on the explanatory variables of the main model and on the lagged residuals, up to order h. The test statistic which is \\(\\chi^2\\) distributed with h degrees of freedom is given by \\(TR^2\\) the auxiliary regression, with T being the number of observations. Alternatively, an \\(F\\) test can be used for testing the combined significance of the lagged residuals in the auxiliary regression. The Breusch-Godfrey test is similar to the Ljung-Box test, but it is specifically designed for use with regression models. Compared to the Durbin-Watson test which is used in traditional econometrics for testing autocorrelation of the residuals of an estimated model, the \\(Q\\), \\(Q^*\\) as well as the BG test have two major advantages: firstly, they can check for autocorrelation of any order, and not only one order at a time. Secondly, the results are also correct if there are lagged endogenous variables in the regression equation, whereas in such cases the results of the Durbin-Watson test are biased in favour of the null hypothesis. As mentioned in Section 5.4.1, we often assume normally distributed white noise. But the fact that the residuals are not autocorrelated does not imply that they are stochastic independence if the variauted, as the usual testing procedures are based on this aspecially the third (skewness) and fourth moments (kurtosis) are important (0 and 3 respectively based on normality hypothesis). \\[ \\begin{aligned} \\text{skewness}: \\hat{S} &amp;= \\frac{1}{T}\\frac{\\sum_{t = 1}^{T}{(x_i - \\mu)^3}}{\\hat{\\gamma}(0)^{3/2}} \\\\ \\text{kurtosis}: \\hat{K} &amp;= \\frac{1}{T}\\frac{\\sum_{t = 1}^{T}{(x_i - \\mu)^4}}{\\hat{\\gamma}(0)^{2}} \\end{aligned} \\] Using the skewness S and the kurtosis K, CARLOS M. JARQUE and A NIL K. BERA (1980) proposed a test for normality. It can be applied directly on the time series itself (or on its differences). Usually, however, it is applied to check estimated regression residuals. The test statistic \\[ \\text{JB} = \\frac{T}{6}[\\hat{S}^2 + \\frac{1}{4}(\\hat{K} - 3)^2] \\] is \\(\\chi^2\\) distributed with 2 degrees of freedom. T is again the sample size. The hypothesis that the variable is normally distributed is rejected whenever the values of the test statistic are larger than the corresponding critical values. # use arima.sim() to stimulate a AR(1) process # list specification for AR(1) model with phi = 0.5, and the std dev of the Gaussian errors to be 0.5 AR_spec &lt;- list(order = c(1, 0, 0), ar = 0.9, sd = 0.1) AR1 &lt;- arima.sim(n = 50, model = AR_spec) AR1_aug &lt;- AR1 %&gt;% as_tsibble() %&gt;% model(ARIMA(value ~ PDQ(1, 0, 0))) %&gt;% augment() # box pierce test, non-seasonal data h = 10 AR1_aug %&gt;% features(.resid, box_pierce, lag = 10, dof = 0) #&gt; # A tibble: 1 x 3 #&gt; .model bp_stat bp_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(value ~ PDQ(1, 0, 0)) 4.08 0.944 # ljung box test AR1_aug %&gt;% features(.resid, ljung_box, lag = 10, dof = 0) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(value ~ PDQ(1, 0, 0)) 5.01 0.890 # Breusch Godfrey test: lmtest::bgtest() # durbin watson test AR1_aug %&gt;% pull(.resid) %&gt;% car::durbinWatsonTest(max.lag = 10) #&gt; [1] 1.892617 1.987601 1.948205 1.773167 2.037944 1.857135 1.832362 1.445376 #&gt; [9] 1.114101 1.588983 # jb test (normality) AR1_aug %&gt;% pull(.resid) %&gt;% normtest::jb.norm.test() #&gt; #&gt; Jarque-Bera test for normality #&gt; #&gt; data: . #&gt; JB = 6.5604, p-value = 0.028 9.1.3 The Wold Decomposition The Wold Decomposition is a general property that all stationary process share. Actually it exists for every covariance stationary, purely non-deterministic stochastic process: After subtracting the mean function, each of such processes can be represented by a linear combination of a series of uncorrelated random variables with zero mean and constant variance, which are the errors made in forecasting \\(y\\) t on the basis of a linear function of lagged \\(y\\)(i.e., the RHS of a MA(\\(\\infty\\)) model after subtracting the constant term). Purely non-deterministic means that all additive deterministic components of a time series have to be subtracted in advance. By using its own lagged values, any deterministic component can be perfectly predicted in advance. This holds, for example, for a constant mean, as well as for periodic, polynomial, or exponential series in t. Thus, one can write (see proof in Seciton 9.4.3): \\[\\begin{equation} \\tag{9.1} y_t - \\mu_t = \\sum_{i-0}^{\\infty}\\psi_i\\varepsilon_i \\end{equation}\\] where \\(\\varepsilon_t\\) is white noise and \\(\\psi_i\\) satisfy \\(\\sum_{i-0}^{\\infty}\\psi_i^2 &lt; \\infty\\). The quadratic convergence of the series of the \\(\\psi_i\\) guarantees the existence of second moments of the process \\(y_t\\). There is no need of any distributional assumption for this decomposition to hold. Especially, there is no need of the \\(\\varepsilon_t\\) to be independent, it is sufficient that they are (linearly) uncorrelated. From Wold Decomposition we can derive \\(y_t\\) has properties as follows: \\[ \\begin{aligned} \\text{E}(y_t) &amp;= \\mu_t \\\\ \\text{Var}(y_t) = \\text{E}[(y_t - \\mu_t)^2] &amp;= E[(\\varepsilon_t + \\psi_1\\varepsilon_{t-1} + ...)^2] = \\sigma^2\\sum_{i=0}^{\\infty}\\psi_i^2 \\\\ \\end{aligned} \\] And as for variance: \\[ \\begin{split} \\text{Cov}(y_t, y_{t+k}) &amp;= \\text{E}[(y_t - \\mu_t)(y_{t+k} - \\mu_{t+k})] \\\\ &amp;= \\text{E}[(\\varepsilon_t + \\psi_1\\varepsilon_{t-1} + \\cdots + \\psi_k\\varepsilon_{t-k} + \\psi_{k+1}\\varepsilon_{t-k-1}) * (\\varepsilon_{t+k} + \\psi_1\\varepsilon_{t+k-1} + \\cdots + \\psi_{k}\\varepsilon_t) + \\psi_{k+1}\\varepsilon_{t-1})] \\\\ \\end{split} \\] We know that for \\(i \\not= j, \\text{E}(\\varepsilon_i\\varepsilon_j) = \\text{Cov}(\\varepsilon_i, \\varepsilon_j) = 0\\) and \\(\\text{E}(\\varepsilon_i^2) = \\text{Var}(\\varepsilon_i^2) = \\sigma^2\\), so the result can be simplfied as : \\[ \\begin{split} \\text{Cov}(y_t, y_{t+k}) &amp;= \\sigma^2(1 ·\\psi_k + \\psi_1\\psi_{k+1} + \\psi_{2}\\psi_{k+2} + \\cdots) \\\\ &amp;= \\sigma^2\\sum_{i=0}^{\\infty}{\\psi_i\\psi_{k+i}} \\end{split} \\] This derivation matches where we started, that \\(y_t\\) is a covariance stationary stochastic process(and by extension variance stationary), since its variance and covariance function is not related to \\(t\\). All stationary models discussed in the following chapters can be represented on the basis of the Wold Decomposition (9.1). However, this representation is, above all, interesting for theoretical reasons: in practice, applications of models with an infinite number of parameters are hardly useful. 9.2 Backshift notation The backward shift operator \\(B\\) (or \\(L\\) in some references) is a useful notational device when working with time series lags: \\[ By_t = y_{t-1} \\] \\(B\\) can be treated as a number in arithmetic.Two applications of \\(B\\) to \\(y_t\\) shifts the data back two periods: \\[ B(By_t) = B^2y_t \\] The backward shift operator is convenient for describing the process of differencing. A first difference can be written as \\[ y&#39;_t = y_t - y_{t-1} = y_t - By_{y-1} = (1 - B)y_t \\] Similarly, the second-order difference would be \\[ \\begin{split} y_t&#39;&#39; &amp;= (y_t - y_{t-1}) - (y_{t-1} - y_{t - 2}) \\\\ &amp;= By_t - 2By_{t} + B^2y_t \\\\ &amp;= (1-B)^2y_t \\end{split} \\] In general, a \\(d\\)th-order difference can be written as \\[ (1 - B)^dy_t \\] Backshift notation is particularly useful when combining differences, as the operator can be treated using ordinary algebraic rules. In particular, terms involving \\(B\\) can be multiplied together. For example, a seasonal difference followed by a first difference can be written as \\[ \\begin{split} (1 - B)(1 - B^m) &amp;= (1 - B^m - B + B^{m + 1})y_t \\\\ &amp;= y_t - y_{t-m} - y_{t-1} + y_{t-m-1} \\end{split} \\] 9.3 Autoregressive models In an autoregression model, we forecast the variable of interest using a linear combination of past values of the variable. The term autoregression indicates that it is a regression of the variable against itself. Thus, an autoregressive model of order \\(p\\) can be written as \\[ y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\dots + \\phi_p y_{t-p} + \\varepsilon_t \\] where \\(\\varepsilon_t\\) is white noise. We refer to this model as AR(p) model, a autoregressive model with order \\(p\\) We can write out an AR(p) model using the backshift operator: \\[ \\begin{aligned} y_t - \\phi_1y_{t-1} - \\phi_2y_{t-2} - \\cdots - \\phi_py_{t-p} &amp;= \\varepsilon_t + c\\\\ (1 - \\phi_1B - \\phi_2B^2 - \\cdots - \\phi_pB^p)y_t &amp;= \\varepsilon_t + c\\\\ \\phi_p(B)y_t &amp;= \\varepsilon_t + c \\end{aligned} \\] The relationship between AR models and stationarity, Whether autoregressive process is stationary is dependent on the values of \\(\\phi_1, \\phi_2, \\dots\\). To yield a stationary autoregressive process, some constraints on the values of the parameters are required. For an AR(1) model: \\(-1 &lt; \\phi_1 &lt; -1\\) (Consider the denominator of the sum of a inifite geometric series) For an AR(2) model: \\(-1 &lt; \\phi_2 &lt; 1\\), \\(\\phi_1 + \\phi_2 &lt; 1, \\phi_2 - \\phi_1 &lt; 1\\). Generally, for a AR(p) model, if we treat B as number (or numbers), we can write out the the formula (\\(\\phi_p(B)\\) below) as \\[ \\begin{aligned} \\phi_p(B)y_t &amp;= \\varepsilon_t + c\\\\ &amp;\\Downarrow \\\\ \\phi_p(B) &amp;= 0 \\end{aligned} \\] To be stationary, all complex roots (\\(B_1\\), \\(B_2\\), …, \\(B_p\\)) of the characteristic equation \\(\\Phi(B) = 1 - \\phi_1B - \\phi_2B^2 - \\cdots - \\phi_pB^p\\) must exceed 1 in absolute value (actually all of them should lie ouside of the unit circle). We can thus derive conditions on the valuse of \\(\\phi_1, \\cdots, \\phi_p\\), but it’s quite complicated, and R takes care of these restrictions when estimating a model. For example, consider this AR(1) model : \\[ \\begin{aligned} y_t &amp;= 0.5y_{t-1} + \\varepsilon_t \\\\ y_t - 0.5y_{t-1} &amp;= \\varepsilon_t \\\\ (1 - 0.5B)y_t &amp;= \\varepsilon_t \\\\ &amp;\\Downarrow \\\\ 1 - 0.5B &amp;= 0 \\\\ B &amp;= 2 \\end{aligned} \\] This model is indeed stationary because \\(|B| &gt; 1\\). From this we can also derive \\(|\\phi_1| &lt; 1\\) for AR(1) model. 9.3.1 Stimulating an AR(p) process set.seed(2020) # specification for AR(1) model with small coef AR_small_spec &lt;- list(order = c(1, 0, 0), ar = 0.1, sd = 0.1) # specification for AR(1) model with large coef AR_large_spec &lt;- list(order = c(1, 0, 0), ar = 0.9, sd = 0.1) ## simulate AR(1) AR1_small &lt;- arima.sim(n = 50, model = AR_small_spec) %&gt;% as_tsibble() AR1_large &lt;- arima.sim(n = 50, model = AR_large_spec) %&gt;% as_tsibble() wrap_plots( autoplot(AR1_small) + ggtitle(expression(paste(phi, &quot; = 0.1&quot;))), autoplot(AR1_large) + ggtitle(expression(paste(phi, &quot; = 0.9&quot;))), ACF(AR1_small) %&gt;% autoplot() + ggtitle(expression(paste(phi, &quot; = 0.1&quot;))), ACF(AR1_large) %&gt;% autoplot() + ggtitle(expression(paste(phi, &quot; = 0.9&quot;))) ) It looks like the time series with the smaller AR coefficient is more “choppy” and seems to stay closer to 0 whereas the time series with the larger AR coefficient appears to wander around more. Remember that as the coefficient in an AR(1) model goes to 0, the model approaches a WN sequence, which is stationary in both the mean and variance. As the coefficient goes to 1, however, the model approaches a random walk (9.3.3), which is not stationary in either the mean or covariance. Next, let’s generate two AR(1) models that have the same magnitude coeficient, but opposite signs, and compare their behavior. set.seed(2020) # specification for AR(1) model with positive coef AR_pos_spec &lt;- list(order = c(1, 0, 0), ar = 0.5, sd = 0.1) # specification for AR(1) model with negative coef AR_neg_spec &lt;- list(order = c(1, 0, 0), ar = -0.5, sd = 0.1) # simulate AR(1) AR1_pos &lt;- arima.sim(n = 50, model = AR_pos_spec) %&gt;% as_tsibble() AR1_neg &lt;- arima.sim(n = 50, model = AR_neg_spec) %&gt;% as_tsibble() wrap_plots( autoplot(AR1_pos) + ggtitle(expression(paste(phi, &quot; = 0.5&quot;))), autoplot(AR1_neg) + ggtitle(expression(paste(phi, &quot; = -0.5&quot;))), ACF(AR1_pos) %&gt;% autoplot() + ggtitle(expression(paste(phi, &quot; = 0.5&quot;))), ACF(AR1_neg) %&gt;% autoplot() + ggtitle(expression(paste(phi, &quot; = -0.5&quot;))) ) Now it appears like both time series vary around the mean by about the same amount, but the model with the negative coefficient produces a much more “sawtooth” time series. It turns out that any AR(1) model with \\(−1 &lt; \\phi_1 &lt; 0\\) will exhibit the 2-point oscillation we see here. We can simulate higher order AR(p) models in the same manner, but care must be exercised when choosing a set of coefficients that result in a stationary model or else arima.sim() will fail and report an error. For example, an AR(2) model with both coefficients equal to 0.5 is not stationary, and therefore this function call will not work: arima.sim(n = 100, model = list(order(2, 0, 0), ar = c(0.5, 0.5))) #&gt; Error in arima.sim(n = 100, model = list(order(2, 0, 0), ar = c(0.5, 0.5))): &#39;ar&#39; part of model is not stationary 9.3.2 Decision of order p How do we decide the order $p $of a AR model? A rule of thumb is to look at partial autocorrelation coefficients, PACF. PACF measures the direct effect of a lagged value on its previous value. Suppose we want to measure the effect of \\(y_{t-2}\\) on \\(y_{t}\\), while \\(r_2\\) could be high, it could also carry the effect of \\(y_{t-2} \\rightarrow y_{t-1} \\rightarrow y_t\\), especially when \\(r_1\\) is also high. This is when partial autocorrelation come to resuce, consider a AR(2) process (which means past values earlier than \\(y_{t-2}\\) cannot have an effect on \\(y_t\\)) \\[ y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\varepsilon_t \\] The partial correlation coefficient for \\(y_{t-k}\\) is simply defined as \\(\\phi_k\\) in the model, i.e., \\(\\phi_1\\) for \\(y_{t-1}\\) and \\(\\phi_2\\) for \\(y_{t-2}\\). Partial correlation coefficient for \\(k &gt; 2\\) is think of as zero. In practice, there are more efficient algorithms for computing \\(\\phi_k\\) than fitting all of these autoregressions, but they give the same results. fpp3::aus_airpassengers %&gt;% PACF(lag_max = 10) %&gt;% autoplot() This tells us only PAC at \\(\\text{lag} = 1\\) is significantly different than 0. As such only among \\(y_{t-1}, y_{t-2}, \\dots, y_{t-10}\\), only \\(y_{t-1}\\) has a significant direct effect on the response, so a AR(1) model may be appropriate. We can compare this to the ACF plot fpp3::aus_airpassengers %&gt;% ACF(lag_max = 10) %&gt;% autoplot() Another characteristic of AR(p) is that ACF plots tails slowly. # model with larger phi has longer tail wrap_plots( ACF(AR1_small) %&gt;% autoplot() + ggtitle(expression(paste(phi, &quot; = 0.1&quot;))), ACF(AR1_large) %&gt;% autoplot() + ggtitle(expression(paste(phi, &quot; = 0.9&quot;))), nrow = 2 ) Further illustration: the ACF for an AR(p) process tails off toward zero very slowly, but the PACF goes to zero for lags &gt; \\(p\\). 9.3.3 Random walk Random walk is a special case of autoregressive process, namely AR(1). A time series is a random walk if \\[ y_t = \\begin{cases} \\varepsilon_1 &amp;t = 1 \\\\ y_{t - 1} + \\varepsilon_t&amp; t = 2, 3, \\dots \\end{cases} \\] where \\(\\varepsilon_t\\) comes from a white noise process. It can also be expressed in the following form: \\[ y_t = \\sum_{i=1}^{t}{\\varepsilon_t} \\] According to this, we also have (covariance can be viewed as autocorrelation in this sense) \\[ \\text{mean}: \\text{E}(y_t) = 0 \\\\ \\text{variance}: \\text{Var}(y_t) = t\\sigma^2 \\\\ \\text{ACF}: r_k(t) = \\frac{t\\sigma^2}{\\sqrt{t\\sigma^2(t+k)\\sigma^2}} \\] Note that a random walk process has changable variance and a ACF not only related to the distance, meaning that it is not statinary(since \\(\\phi_1 = 1 \\not&lt;|1|\\)). In other words, it does not satisfy variance stationarity and covariance stationarity. This non-stationary process is often suitable fro describing economic phenomena. Random walk can be extended by adding a drift \\(\\mu\\), which would be the new expectation of the process. Random walk with a drift (also called a biased random walk) is the process behind a drift model \\[ \\begin{split} y_t &amp;= \\mu + y_{t-1} + \\varepsilon_t \\\\ &amp;= \\mu + \\mu + y_{t-2} + \\varepsilon_t + \\varepsilon_{t-1}\\\\ \\vdots \\\\ &amp;= t\\mu + \\sum_{i=1}^{t}\\varepsilon_i \\end{split} \\] With little effort, we can show that for a drift process \\(\\text{E}(y_t) = \\mu\\), \\(\\text{Var}(y_t) = t\\sigma^2\\), and ACF stays the same, so that a biased random walk is still non-stationary. A random walk with drift can be considered as a curve fluctuating around line \\(y = \\mu t\\) with increasing volatility as \\(t\\) increases. # stimulate a random walk with drift RW &lt;- function(N, x0, mu, variance) { z&lt;-cumsum(rnorm(n = N, mean = 0, sd=sqrt(variance))) t &lt;- 1:N x &lt;- x0 + t*mu+z x } # mu is the drift set.seed(2020) rw &lt;- RW(500, 0, 1, 1) %&gt;% enframe() %&gt;% as_tsibble(index = name) rw %&gt;% autoplot() + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) rw %&gt;% ACF() %&gt;% autoplot() 9.4 Moving average models Rather than using past values of the forecast variable in a regression, a moving average model uses past forecast errors in a regression-like model.2 \\[ y_t = c + \\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2} + \\dots + \\theta_q\\varepsilon_{t-q} + \\varepsilon_{t} \\] where \\(\\varepsilon_t\\) is white noise. We refer to this as a MA(q) model, a moving average model of order \\(q\\). Of course, we do not observe the values of \\(\\varepsilon_t\\), so it is not really a regression in the usual sense. Notice that each value of \\(y_t\\) can be thought of as a weighted moving average of the past few forecast errors. However, moving average models should not be confused with the moving average smoothing we discussed in Section 3.2. A moving average model is used for forecasting future values, while moving average smoothing is used for estimating the trend-cycle of past values. It is easy to show MA(q) process is always stationary. A MA(1) process has the following properties \\[ \\begin{aligned} \\text{Mean stationarity}: \\text{E}(y_t) &amp;= c \\\\ \\text{Variance stationarity}:\\text{Var}(y_t) &amp;= (1 + \\theta_1^2)\\sigma^2 \\\\ \\text{Covariance stationarity}:\\text{ACF} = \\gamma(k) &amp;= \\begin{cases} \\frac{\\theta_1}{1 + \\theta_1^2} &amp; k = 1\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{aligned} \\] Proof for ACF : \\[ \\begin{split} \\frac{\\gamma(1)}{\\gamma(0)} &amp;= \\frac{\\text{Covariance for lag} 1}{\\text{variance for lag}1} \\\\ &amp;= \\frac{E[(y_t - E(y_t))(y_{t-1} - E(y_{t-1}))]}{(1 + \\theta_1^2)\\sigma^2} \\\\ &amp;= \\frac{E[(\\varepsilon_{t} + \\theta_1\\varepsilon_{t-1})(\\varepsilon_{t-1} + \\theta_1\\varepsilon_{t-2})]}{(1 + \\theta_1^2)\\sigma^2} \\\\ &amp;= \\frac{E(\\varepsilon_t \\varepsilon_{t-1} + \\theta_1\\varepsilon_t\\varepsilon_{t-2} + \\theta_1 \\varepsilon_{t-1}^2 + \\theta_1^2\\varepsilon_{t-1}\\varepsilon_{t-2})}{(1 + \\theta_1^2)\\sigma^2} \\\\ &amp;= \\frac{\\theta_1E(\\varepsilon_{t-1}^2)}{(1 + \\theta_1^2)\\sigma^2} \\\\ &amp;= \\frac{\\theta_1\\sigma^2}{(1 + \\theta_1^2)\\sigma^2} \\\\ &amp;= \\frac{\\theta_1}{1 + \\theta_1^2} \\end{split} \\] For \\(\\text{lag} &gt; 1\\), there will be no square term like \\(\\varepsilon_{t-k}^2\\), but only cross terms like \\(\\varepsilon_t\\varepsilon_{t-k}\\)，whose expectation would be zero by defination, so that ACF will be zero, which leads to covariance stationarity. 9.4.1 Simulating an MA(q) process We can simulate MA(q) processes just as we did for AR(p) processes using arima.sim(). Here are 3 different ones with contrasting \\(\\theta\\): set.seed(2020) ## list description for MA(1) model with small coef MA_sm_spec &lt;- list(order = c(0, 0, 1), ma = 0.2, sd = 0.1) ## list description for MA(1) model with large coef MA_lg_spec &lt;- list(order = c(0, 0, 1), ma = 0.8, sd = 0.1) ## list description for MA(1) model with large coef MA_neg_spec &lt;- list(order = c(0, 0, 1), ma = -0.5, sd = 0.1) ## simulate MA(1) MA1_sm &lt;- arima.sim(n = 50, model = MA_sm_spec) %&gt;% as_tsibble() MA1_lg &lt;- arima.sim(n = 50, model = MA_lg_spec) %&gt;% as_tsibble() MA1_neg &lt;- arima.sim(n = 50, model = MA_neg_spec) %&gt;% as_tsibble() wrap_plots( autoplot(MA1_sm) + ggtitle(expression(paste(theta, &quot; = 0.2&quot;))), autoplot(MA1_lg) + ggtitle(expression(paste(theta, &quot; = 0.8&quot;))), autoplot(MA1_neg) + ggtitle(expression(paste(theta, &quot; = -0.5&quot;))) ) 9.4.2 Decision of order q The derivation of the ACF function has shed light on how we would decide the order \\(q\\). For a MA(2) process, ACF would be 0 for \\(k = 3, 4, \\dots\\). In general, for a MA(q) process, ACF will be nonzero for \\(k = 1, 2, \\dots,q\\) and zero after \\(q\\). Let’s prove this, a MA(q) model can be written as: \\[ y_{t-k} = c + \\theta_1\\varepsilon_{t-k -1} + \\theta_2\\varepsilon_{t- k - 2} + \\dots + \\theta_q\\varepsilon_{t-k -q} + \\varepsilon_{t-q} \\] To see at which point ACF will be zero, consider the covariance (if covariance between \\(y_t\\) and \\(y_{t-k}\\) is zero, then \\(r_k\\) is zero) \\[ \\text{Cov}(y_t, y_{t-k}) = \\text{E}(y_ty_{t-k}) - \\text{E}(y_t)\\text{E}(y_{t-k}) \\] We know that \\(\\text{E}(y_t)\\text{E}(y_{t-k}) = c^2\\). So the problem is, what is included in \\(E(y_ty_{t-k})\\), and when will it be \\(c^2\\), so that the covariance would be zero? Follow MA(q), \\(y_t\\) includes \\((c, \\varepsilon_t, \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots, \\varepsilon_{t-q})\\), and \\(y_{t-k}\\) includes \\((c, \\varepsilon_{t-k}, \\varepsilon_{t-k -1}, \\varepsilon_{t-k -2}, \\dots, \\varepsilon_{t - k -q})\\). When they are multiplied together, there will be \\[ \\begin{aligned} \\text{constant} &amp;: c^2 \\rightarrow E(c^2) = c^2 \\\\ \\text{cross terms} &amp;: \\varepsilon_i\\varepsilon_j, i \\not= j \\rightarrow E(\\varepsilon_i\\varepsilon_j) = 0 \\\\ \\text{square terms} &amp;: \\varepsilon_i^2 \\rightarrow E(\\varepsilon_i^2) = \\text{Var}(\\varepsilon_i) + \\text{E}(\\varepsilon_i)^2 = \\sigma^2 &gt; 0 \\end{aligned} \\] Now everything is clear, ACF will be zero, if and only if there is no square term. That is to say, there is no same error term when we decompose \\(y_t\\) and \\(y_{t-k}\\). This means the earliest error in \\((\\varepsilon_t, \\varepsilon_{t-1}, \\varepsilon_{t-2}, \\dots, \\varepsilon_{t-q})\\) is still earlier than the latest error term in of \\((\\varepsilon_{t-k}, \\varepsilon_{t-k -1}, \\varepsilon_{t-k -2}, \\dots, \\varepsilon_{t - k -q})\\) \\[ \\begin{aligned} t - q &amp;&gt; t - k \\\\ k &amp;&gt; q \\end{aligned} \\] Now that we have proved that when \\(k &gt; q\\), ACF would be zero and otherwise non-zero. So, a sample ACF with significant autocorrelations at lag 1 to lag q, but non-significant autocorrelations for after q indicates a possible MA(q) model(i.e., an ACF plot that cuts off at lag q). Another characteristic of a MA(q) process is that their PACF tails off toward zero very slowly, in contrast to AR(p) whose ACF plot has a long tail. 9.4.3 Koyck transformation and Invertibility Koyck transformation is one that converts a AR(p) model to MA(\\(\\infty\\)) model . Take an AR(1) model for example, which can be written as \\[ \\begin{aligned} (1 - \\phi_1B)y_t &amp;= \\varepsilon_t + c \\\\ y_t &amp;= \\frac{\\varepsilon_t + c}{1 - \\phi_1B} \\end{aligned} \\] Recall that the infinite sum of a geometric series is \\[ \\begin{aligned} s &amp;= a + ar + ar^2 + \\cdots \\\\ &amp;= \\frac{a}{1 - r} \\quad (|r| &lt; 1) \\end{aligned} \\] It follows that we can rewritten our formula as (backshift operator on a constant is still that constant, so \\(|\\phi_1| &lt; 1\\) ): \\[ \\begin{aligned} y_t &amp;= (\\varepsilon_t + c) + (\\varepsilon_t + c)\\phi_1B + (\\varepsilon_t + c)(\\phi_1B)^2 + \\cdots \\\\ &amp;= \\varepsilon_t + (c + \\phi_1c + \\phi_1^2c + \\dots) + (\\phi_1\\varepsilon_{t-1} + \\phi_1^2\\varepsilon_{t-2} + \\cdots) \\end{aligned} \\] For a AR(1) model, we have \\(|\\phi_1| &lt; 1\\). so that \\((c + \\phi_1c + \\phi_1^2c + c\\dots)\\) will converge to a constant. And what we derived is exactly a MA(\\(\\infty\\)) model. This transformation is also a special case of the Wold Decomposition. Since when AR(1) when \\(|\\phi_1| &lt; 1\\) is covariance stationary, and \\(\\text{E}(y_t) = c + \\phi_1c + \\phi_1^2c + \\dots\\). If we substract the mean from the RHS then the equation is Equation (9.1) Invertibility is like the opposite of Koyck transformation. It describes the fact that it is possible to write any stationary MA(\\(\\infty\\)) model as a AR(p) model. Again, we demonstrate this with an MA(1) model (here I put a negative sign on \\(\\theta_1\\) just for convenience): \\[ \\begin{aligned} y_t &amp;= c - \\theta_1\\varepsilon_{t-1} + \\varepsilon_t \\\\ y_t &amp;= (1 - \\theta_1B)\\varepsilon_t + c\\\\ \\varepsilon_t &amp;= \\frac{y_t - c}{1 - \\theta_1B} \\end{aligned} \\] The rest is the same, \\(\\frac{y_t - c}{1 - \\theta_1B}\\) can be considered as the infinite sum of a geometric series, when \\(|\\theta_1| &lt; 1\\). And the final equation is \\[ y_t = (c + \\theta_1c + \\theta_1^2c + \\cdots) + (\\theta_1y_{t-1} + \\theta_1^2y_{t-2} + \\cdots) + \\varepsilon_t \\] With \\(|\\theta_1| &lt; 1\\), \\((c + \\theta_1c + \\theta_1^2c + \\cdots)\\) will converge to a constant and the equation qualifies for a AR(\\(\\infty\\)) model. Thus, when \\(|\\theta_1| &lt; 1\\) the MA model is invertible. (From another perspective, if \\(|\\theta_1| &gt; 1\\) \\(\\phi_1, ... ,\\phi_p\\) in MA(\\(\\infty\\)) model will not met stationary conditions). Although we no longer need to place constraints on MA models like we did on AR models since it is always stationary, we do hope that MA models are invertible, because only when an MA(q) process is invertible can it be uniquely identified. Consider, for example, the following case in which we know that for a MA(1) model \\(\\gamma(1) = r_1\\), and the way we would compute \\(\\theta_1\\): \\[ \\frac{\\theta}{1 + \\theta^2} = r_1 \\\\ (1 + \\theta^2)r_1 - \\theta = 0 \\\\ \\theta^2 + \\frac{1}{r_1}\\theta + 1 = 0 \\] The equation has the following two roots: \\[ \\theta_{1,2} = -\\frac{1}{2r}(1 \\pm \\sqrt{1 - 4r_1^2}) \\] Because \\(|r_1| = |\\frac{\\theta_1}{1 + \\theta_1^2}| &lt; 1/2\\) the quadratic equation always results in real roots. They also have the property that \\(\\theta_1\\theta_2 = 1\\) (Vieta theorem). This gives us the possibility to model the same autocorrelation structure with two different parameters, where one is the inverse of the other. n order to get a unique parameterisation, we require a further property of the MA(1) process. We ask under which conditions the MA(1) process can have an autoregressive representation. And we already know that this is true when \\(|\\theta| &lt; 1\\), and this condition also helps us to choose between \\(\\theta_1\\) and \\(\\theta_2\\), so that unique model is identified. For higher order models, given \\(\\gamma(1), \\gamma(2), \\dots, \\gamma(q)\\) (we already proved that when k &gt; q \\(\\gamma(k) = 0\\)), we would still have multiple solution set \\(\\theta_1, \\theta_2, \\dots, \\theta_q\\). To get a unique parameterisation, the invertibility condition is again required, i.e. it must be possible to represent the MA(q) process as a stationary AR(p) process. The condition for order q, similar to AR(p), is that all roots of \\[ 1 + \\theta_1B + \\theta_2B^2 + \\dots + \\theta_qB^q = 0 \\] are larger than one in absolute value. 9.5 ARMA models In the following, we introduce processes which contain both an autoregressive (AR) term of finite order p and a moving average (MA) term of finite order q. Hence, these mixed processes are denoted as ARMA(p, q) processes. They enable us to describe processes in which neither the autocorrelation nor the partial autocorrelation function breaks off after a finite number of lags. Again, we start with the simplest case, the ARMA(1, 1) process. An ARMA(1, 1) process can be written as follows (Note the negative sign before \\(\\theta_1\\)) \\[ y_t = c + \\phi_1y_{t-1} - \\theta_1\\varepsilon_{t-1} + \\varepsilon_t \\] using the backshift notation \\[ (1 - \\phi_1B)y_t = c + (1 - \\theta_1B)\\varepsilon_t \\] where \\(\\varepsilon_t\\) comes from pure random process. To get the Wold representation of an ARMA(1, 1) process, we solve the equation below for \\(y_t\\) \\[\\begin{equation} \\tag{9.2} y_t = \\frac{c}{1 - \\phi_1B} + \\frac{1 - \\theta_1B}{1 - \\phi_1B}\\varepsilon_t \\end{equation}\\] It is obvious that \\(\\phi_1 \\not= \\theta_1\\) must hold, because otherwise \\(y_t\\) would be a pure random process fluctuating around the mean \\(\\mu = c/(1 – \\phi_1B)\\). Recall the Wold Decomposition in Equation (9.1). The \\(\\phi_i,i = 0, 1, 2, ...\\), can be determined as follows: \\[ \\begin{aligned} \\frac{1 - \\theta_1B}{1 - \\phi_1B} &amp;= \\psi_0 + \\psi_1B + \\psi_2B^2 + \\psi_3B^2 + \\cdots \\\\ 1 - \\theta_1B &amp;= (1 - \\phi_1B)( \\psi_0 + \\psi_1B + \\psi_2B^2 + \\psi_3B^2 + \\cdots) \\\\ 1 - \\theta_1B &amp;= \\psi_0 + \\psi_1B + \\psi_2B^2 + \\psi_3B^2 +\\cdots - \\phi_1\\psi_0B - \\phi_1\\psi_1B^2 - \\phi_1\\psi_2B^3 - \\cdots \\end{aligned} \\] Comparing the coefficients of the two lag polynomials we get \\[ \\begin{aligned} B^0&amp;: \\psi_0 = 1 \\\\ B^1&amp;: \\psi_1 - \\phi_1\\psi_0 = \\theta_1 \\Rightarrow \\psi_1 = \\phi_1 - \\theta_1 \\\\ B^2&amp;: \\psi_2 - \\phi_1\\psi_1 = 0 \\Rightarrow \\psi_2 = \\phi_1(\\phi_1 - \\theta_1) \\\\ B^3&amp;: \\psi_3 - \\phi_1\\psi_2= 0 \\Rightarrow \\psi_3 = \\phi_1^2(\\phi_1 - \\theta_1) \\\\ &amp; \\vdots \\\\ B^i&amp;: \\psi_i - \\phi_1\\psi_{i-1} = 0 \\Rightarrow \\psi_i = \\phi_1^{i-1}(\\phi_1 - \\theta_1)^2 \\end{aligned} \\] Note that the Wold Decomposition also requires \\(\\sum_{i-0}^{\\infty}\\psi_i^2 &lt; \\infty\\) (this is in the mean function), and this can be only satisfied when \\(|\\phi_1| &lt; 1\\). This corresponds to the stability condition of the AR term. Thus, the ARMA(1, 1) process is stationary if, with stochastic initial conditions, it has a stable AR(1) term. The Wold representation is \\[ y_t = \\frac{c}{1 - \\phi_1} + \\varepsilon_t + (\\phi_1 - \\theta_1)\\varepsilon_{t-1} + \\phi_1(\\phi_1 - \\theta_1)\\varepsilon_{t-2} + \\phi_1^2(\\phi_1 - \\theta_1)\\varepsilon_{t-3} \\] Thus, the ARMA(1, 1) process can be written as an MA(\\(\\infty\\)) process. To invert the MA(1) part, \\(|\\theta_1| &lt; 1\\) must hold. From Equation (9.2) we know: \\[ \\varepsilon_t = \\frac{-c}{1 - \\theta_1} + \\frac{1 - \\phi_1B}{1 - \\theta_1B}y_t \\] If \\(1/(1 – \\theta_1B)\\) is developed into a geometric series we get \\[ \\begin{aligned} \\varepsilon_t &amp;= \\frac{-c}{1 + \\theta_1} + (1 - \\phi_1B)(1 + \\theta_1B + \\theta_1^2B^2 + \\cdots)y_t \\\\ &amp;= \\frac{-c}{1 + \\theta_1} + y_t + (\\theta_1 - \\phi_1)y_{t-1} + \\theta_1(\\theta_1 - \\phi_1)y_{t-2} + \\theta_1^2(\\theta_1 - \\phi_1)y_{t-3} + \\cdots \\end{aligned} \\] This proves to be an AR(\\(\\infty\\)) representation. It shows that the combination of an AR(1) and an MA(1) term leads to a process with both MA(\\(\\infty\\)) and AR(\\(\\infty\\)) representation if the AR term is stable and the MA term invertible. More generally, ARMA(p, q) is staionary and invertible if the AR term is stationary and the MA term invertible (all complex roots of \\(\\Phi(B)\\) and that of \\(\\Theta(B)\\) exceeds 1 in absolute value, Section @ref(plotting-the-characteristic-roots introduces gg_arma() to visualized characteristic roots)). It can either be represented as an AR(\\(\\infty\\)) or as an MA(\\(\\infty\\)) process. Thus, neither its autocorrelation nor its partial autocorrelation function breaks off. In short, it is possible to generate stationary stochastic processes with infinite AR and MA orders by using only a finite number of parameters. It can be proved that an stationary ARMA(1, 1) has the following properties: \\[ \\begin{aligned} \\text{E}(y_t) &amp;= \\frac{c}{1 - \\phi_1} \\\\ \\text{Var}(y_t) = \\gamma(0) &amp;= \\frac{1 + \\theta_1^2 - 2\\phi_1\\theta_1}{1 - \\alpha^2}\\sigma^2 \\\\ \\text{Cov}(y_t, y_{t+k}) = \\gamma(k) &amp;= \\phi_1\\gamma(k-1) \\end{aligned} \\] We also know that \\(r_1\\), (the initial value of \\(r_k\\)) \\[ r_1 = \\frac{(\\phi_1 - \\theta_1)(1 - \\phi_1\\theta_1)}{1 + \\theta_1^2 - 2\\phi_1\\theta_1} \\] If the process is stable and invertible, i.e. for \\(|\\phi_1| &lt; 1\\) and \\(|\\theta_1| &lt; 1\\), the sign of \\(r_k\\) is determined by the sign of \\((\\phi_1 - \\theta_1)\\) because of\\((1 + \\theta_1^2 -2\\phi_1 \\theta_1) = (\\theta_1 - \\phi_1)^2 + 1 - \\phi_1^2 &gt; 0\\) and \\((1 – \\phi_1\\theta_1) &gt; 0\\). Moreover, it follows from \\(\\gamma(k)\\) that the autocorrelation function – as in the AR(1) process – is monotonic for \\(\\phi_1 &gt; 0\\) and oscillating for \\(\\phi_1 &lt; 0\\). Due to \\(|\\phi_1| &lt; 1\\) with \\(k\\) increasing, the autocorrelation function also decreases in absolute value. Thus, the following typical autocorrelation structures are possible: \\(\\phi_1 &gt; 0\\) and \\(\\phi_1 &gt; \\theta_1\\): The autocorrelation function is always positive. \\(\\phi_1 &lt; 0\\) and \\(\\phi_1 &lt; \\theta_1\\): The autocorrelation function oscillates; the initial condition \\(r_1\\) is negative. \\(\\phi_1 &gt; 0\\) and \\(\\phi_1 &lt; \\theta_1\\): The autocorrelation function is negative from \\(r_1\\) onwards. \\(\\phi_1 &lt; 0\\) and \\(\\phi_1 &gt; \\theta_1\\): The autocorrelation function oscillates; the initial condition \\(r_1\\) is positive. 9.5.1 Three representations of an ARMA model This is a supplementary section elaborating on the details among the relationship between ARMA, AR, and MA models. For a stationary ARMA(p, q) model: \\[ (1 - \\phi_1 B - \\cdots - \\phi_p B^p)y_t = c + (1 + \\theta_1 B + \\cdots +\\theta_q B^q)\\varepsilon_t \\] For the AR and MA representations, we use long division of two polynomials. Given two polynomials \\(\\Phi(B) = 1 - \\sum_{i=1}^{p}\\phi_iB^i\\) and \\(\\Theta(B) = 1 - \\sum_{i=1}^{q}\\theta_iB^i\\) we can obtain, by long division, that the following results take form of \\[ \\frac{\\Theta(B)}{\\Phi(B)} = 1 + \\psi_1 B + \\psi_2 B^2 + \\cdots \\equiv \\Psi(B) \\tag{1} \\] and \\[ \\frac{\\Phi(B)}{\\Theta(B)}= 1 - \\pi_1 B - \\pi_2 B ^2 - \\cdots \\equiv \\Pi(B) \\tag{2} \\] 9.5.1.1 AR representation From Eq (2), the ARMA(p, q) model can be written as \\[ \\frac{\\Phi(B)}{\\Theta(B)}y_t = c + \\varepsilon_t \\] So \\[ y_t = c + \\pi_1y_{t-1} + \\pi_2y_{t-2} + \\pi_3y_{t-3} + \\cdots + \\varepsilon_t \\] 9.5.1.2 MA representation From Eq(1), the ARMA(p, q) model can be written as \\[ y_t = \\frac{c}{\\Phi(B)} + \\frac{\\Theta(B)}{\\Phi(B)}\\varepsilon_t \\] So \\[ y_t = \\frac{c}{\\Phi(B)} + \\psi_1\\varepsilon_{t-1} + \\psi_2\\varepsilon_{t-2} + \\cdots + \\varepsilon_{t} \\] Many textbooks and software programs define the model with negative signs before the \\(\\theta\\) terms (R uses positive signs). This doesn’t change the general theoretical properties of the model, although it does flip the algebraic signs of estimated coefficient values and (unsquared) \\(\\theta\\) terms in formulas for ACFs and variances↩︎ "],
["arima-models.html", "Chapter 10 ARIMA models 10.1 Differencing 10.2 Non-seasonal ARIMA models 10.3 Estimation and order selection 10.4 The ARIMA() function 10.5 Forecasting with ARIMA models 10.6 Seasonal ARIMA models 10.7 ETS and ARIMA", " Chapter 10 ARIMA models library(tsibble) library(tsibbledata) library(fable) library(feasts) library(lubridate) library(pins) library(slider) 10.1 Differencing All processes (AR, MA, ARMA) disscussed in Chapter 9 belong to a larger family called linear stationary processes. This means our models would normally be confined to a stataionry data-generating system. By differencing, we compute the differences between consecutive observations, and this has been shown as a easy but effective way to make a non-stationary time series stationary. While transformations such as logarithms can help to stabilise the variance of a time series, differencing can help stabilise the mean of a time series by removing changes in the level of a time series, and therefore eliminating (or reducing) trend and seasonality. Suppose there is a undelying linear trend behind the observed time series \\[ y_t = \\beta_0 + \\beta_1t + \\varepsilon_t \\] where \\(\\varepsilon_t\\) is white noise. The first difference is defined as $$ \\begin{split} y’t &amp;= y_t - y{t-1} \\ &amp;= (_0 + _1t + _t) - [_0 + 1(t-1) + {t-1}] \\ &amp;= _1 + (t - {t-1}) \\end{split} $$ Note that \\(y&#39;_t\\) satisfies all stationary conditions, \\(\\text{E}(y&#39;_t) =0\\), \\(\\text{Var}(y&#39;_t) = 2\\sigma^2\\), \\(\\text{Cov}(y&#39;_t, y&#39;_{t+k}) = 0\\). So it can be modelled as previously discussed. Another example is a (biased) random walk process 9.3.3, where the first difference is \\[ y&#39;_t = c + \\varepsilon_t \\] 10.1.1 Second-order differencing Occasionally the differenced data will not appear to be stationary and it may be necessary to difference the data a second time to obtain a stationary series: \\[ \\begin{aligned} y&#39;&#39;_t &amp;= (y_t - y_{t-1}) - (y_{t-1} - y_{t-2}) \\\\ &amp;= y_t - 2y_{t-1} + y_{t-2} \\end{aligned} \\] In practice, it is almost never necessary to go beyond second-order differences. 10.1.2 Seasonal differencing A seasonal difference is the difference between an observation and the previous observation from the same season. So \\[ y&#39;_t = y_t - y_{t-m} \\] If seasonally differenced data appear to be white noise, then an appropriate model for the original data is \\[ y_t = y_{t-m} + \\varepsilon_t \\] Forecasts from this model are equal to the last observation from the relevant season. That is, this model gives seasonal naïve forecasts, introduced in Section 5.2. And second seasonal difference is \\[ \\begin{split} y&#39;&#39;_t &amp;= y&#39;t - y&#39;_{t-1} \\\\ &amp;= (y_t - y_{t-m}) - (y_{t-1} - y_{t-1-m}) \\\\ &amp;= y_t - y_{t-1} - y_{t-m} + y_{t-1-m} \\end{split} \\] When both seasonal and first differences are applied, it makes no difference which is done first—the result will be the same. However, if the data have a strong seasonal pattern, it is recommended that seasonal differencing be done first, because the resulting series will sometimes be stationary and there will be no need for a further first difference. If first differencing is done first, there will still be seasonality present. Sometimes it is necessary to take both a seasonal difference and a first difference to obtain stationary data, as is shown below. Here, the data are first transformed using logarithms (second panel), then seasonal differences are calculated (third panel). The data still seem somewhat non-stationary, and so a further lot of first differences are computed (bottom panel). PBS %&gt;% filter(ATC2 == &quot;H02&quot;) %&gt;% summarize(Cost = sum(Cost) / 1e6) %&gt;% transmute( sales = Cost, sales_log = log(Cost), seasonal_difference = log(Cost) %&gt;% difference(lag = 12), double_difference = log(Cost) %&gt;% difference(lag = 12) %&gt;% difference(lag = 1) ) %&gt;% pivot_longer(-Month, names_to = &quot;measure&quot;) %&gt;% mutate(measure = fct_relevel(measure, c(&quot;sales&quot;, &quot;sales_log&quot;, &quot;seasonal_difference&quot;, &quot;double_difference&quot;))) %&gt;% ggplot() + geom_line(aes(Month, value)) + facet_wrap(~ measure, ncol = 1, scales = &quot;free_y&quot;) + labs(title = &quot;Corticosteroid drug sales&quot;, x = &quot;Year&quot;, y = NULL) 10.1.3 Unit root tests One way to determine more objectively whether differencing is required is to use a unit root test. These are statistical hypothesis tests of stationarity that are designed for determining whether differencing is required. A number of unit root tests are available, which are based on different assumptions and may lead to conflicting answers. Here we use the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test implemented by urca::ur.kpss(). In this test, the null hypothesis is that the data are stationary, and we look for evidence that the null hypothesis is false. Consequently, small p-values suggest that differencing is required. google_2015 &lt;- gafa_stock %&gt;% filter(Symbol == &quot;GOOG&quot;) %&gt;% mutate(day = row_number()) %&gt;% update_tsibble(index = day, regular = TRUE) %&gt;% filter(year(Date) == 2015) google_2015 %&gt;% features(Close, unitroot_kpss) #&gt; # A tibble: 1 x 3 #&gt; Symbol kpss_stat kpss_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 GOOG 3.56 0.01 The test statistic is much bigger than the 1% critical value, indicating that the null hypothesis is rejected. That is, the data are not stationary. We can difference the data, and apply the test again. google_2015 %&gt;% mutate(Close = difference(Close, 1)) %&gt;% features(Close, unitroot_kpss) #&gt; # A tibble: 1 x 3 #&gt; Symbol kpss_stat kpss_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 GOOG 0.0989 0.1 This time, the test statistic is tiny, and well within the range we would expect for stationary data. So we can conclude that the differenced data are stationary. This process of using a sequence of KPSS tests to determine the appropriate number of first differences is carried out using the unitroot_ndiffs() feature. # 1st difference is needed google_2015 %&gt;% features(Close, unitroot_ndiffs) #&gt; # A tibble: 1 x 2 #&gt; Symbol ndiffs #&gt; &lt;chr&gt; &lt;int&gt; #&gt; 1 GOOG 1 A similar feature for determining whether seasonal differencing is required is unitroot_nsdiffs(), which uses the measure of seasonal strength introduced in Section 4.3. Recall that the strength of seasonality \\(F_s\\) is defined as \\[ F_S = \\max(0\\,, 1- \\frac{\\text{Var}(R_t)}{\\text{Var}(S_t + R_t)}) \\] where \\(R_t\\) is the remainder component and \\(S_t\\) the seasonal component. No seasonal differences are suggested if \\(F_S &lt; 0.64\\), otherwise one seasonal difference is suggested. We can apply unitroot_nsdiffs() to the monthly total Australian retail turnover. aus_total_retail &lt;- aus_retail %&gt;% summarize(Turnover = sum(Turnover)) aus_total_retail %&gt;% mutate(log_turnover = log(Turnover)) %&gt;% features(log_turnover, unitroot_nsdiffs) #&gt; # A tibble: 1 x 1 #&gt; nsdiffs #&gt; &lt;int&gt; #&gt; 1 1 aus_total_retail %&gt;% mutate(log_turnover = log(Turnover) %&gt;% difference(12)) %&gt;% features(log_turnover, unitroot_ndiffs) #&gt; # A tibble: 1 x 1 #&gt; ndiffs #&gt; &lt;int&gt; #&gt; 1 1 Because unitroot_nsdiffs() returns 1 (indicating one seasonal difference is required), we apply the unitroot_ndiffs() function to the seasonally differenced data. These functions suggest we should do both a seasonal difference and a first difference. aus_total_retail %&gt;% mutate(log_turnover = log(Turnover) %&gt;% difference(12) %&gt;% difference(1)) %&gt;% autoplot(log_turnover) 10.2 Non-seasonal ARIMA models If we combine differencing with autoregression and a moving average model, we obtain a non-seasonal ARIMA model. ARIMA is an acronym for AutoRegressive Integrated Moving Average (in this context, “integration” is the reverse of differencing). The full model can be written as \\[\\begin{equation} \\tag{10.1} y&#39;_t =c + \\phi_1y_{t-1} + \\cdots + \\phi_py_{t-p} + \\theta_1\\varepsilon_{t-1} + \\cdots \\theta_q\\varepsilon_{t-q} + \\varepsilon_t \\end{equation}\\] We call this an ARIMA(p, d, q) model, where \\[ \\begin{aligned} p &amp;= \\text{order of the autoregressive part} \\\\ d &amp;= \\text{degree of first differencing involved} \\\\ q &amp;= \\text{order of the moving average part} \\end{aligned} \\] The same stationarity and invertibility conditions that are used for autoregressive and moving average models also apply to an ARIMA model. Many of the models we have already discussed are special cases of the ARIMA model Once we start combining components in this way to form more complicated models, it is much easier to work with the backshift notation. For example, Equation (10.1) can be written in backshift notation as \\[ (1 - \\phi_1B - \\cdots - \\phi_pB^p)(1 - B)^dy_t = c + (1 + \\theta_1B + \\cdots + \\theta_1B^q)\\varepsilon_t \\] 10.2.1 Understanding ARIMA models The constant \\(c\\) has an important effect on the long-term forecasts obtained from these models. If \\(c = 0\\) and \\(d = 0\\) , the long-term forecasts will go to zero. If \\(c = 0\\) and \\(d = 1\\), the long-term forecasts will go to a non-zero constant. If \\(c = 0\\) and \\(d = 2\\), the long-term forecasts will follow a straight line. If \\(c \\not= 0\\) and \\(d = 0\\), the long-term forecasts will go to the mean of the data. If \\(c \\not= 0\\) and \\(d = 1\\) , the long-term forecasts will follow a straight line. If \\(c \\not= 0\\) and \\(d = 2\\), the long-term forecasts will follow a quadratic trend. The value of \\(d\\) also has an effect on the prediction intervals — the higher the value of \\(d\\), the more rapidly the prediction intervals increase in size. For \\(d = 0\\), the long-term forecast standard deviation will go to the standard deviation of the historical data, so the prediction intervals will all be essentially the same. In the case of us_change_fit, \\(c \\not= 0\\) and \\(d = 0\\), long-term forecasts go to the mean of the data. The value of \\(p\\) is important if the data show cycles. To obtain cyclic forecasts, it is necessary to have \\(p \\ge 2\\), along with some additional conditions on the parameters. For an AR(2) model, cyclic behaviour occurs if \\(\\phi_1^2 + 4\\phi_2&lt;0\\). In that case, the average period of the cycles is \\[ \\frac{2\\pi}{\\arccos[-\\phi_1(1 - \\phi_2) / 4\\phi_2]} \\] 10.3 Estimation and order selection Recall how ACF and PACF plot would help us pick an appropriate AR(p) or MA(q) model. However, for ARIMA models, ACF and PACF plots are only helpful when one of \\(p\\) and \\(q\\) is zero. If \\(p\\) and \\(q\\) are both positive, then the plots do not help in finding suitable values of \\(p\\) and \\(q\\). (Think of an ARMA(p, q) process, neither its autocorrelation nor its partial autocorrelation function breaks off) The data may follow an ARIMA(p, d, 0) model if the ACF and PACF plots of the differenced data show the following patterns: the ACF is exponentially decaying or sinusoidal; there is a significant spike at lag p in the PACF, but none beyond lag p The us_change data may follow an ARIMA(0, d, q) model if the ACF and PACF plots of the differenced data show the following patterns: the PACF is exponentially decaying or sinusoidal; there is a significant spike at lag q in the ACF, but none beyond lag q. us_change &lt;- read_csv(&quot;https://otexts.com/fpp3/extrafiles/us_change.csv&quot;) %&gt;% mutate(time = yearquarter(Time)) %&gt;% as_tsibble(index = time) us_change %&gt;% ACF(Consumption) %&gt;% autoplot() us_change %&gt;% PACF(Consumption) %&gt;% autoplot() The pattern in the first three spikes is what we would expect from an ARIMA(3, 0, 0), as the PACF tends to decrease. So in this case, the ACF and PACF lead us to think an ARIMA(3, 0, 0) model might be appropriate. us_change_fit2 &lt;- us_change %&gt;% model(ARIMA(Consumption ~ PDQ(0, 0, 0) + pdq(3, 0, 0))) us_change_fit2 %&gt;% report() #&gt; Series: Consumption #&gt; Model: ARIMA(3,0,0) w/ mean #&gt; #&gt; Coefficients: #&gt; ar1 ar2 ar3 constant #&gt; 0.2274 0.1604 0.2027 0.3050 #&gt; s.e. 0.0713 0.0723 0.0712 0.0421 #&gt; #&gt; sigma^2 estimated as 0.3494: log likelihood=-165.17 #&gt; AIC=340.34 AICc=340.67 BIC=356.5 This model is actually slightly better than the model identified by ARIMA() (with an AICc value of 340.67 compared to 342.08). The ARIMA() function did not find this model because it does not consider all possible models in its search. Use stepwise = FALSE and approximation = FALSE to expand search region us_change_fit3 &lt;- us_change %&gt;% model(ARIMA(Consumption ~ PDQ(0, 0, 0), stepwise = FALSE, approximation = FALSE)) report(us_change_fit3) #&gt; Series: Consumption #&gt; Model: ARIMA(3,0,0) w/ mean #&gt; #&gt; Coefficients: #&gt; ar1 ar2 ar3 constant #&gt; 0.2274 0.1604 0.2027 0.3050 #&gt; s.e. 0.0713 0.0723 0.0712 0.0421 #&gt; #&gt; sigma^2 estimated as 0.3494: log likelihood=-165.17 #&gt; AIC=340.34 AICc=340.67 BIC=356.5 10.3.1 MLE Once the model order has been identified (i.e., the values of p, d and q), we need to estimate the parameters \\(c, \\phi_1, \\dots, \\phi_p, \\theta_1, \\dots, \\theta_q\\). When R estimates the ARIMA model, it uses MLE. For ARIMA models, MLE is similar to the least squares estimates that would be obtained by minimising \\[ \\sum_{t=1}^{T}{\\varepsilon_t^2} \\] 10.3.2 Information Criteria Akaike’s Information Criterion (AIC), which was useful in selecting predictors for regression, is also useful for determining the order of an ARIMA model. It can be written as \\[ \\text{AIC} = -2\\log{L} + 2(p + q + k + 1) \\] where \\(L\\) is the likelihood of the data. Note that the last term in parentheses is the number of parameters in the model (including \\(\\sigma^2\\), the variance of the residuals). For ARIMA models, the corrected AIC can be written as \\[ \\text{AIC}_c = AIC + \\frac{2(p + q + k + 1)(p + q + k + 2)}{T - p -q -k -2} \\] and the Bayesian Information Criterion can be written as \\[ \\text{BIC} = \\text{AIC} + (\\log{T} - 2)(p + q + k + 1) \\] It is important to note that these information criteria tend not to be good guides to selecting the appropriate order of differencing (\\(d\\)) of a model, but only for selecting the values of \\(p\\) and \\(q\\). This is because the differencing changes the data on which the likelihood is computed, making the AIC values between models with different orders of differencing not comparable. So we need to use some other approach to choose \\(d\\), and then we can use the AICc to select \\(p\\) and \\(q\\). However, when comparing models using a test set, it does not matter how the forecasts were produced — the comparisons are always valid. 10.4 The ARIMA() function By default ARIMA(value) creates a automatic specification where the model space are confined to non-seasonal component \\(p \\in \\{1, 2, 3, 4, 5\\}, d \\in \\{0, 1, 2\\}, q \\in \\{0, 1, 2, 3, 4, 5\\}\\) and seasonal component (Section 10.6) \\(P \\in \\{0, 1, 2\\}, D \\in \\{0, 1\\}, Q \\in \\{0, 1, 2\\}\\). We can also use p_init andn q_init to specify the initial value for \\(p\\) and \\(q\\) if a stepwise procedure indroduced in the next section is used. We can also achieve manual specification of the model space in pdq() and PDQ() where pdq() specifies the non-seasonal part and PDQ() the seasonal part. In the case of a non-seasonal model, it is common to set ARIMA(value ~ PDQ(0, 0, 0)). This eliminates the seasonal part and keeps the default process of searching for a optimal seasonal part. Further, to search the best non-seasonal ARIMA model with \\(p \\in \\{1, 2, 3\\}\\), \\(q \\in \\{0, 1, 2\\}\\) and \\(d = 1\\), you could use ARIMA(y ~ pdq(1:3, 1, 0:2) + PDQ(0, 0, 0)). The default procedure uses some approximations to speed up the search. These approximations can be avoided with the argument approximation = FALSE. It is possible that the minimum AICc model will not be found due to these approximations, or because of the use of a stepwise procedure. A much larger set of models will be searched if the argument stepwise = FALSE is used. 10.4.1 Algorithm The ARIMA() function in R uses a variation of the Hyndman-Khandakar algorithm (2008). The algorithm combines unit root tests, minimization of \\(\\text{AIC}_c\\) and \\(\\text{MLE}\\) to obtain a “optimal” ARIMA model. The default behaviour is discussed in minute details at https://otexts.com/fpp3/arima-r.html. Here we only cover non-seasonal model, and the procedure for a SARIMA model is quite the same, \\(P\\) and \\(Q\\) are ultimately determined by minimizing \\(\\text{AIC}_c\\). The number of differences \\(0 \\le d \\le 2\\) is determined using repeated KPSS tests 10.1.3. (For \\(D\\), use unitroot_nsdiffs()) Use a stepwise search (rather than exhaustive) to determine \\(p\\) and \\(q\\) after differencing the time series \\(d\\) times. Four initial models are fitted: ARIMA(0, \\(d\\), 0) ARIMA(2, \\(d\\), 2) ARIMA(1, \\(d\\), 0) ARIMA(0, \\(d\\), 1) A constant is included unless \\(d = 2\\). If \\(d \\le 1\\), an additional model is also fitted: ARIMA(0, \\(d\\), 0). The best model (with the smallest \\(\\text{AIC}_c\\) value) fitted in the previous step is set to be the “current” model. Then variations on the current model are considered: vary \\(p\\) and/or \\(q\\) from the current model by \\(\\pm1\\) and exclude / include the constant term \\(c\\) in the model The best model considered so far (either the current model or one of these variations) becomes the new current model. Then the second step is repeated until no lower AICc can be found. 10.4.2 Modelling procedure A general strategy is recommended when fitting non-seasonanl arima models Plot the data and identify any unusual observations https://github.com/business-science/anomalize If necessary, transform the data (such as a Box-Cox transformation(ts %&gt;% features(var, features = guerrero)) or a logrithm transformation) to stabilise the variance. If the data are non-stationary, take first differences of the data until the data are stationary. Examine the ACF/PACF: Is an ARIMA(p, d, 0) or ARIMA(0, d, q) model appropriate? Try your chosen model(s), and use the \\(\\text{AIC}_c\\) to search for a better model. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals(9.1.2). If they do not look like white noise, try a modified model. Once the residuals look like white noise, calculate forecasts. The ARIMA() function takes care of step 3 - 5, so there are still possible needs to transform the data, to diagnose residuals and to give forecasts. 10.4.3 Example: Seasonally adjusted electrical equipment orders elec_equip is highly seasonal. Instead of creating a seaonal ARIMA model, we tend to use the seasonal adjusted series to build a non-seasonal model here elec_equip &lt;- as_tsibble(fpp2::elecequip) elec_equip %&gt;% autoplot() elec_equip %&gt;% ACF() %&gt;% autoplot() elec_equip %&gt;% features(value, feat_stl) #&gt; # A tibble: 1 x 9 #&gt; trend_strength seasonal_streng~ seasonal_peak_y~ seasonal_trough~ spikiness #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.943 0.908 0 8 0.00251 #&gt; # ... with 4 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, #&gt; # stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt; Use STL decomposition to obtain the seasonal adjusted data elec_adjusted &lt;- elec_equip %&gt;% model(STL(value)) %&gt;% components() %&gt;% select(index, season_adjust) %&gt;% as_tsibble(index = index) elec_adjusted %&gt;% features(season_adjust, feat_stl) #&gt; # A tibble: 1 x 9 #&gt; trend_strength seasonal_streng~ seasonal_peak_y~ seasonal_trough~ spikiness #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.944 0.0300 0 6 0.00249 #&gt; # ... with 4 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, #&gt; # stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt; Plot the data for detection of outliers and anomalies elec_adjusted %&gt;% autoplot() Is there a need for transformation ? # close to 1 elec_adjusted %&gt;% features(season_adjust, guerrero) #&gt; # A tibble: 1 x 1 #&gt; lambda_guerrero #&gt; &lt;dbl&gt; #&gt; 1 -1.00 Fit a ARIMA model: elec_fit &lt;- elec_adjusted %&gt;% model(ARIMA(season_adjust ~ PDQ())) # ARIMA(1, 1, 5) without constant elec_fit %&gt;% report() #&gt; Series: season_adjust #&gt; Model: ARIMA(1,1,5) #&gt; #&gt; Coefficients: #&gt; ar1 ma1 ma2 ma3 ma4 ma5 #&gt; 0.5672 -0.9543 0.3220 0.2300 -0.2863 0.2745 #&gt; s.e. 0.1490 0.1493 0.1064 0.0924 0.0913 0.0774 #&gt; #&gt; sigma^2 estimated as 8.348: log likelihood=-478.48 #&gt; AIC=970.97 AICc=971.57 BIC=993.84 Visualization and statistical tests on residuals elec_fit %&gt;% gg_tsresiduals() elec_aug &lt;- elec_fit %&gt;% augment() elec_aug %&gt;% features(.resid, ljung_box, lag = 8, dof = 7) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(season_adjust ~ PDQ()) 2.03 0.155 Producing forecasts elec_fit %&gt;% forecast(h = 24) %&gt;% autoplot(elec_adjusted) Note that Section 10.2.1, we mentioned that the long term forecast of an ARIMA model with no constant and \\(d = 1\\) will go to a constant. 10.4.4 Plotting the characteristic roots The stationarity conditions for the model are that the p complex roots of \\(\\Phi(B)\\) lie outside the unit circle, and the invertibility conditions are that the q complex roots of \\(\\Theta(B)\\) lie outside the unit circle. So we can see whether the model is close to invertibility or stationarity by a plot of the roots in relation to the complex unit circle. It is easier to plot the inverse roots instead, as they should all lie within the unit circle. This is easily done in R. For the \\(\\text{ARIMA}(3, 1, 1)\\) model fitted to the seasonally adjusted electrical equipment index gg_arma(elec_fit) The ARIMA() function will never return a model with inverse roots outside the unit circle. Models automatically selected by the ARIMA() function will not select a model with roots close to the unit circle. 10.5 Forecasting with ARIMA models 10.5.1 Point forecasts https://otexts.com/fpp3/arima-forecasting.html 10.5.2 Prediction intervals The calculation of ARIMA prediction intervals is more difficult, and here only a simple case is presented. The first prediction interval is easy to calculate. If \\(\\hat{\\sigma}^2\\) is the standard deviation of the residuals, then a 95% prediction interval is given by \\(\\hat{y}_{T + 1|T} \\pm 1.96 \\hat{\\sigma}^2\\). This result is true for all ARIMA models regardless of their parameters and orders. Multi-step prediction intervals for ARIMA(0, 0, q) models are relatively easy to calculate. We can write the model as \\[ y_t = \\varvarepsilon_t + \\sum_{i=1}^{q}\\theta_i\\varepsilon_{t-i} \\] Then, the estimated forecast variance can be written as \\[ \\hat{\\sigma}_h = \\hat{\\sigma}^2[1 + \\sum_{i = 1}^{h - 1}\\hat{\\theta}_i^2] \\] and a 95% prediction interval is given by \\(y_{T+h|T}±1.96 \\sqrt{\\hat{\\sigma}_h^2}\\). In Section 9.4.3, we showed that an AR(1) model can be written as an MA(\\(\\infty\\)) model. Using this equivalence, the above result for MA(q) models can also be used to obtain prediction intervals for AR(1) models. The prediction intervals for ARIMA models are based on assumptions that the residuals are uncorrelated and normally distributed. If either of these assumptions does not hold, then the prediction intervals may be incorrect. For this reason, always plot the ACF and histogram of the residuals to check the assumptions before producing prediction intervals. In general, prediction intervals from ARIMA models increase as the forecast horizon increases. For stationary models (i.e., with \\(d = 0\\)) they will converge, so that prediction intervals for long horizons are all essentially the same. For \\(d \\ge 1\\), the prediction intervals will continue to grow into the future. As with most prediction interval calculations, ARIMA-based intervals tend to be too narrow. This occurs because only the variation in the errors has been accounted for. There is also variation in the parameter estimates, and in the model order, that has not been included in the calculation. In addition, the calculation assumes that the historical patterns that have been modelled will continue into the forecast period. 10.6 Seasonal ARIMA models So far, we have restricted our attention to non-seasonal data and non-seasonal ARIMA models. However, ARIMA models are also capable of modelling a wide range of seasonal data. A seasonal ARIMA model includes additional seasonal terms, written as follows where \\(m\\) is number of observations per year. The seasonal part of the model consists of terms that are similar to the non-seasonal components of the model, but involve backshifts of the seasonal period. For example, an \\(\\text{ARIMA}(1, 1, 1)(1, 1, 1)_4\\) model (without a constant) is for quarterly data (m = 4), and can be written as (Not that a seasonal difference is not a m order difference) \\[ (1 - \\Phi_1B^4)(1 - \\phi_1B)(1 - B^4)(1 - B)y_t = \\varepsilon_t(1 - \\theta_1B)(1 - \\Theta_1B^4) \\] It can be easily shown that a SARIMA model can be converted to an ARMA model. For autoregression terms \\[ ar = (1 - \\phi_1B - \\cdots - \\phi_pB^p)(1 - B)^d \\\\ sar = (1 - \\Phi_1 B^m - \\cdots - B^{Pm})(1 - B^m)^D \\] Then, the AR polynomial of the inverted ARMA model can be written as \\[ AR = ar \\times sar \\] For moving average terms, we have \\[ ma = (1 + \\theta_1B + \\cdots + \\theta_qB^q) \\\\ sma = (1 + \\Theta_1B^m + \\cdots + \\Theta_1B^{Pm})(1 - B^m)^D \\] Then, the MA polynomial of the inverted ARMA model can be written as \\[ MA = ma \\times sma \\] In this way, we can get the ARMA(u, v) model inverted from the SARIMA model. It can be written as \\[ (1 - \\phi_1&#39;B - \\cdots - \\phi_u&#39;B^u)y_t = (1 - \\theta_1&#39;B - \\cdots - \\theta_v&#39;B^v)\\varepsilon_t \\] From Section 9.5.1, we also know that an ARMA model can be converted to AR or MA models under certain circumstances, so that SARIMA models can be further transformed in a similar way. 10.6.1 ACF and PACF The seasonal part of an AR or MA model will be seen in the seasonal lags of the PACF and ACF. For example, an \\(\\text{ARIMA}(0, 0, 0)(0, 0, 1)_{12}\\) model will show: a spike at lag 12 in the ACF but no other significant spikes; exponential decay in the seasonal lags of the PACF (i.e., at lags 12, 24, 36, …). Similarly, an \\(\\text{ARIMA}(0, 0, 0)(1, 0, 0)_{12}\\) model will show: exponential decay in the seasonal lags of the ACF; a single significant spike at lag 12 in the PACF. 10.6.2 Example: European quarterly retail trade eu_retail: quarterly European retail trade data from 1996 to 2011 eu_retail &lt;- as_tsibble(fpp2::euretail) eu_retail %&gt;% autoplot(value) The data is non-stationary, with a degree of seasonality eu_retail %&gt;% features(value, feat_stl) #&gt; # A tibble: 1 x 9 #&gt; trend_strength seasonal_streng~ seasonal_peak_y~ seasonal_trough~ spikiness #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.998 0.703 0 1 3.77e-7 #&gt; # ... with 4 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, #&gt; # stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt; We will first take a seasonal difference. These still appear to be some sort of nonstationarity, so we take an additional first difference eu_retail %&gt;% gg_tsdisplay(difference(value, 4), plot_type=&#39;partial&#39;) eu_retail %&gt;% gg_tsdisplay(value %&gt;% difference(4) %&gt;% difference(), plot_type=&#39;partial&#39;) Our aim now is to find an appropriate ARIMA model based on the ACF and PACF plots. The significant spike at lag 1 in the ACF suggests a non-seasonal MA(1) component, and the significant spike at lag 4 in the ACF suggests a seasonal MA(1) component. Consequently, we begin with an \\(\\text{ARIMA}(0, 1, 1)(0, 1, 1)_4\\) model, indicating a first and seasonal difference, and non-seasonal and seasonal MA(1) components. By analogous logic applied to the PACF, we could also have started with an \\(\\text{ARIMA}(1,1,0)(1,1,0)_4\\) model. (Not that only either \\(p\\) and \\(q\\) is zero can ACF and PACF plots be used to decide orders. 9.5) eu_retail_fit &lt;- eu_retail %&gt;% model(ARIMA(value ~ pdq(0, 1, 1) + PDQ(0, 1, 1))) eu_retail_fit %&gt;% report() #&gt; Series: value #&gt; Model: ARIMA(0,1,1)(0,1,1)[4] #&gt; #&gt; Coefficients: #&gt; ma1 sma1 #&gt; 0.2903 -0.6913 #&gt; s.e. 0.1118 0.1193 #&gt; #&gt; sigma^2 estimated as 0.188: log likelihood=-34.64 #&gt; AIC=75.28 AICc=75.72 BIC=81.51 Check residuals eu_retail_aug &lt;- eu_retail_fit %&gt;% augment() eu_retail_aug %&gt;% ACF(.resid) %&gt;% autoplot() eu_retail_aug %&gt;% PACF(.resid) %&gt;% autoplot() Both the ACF and PACF show significant spikes at lag 2, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model. What if we just let ARIMA() pick a model for us? eu_retail_fit2 &lt;- eu_retail %&gt;% model(ARIMA(value)) eu_retail_fit2 %&gt;% report() #&gt; Series: value #&gt; Model: ARIMA(0,1,3)(0,1,1)[4] #&gt; #&gt; Coefficients: #&gt; ma1 ma2 ma3 sma1 #&gt; 0.2630 0.3694 0.4200 -0.6636 #&gt; s.e. 0.1237 0.1255 0.1294 0.1545 #&gt; #&gt; sigma^2 estimated as 0.156: log likelihood=-28.63 #&gt; AIC=67.26 AICc=68.39 BIC=77.65 The automatic picked model is \\(\\text{ARIMA}(0, 1, 3)(0, 1, 1)_4\\), and has lower \\(\\text{AIC}_c\\), 68.39 compared to 75.72. This in some sense explains why previous residuals show some non-seaonal correlation pattern and how it is fixed. eu_retail_fit2 %&gt;% gg_tsresiduals() eu_retail_fit2 %&gt;% augment() %&gt;% features(.resid, ljung_box, lag = 8, dof = 5) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(value) 0.511 0.916 Forecasts from the model for the next three years are shown below eu_retail_fit2 %&gt;% forecast(h = 12) %&gt;% autoplot(eu_retail) The forecasts follow the recent trend in the data, because of the double differencing. The large and rapidly increasing prediction intervals show that the retail trade index could start increasing or decreasing at any time — while the point forecasts trend downwards, the prediction intervals allow for the data to trend upwards during the forecast period. 10.6.3 Example: Corticosteroid drug sales in Australia We will try to forecast monthly corticosteroid drug sales in Australia. There is a small increase in the variance with the level, so we take logarithms to stabilise the variance. h02 &lt;- tsibbledata::PBS %&gt;% filter(ATC2 == &quot;H02&quot;) %&gt;% summarize(cost = sum(Cost) / 1e6) h02 %&gt;% mutate(cost_log = log(cost)) %&gt;% pivot_longer(c(cost, cost_log), names_to = &quot;measure&quot;, values_to = &quot;value&quot;) %&gt;% ggplot() + geom_line(aes(Month, value)) + facet_wrap(~ measure, scales = &quot;free_y&quot;, nrow = 2) The data are strongly seasonal and obviously non-stationary, so seasonal differencing will be used. It is unclear that whether another difference should be used at this point, and we decide not to. The last few observations appear to be different (more variable) from the earlier data. This may be due to the fact that data are sometimes revised when earlier sales are reported late. h02 %&gt;% gg_tsdisplay(difference(log(cost), 12), plot_type=&#39;partial&#39;, lag_max = 24) In the plots of the seasonally differenced data, there are spikes in the PACF at lags 12 and 24, but nothing at seasonal lags in the ACF. This may be suggestive of a seasonal AR(2) term. In the non-seasonal lags, there are three significant spikes in the PACF, suggesting a possible AR(3) term. The pattern in the ACF is not indicative of any simple model. Consequently, this initial analysis suggests that a possible model for these data is an \\(\\text{ARIMA}(3, 0, 0)(2, 1, 0)_12\\). The \\(\\text{AIC}_c\\) of such model and some variations are shown: Of these models, the best is the \\(ARIMA(3, 0, 1)(0, 1, 2)_12\\) model h02_fit &lt;- h02 %&gt;% model(ARIMA(log(cost) ~ 0 + pdq(3, 0, 1) + PDQ(0, 1, 2))) # without constant h02_fit %&gt;% report() #&gt; Series: cost #&gt; Model: ARIMA(3,0,1)(0,1,2)[12] #&gt; Transformation: log(.x) #&gt; #&gt; Coefficients: #&gt; ar1 ar2 ar3 ma1 sma1 sma2 #&gt; -0.1603 0.5481 0.5678 0.3827 -0.5222 -0.1768 #&gt; s.e. 0.1636 0.0878 0.0942 0.1895 0.0861 0.0872 #&gt; #&gt; sigma^2 estimated as 0.004278: log likelihood=250.04 #&gt; AIC=-486.08 AICc=-485.48 BIC=-463.28 h02_aug &lt;- h02_fit %&gt;% augment() h02_aug %&gt;% ACF(.resid, lag_max = 36) %&gt;% autoplot() h02_aug %&gt;% PACF(.resid, lag_max = 36) %&gt;% autoplot() h02_aug %&gt;% features(.resid, ljung_box, lag = 24, dof = 7) # h = 2m for seasonal data #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(log(cost) ~ 0 + pdq(3, 0, 1) + PDQ(0, 1, 2)) 23.7 0.129 Produce forecasts h02_fit %&gt;% forecast() %&gt;% autoplot(h02) 10.7 ETS and ARIMA While linear exponential smoothing models are all special cases of ARIMA models, the non-linear exponential smoothing models have no equivalent ARIMA counterparts. On the other hand, there are also many ARIMA models that have no exponential smoothing counterparts. In particular, all ETS models are non-stationary, while some ARIMA models are stationary. The ETS models with seasonality or non-damped trend or both have two unit roots (i.e., they need two levels of differencing to make them stationary). All other ETS models have one unit root (they need one level of differencing to make them stationary). The AICc is useful for selecting between models in the same class. For example, we can use it to select an ARIMA model between candidate ARIMA models or an ETS model between candidate ETS models. However, it cannot be used to compare between ETS and ARIMA models because they are in different model classes, and the likelihood is computed in different ways. The examples below demonstrate selecting between these classes of models. 10.7.1 Example: Comparing ARIMA() and ETS() on non-seasonal data We can use time series cross-validation to compare an ARIMA model and an ETS model. Let’s consider the Australian population from the global_economy dataset aus_economy &lt;- global_economy %&gt;% filter(Code == &quot;AUS&quot;) %&gt;% mutate(Population = Population/1e6) aus_economy %&gt;% slice(-n()) %&gt;% stretch_tsibble(.init = 10) %&gt;% model( ETS(Population), ARIMA(Population) ) %&gt;% forecast(h = 1) %&gt;% accuracy(aus_economy) #&gt; # A tibble: 2 x 10 #&gt; .model Country .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(Population) Australia Test 0.0420 0.194 0.0789 0.277 0.509 0.317 0.188 #&gt; 2 ETS(Population) Australia Test 0.0202 0.0774 0.0543 0.112 0.327 0.218 0.506 In this case the ETS model has higher accuracy on the cross-validated performance measures. Below we generate and plot forecasts for the next 5 years generated from an ETS model. aus_economy %&gt;% model(ETS(Population)) %&gt;% forecast(h = &quot;5 years&quot;) %&gt;% autoplot(aus_economy) 10.7.2 Example: Comparing ARIMA() and ETS() on seasonal data In this case we want to compare seasonal ARIMA and ETS models applied to the quarterly cement production data (from aus_production). This time we separate the original data into testing set and training set. We create a training set from the beginning of 1988 to the end of 2007 and select an ARIMA and an ETS model using the ARIMA() and ETS() functions. # Consider the cement data beginning in 1988 cement &lt;- aus_production %&gt;% filter(year(Quarter) &gt;= 1988) # Use 20 years of the data as the training set train &lt;- cement %&gt;% filter(year(Quarter) &lt;= 2007) cement_fit_arima &lt;- train %&gt;% model(ARIMA(Cement)) report(cement_fit_arima) #&gt; Series: Cement #&gt; Model: ARIMA(1,0,1)(2,1,1)[4] w/ drift #&gt; #&gt; Coefficients: #&gt; ar1 ma1 sar1 sar2 sma1 constant #&gt; 0.8886 -0.2366 0.081 -0.2345 -0.8979 5.3884 #&gt; s.e. 0.0842 0.1334 0.157 0.1392 0.1780 1.4844 #&gt; #&gt; sigma^2 estimated as 11456: log likelihood=-463.52 #&gt; AIC=941.03 AICc=942.68 BIC=957.35 Residuals from the ARIMA model appear to be white noise. cement_fit_arima %&gt;% gg_tsresiduals() augment(cement_fit_arima) %&gt;% features(.resid, ljung_box, lag = 8, dof = 7) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(Cement) 0.783 0.376 The output below also shows the ETS model selected and estimated by ETS(). This model also does well in capturing all the dynamics in the data cement_fit_ets &lt;- train %&gt;% model(ETS(Cement)) report(cement_fit_ets) #&gt; Series: Cement #&gt; Model: ETS(M,N,M) #&gt; Smoothing parameters: #&gt; alpha = 0.7533714 #&gt; gamma = 0.0001000093 #&gt; #&gt; Initial states: #&gt; l s1 s2 s3 s4 #&gt; 1694.712 1.031179 1.045209 1.011424 0.9121874 #&gt; #&gt; sigma^2: 0.0034 #&gt; #&gt; AIC AICc BIC #&gt; 1104.095 1105.650 1120.769 cement_fit_ets %&gt;% gg_tsresiduals() augment(cement_fit_ets) %&gt;% features(.resid, ljung_box, lag = 8, dof = 6) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ETS(Cement) 5.89 0.0525 The output below evaluates the forecasting performance of the two competing models over the test set. In this case the ARIMA model seems to be the slightly more accurate model based on the test set RMSE, MAPE and MASE. bind_rows( cement_fit_arima %&gt;% accuracy(), cement_fit_ets %&gt;% accuracy(), cement_fit_arima %&gt;% forecast(h = &quot;2 years 6 months&quot;) %&gt;% accuracy(cement), cement_fit_ets %&gt;% forecast(h = &quot;2 years 6 months&quot;) %&gt;% accuracy(cement) ) #&gt; # A tibble: 4 x 9 #&gt; .model .type ME RMSE MAE MPE MAPE MASE ACF1 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(Cement) Training -6.21 100. 79.9 -0.670 4.37 0.546 -0.0113 #&gt; 2 ETS(Cement) Training 12.8 103. 80.0 0.427 4.41 0.547 -0.0528 #&gt; 3 ARIMA(Cement) Test -161. 216. 186. -7.71 8.68 1.27 0.387 #&gt; 4 ETS(Cement) Test -171. 222. 191. -8.07 8.85 1.30 0.579 Notice that the ETS model fits the training data slightly better than the ARIMA model, but that the ARIMA model provides more accurate forecasts on the test set. A good fit to training data is never an indication that the model will forecast well. Below we generate and plot forecasts from an ETS model for the next 3 years. # Generate forecasts from an ETS model cement %&gt;% model(ETS(Cement)) %&gt;% forecast(h = &quot;3 years&quot;) %&gt;% autoplot(cement) "],
["dynamic-regression-models.html", "Chapter 11 Dynamic regression models 11.1 Estimation 11.2 Regression with ARIMA errors 11.3 Forecasting 11.4 Deterministic and stochastic trends 11.5 Dynamic harmonic regression 11.6 Lagged predictors", " Chapter 11 Dynamic regression models library(tsibble) library(tsibbledata) library(fable) library(feasts) library(lubridate) The time series models in the previous two chapters allow for the inclusion of information from past observations of a series, but not for the inclusion of other information (external variables) that may also be relevant. On the other hand, the regression models in Chapter 7 allow for the inclusion of a lot of relevant information from predictor variables, but do not allow for the subtle time series dynamics that can be handled with ARIMA models. In this chapter, we consider how to extend ARIMA models in order to allow other information to be included in the models. In Chapter 7 we considered the time series linear model: \\[ y_t = \\beta_0 + \\beta_1x_{1t} + \\beta_2x_{2t} + \\dots + \\beta_kx_{kt} + \\varepsilon_t \\] where \\(\\varepsilon_t\\) is usually assumed to be an uncorrelated error term (i.e., it is white noise). We considered tests such as the Breusch-Godfrey test for assessing whether the resulting residuals were significantly correlated. A regression model showing high \\(R^2\\) as well as high \\(\\hat{\\sigma}^2\\) is likely to be a spurious regression (Section 7.3.5). In this chapter, we will allow the errors from a regression to contain autocorrelation. Such correlated errors are denoted by \\(\\eta_t\\). The error series \\(\\eta_t\\) is assumed to follow an ARIMA model. For example, if \\(\\eta_t\\) follows an ARIMA(1, 1, 1) model, we can write \\[ y_t = \\beta_0 + \\beta_1x_{1t} + \\beta_2x_{2t} + \\dots + \\beta_kx_{kt} + \\eta_t \\\\ (1 - \\phi_1B)(1 - B)\\eta_t = (1 - \\theta_1B)\\varepsilon_t \\] where \\(\\varepsilon_t\\) is a white noise series. Notice that the model has two error terms here — the error from the regression model, which we denote by \\(\\eta_t\\), and the error from the ARIMA model, which we denote by \\(\\varepsilon_t\\). Only the ARIMA model errors are assumed to be white noise. 11.1 Estimation When we estimate the parameters from the model, we need to minimise the sum of squared εt values. If we minimise the sum of squared \\(\\eta_t\\) values instead (which is what would happen if we estimated the regression model ignoring the autocorrelations in the errors), then several problems arise. \\(\\hat{\\beta}_0, \\dots, \\hat{\\beta}_k\\) are no longer the best estimates, as some information has been ignored in the calculation Any statistical tests associated with the model (e.g., t-tests on the coefficients) will be incorrect. The \\(\\text{AIC}_c\\) values of the fitted models are no longer a good guide as to which is the best model for forecasting. In most cases, the \\(p\\)-values associated with the coefficients will be too small, and so some predictor variables will appear to be important when they are not. This is known as “spurious regression” (see Section 7.3.5). Minimising the sum of squared \\(\\eta_t\\) values avoids these problems. Alternatively, maximum likelihood estimation can be used; this will give similar estimates of the coefficients. An important consideration when estimating a regression with ARMA errors is that all of the variables in the model must first be stationary. Thus, we first have to check that \\(y_t\\) and all of the predictors \\(x_{1t}, \\dots ,x_{kt}\\) appear to be stationary. If we estimate the model when any of these are non-stationary, the estimated coefficients will not be consistent estimates (and therefore may not be meaningful). We therefore first difference the non-stationary variables in the model. It is often desirable to maintain the form of the relationship between \\(y_t\\) and the predictors, and consequently it is common to difference all of the variables if any of them need differencing. The resulting model is then called a “model in differences”, as distinct from a “model in levels”, which is what is obtained when the original data are used without differencing. If all of the variables in the model are stationary, then we only need to consider ARMA errors for the residuals. It is easy to see that a regression model with ARIMA(p, d, q) errors is equivalent to regression model in d-differences with ARMA(p, q) errors. For example, we obtain from a univariate regression model with ARIMA(1, 1, 1) errors that \\[\\begin{equation} \\tag{11.1} y_t = \\beta_0 + \\beta_1x_t + \\eta_t \\\\ (1 - \\phi_1B)(1 - B)\\eta_t = c + (1 + \\theta_1B)\\varepsilon_t \\\\ \\end{equation}\\] Expand the second equation we get \\[ \\eta_t&#39; = c + \\phi_1\\eta_{t-1}&#39; + \\theta_1\\varepsilon_{t-1} + \\varepsilon_t \\] Note that \\(\\eta_t&#39;\\) can be considered as a ARMA(1, 1) series. Difference the first equation in (11.1) we get \\[ y_t&#39; = \\beta_1x_t&#39; + \\eta_t&#39; \\] This is now a linear model with outcome \\(y_t&#39;\\), predictor \\(x&#39;_t\\) and ARMA(1, 1) error \\(\\eta_t&#39;\\). Let \\(\\eta_t = \\eta_t&#39;\\), the differenced model can be written as \\[\\begin{equation} \\tag{11.2} y_t&#39; = \\beta_1x_t&#39; + \\eta_t \\\\ (1 - \\phi_1B)\\eta_t = c + (1 + \\theta_1B)\\varepsilon_t{ \\end{equation}\\] The model in differences (11.2) is equavalent to our former undifferenced model (11.1), with \\(\\eta_t\\) denoting a ARMA(1, 1) and ARIMA(1, 1) errors respectively. 11.2 Regression with ARIMA errors ARIMA() will fit a regression model with ARIMA errors if exogenous regressors are included in the formula. The pdq() special specifies the order of the ARIMA error model. If differencing is specified, then the differencing is applied to all variables in the regression model before the model is estimated. ARIMA(y ~ x + pdq(1, 1, 0)) will fit the model \\(y_t&#39; = \\beta_1x_t&#39; + \\eta_t&#39;\\) where \\(\\eta_t&#39; = c + \\phi_1\\eta_{t-1}&#39; + \\varepsilon_t\\) is an ARMA(1, 0, 0) error. This is equivalent to fitting the model \\[ y_t = \\beta_0 + \\beta_1 x + \\eta_t \\\\ (1 - \\phi_1B)(1 - B)\\eta_t = c + \\varepsilon_t \\] where \\(\\eta_t\\) is ARIMA(1, 1, 1) error. The ARIMA() function can also be used to select the best ARIMA model for the errors. This is done by not specifying the pdq() special, ARIMA(y ~ x) will find the most approriate \\(p\\), \\(d\\), \\(q\\) for ARIMA error \\(\\varepsilon_t\\). If differencing is required, then all variables are differenced during the estimation process, although the final model will be expressed in terms of the original variables. The AICc is calculated for the final model, and this value can be used to determine the best predictors. That is, the procedure should be repeated for all subsets of predictors to be considered, and the model with the lowest AICc value selected. 11.2.1 Example: US Personal Consumption and Income us_change &lt;- readr::read_csv(&quot;https://otexts.com/fpp3/extrafiles/us_change.csv&quot;) %&gt;% mutate(Time = yearquarter(Time)) %&gt;% as_tsibble(index = Time) us_change %&gt;% pivot_longer(c(Consumption, Income), names_to = &quot;var&quot;, values_to = &quot;value&quot;) %&gt;% ggplot(aes(Time, value)) + geom_line() + facet_wrap(~ var, nrow = 2) + labs(title = &quot;Quarterly changes in US consumption and personal income&quot;, x = &quot;Time&quot;, y = NULL) We start by fitting a time series regression model, without ARIMA error: us_change_lm &lt;- us_change %&gt;% lm(Consumption ~ Income, data = .) glance(us_change_lm) #&gt; # A tibble: 1 x 12 #&gt; r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.159 0.154 0.603 35.0 1.58e-8 1 -170. 345. 355. #&gt; # ... with 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; us_change %&gt;% ggplot(aes(Income, Consumption)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) The linear model fit the data rather poorly (\\(R^2 = 0.16\\)). We then fit a regression model with ARIMA errors us_change_dynamic &lt;- us_change %&gt;% model(ARIMA(Consumption ~ Income)) us_change_dynamic %&gt;% report() #&gt; Series: Consumption #&gt; Model: LM w/ ARIMA(1,0,2) errors #&gt; #&gt; Coefficients: #&gt; ar1 ma1 ma2 Income intercept #&gt; 0.6922 -0.5758 0.1984 0.2028 0.5990 #&gt; s.e. 0.1159 0.1301 0.0756 0.0461 0.0884 #&gt; #&gt; sigma^2 estimated as 0.3219: log likelihood=-156.95 #&gt; AIC=325.91 AICc=326.37 BIC=345.29 The data are clearly already stationary (as we are considering percentage changes rather than raw expenditure and income), so there is no need for any differencing. The fitted model is \\[ y_t = 0.6 + 0.2 \\text{Income} + \\eta_t \\\\ \\eta_t = 0.69\\eta_{t-1} - 0.58\\varepsilon_{t-1} + 0.2\\varepsilon_{t-2} + \\varepsilon_t \\\\ \\varepsilon_t \\sim NID(0, 0.32) \\] We can recover estimates of both the \\(\\eta_t\\) and \\(\\varepsilon_t\\) series using the residuals() function (augment returns arima errors). It is the ARIMA errors that should resemble a white noise series. # regression errors res1 &lt;- residuals(us_change_dynamic, type = &quot;regression&quot;) %&gt;% as_tibble() %&gt;% mutate(type = &quot;regression&quot;) # arima errors res2 &lt;- residuals(us_change_dynamic, type = &quot;innovation&quot;) %&gt;% as_tibble() %&gt;% mutate(type = &quot;arima&quot;) bind_rows(res1, res2) %&gt;% ggplot(aes(Time, .resid, color = type)) + geom_line() us_change_dynamic %&gt;% gg_tsresiduals() augment(us_change_dynamic) %&gt;% features(.resid, ljung_box, dof = 6, lag = 10) # unseasonal data lag = 10, seaonsal data lag = 2m #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(Consumption ~ Income) 6.05 0.196 11.3 Forecasting To forecast using a regression model with ARIMA errors, we need to forecast the regression part of the model and the ARIMA part of the model, and combine the results. As with ordinary regression models, in order to obtain forecasts we first need to forecast the predictors. We continue with the us_change_dynamic model, assuming that the future percentage changes in personal disposable income will be equal to the mean percentage change from the last forty years. us_change_future &lt;- new_data(us_change, 8) %&gt;% mutate(Income = mean(us_change$Income)) us_change_dynamic %&gt;% forecast(new_data = us_change_future) %&gt;% autoplot(us_change, level = 95) It is important to realise that the prediction intervals from regression models (with or without ARIMA errors) do not take into account the uncertainty in the forecasts of the predictors. So they should be interpreted as being conditional on the assumed (or estimated) future values of the predictor variables. 11.3.1 Example: Forecasting electricity demand Daily electricity demand can be modelled as a function of temperature. As can be observed on an electricity bill, more electricity is used on cold days due to heating and hot days due to air conditioning. The higher demand on cold and hot days is reflected in a u-shape, where daily demand is plotted versus daily maximum temperature vic_elec_daily &lt;- vic_elec %&gt;% filter(year(Time) == 2014) %&gt;% index_by(date = as_date(Time)) %&gt;% summarize( demand = sum(Demand) / 1e3, temperature = max(Temperature), holiday = any(Holiday) ) %&gt;% mutate(day_type = case_when( holiday == &quot;TRUE&quot; ~ &quot;holiday&quot;, wday(date) %in% 2:6 ~ &quot;weekday&quot;, TRUE ~ &quot;weekend&quot; )) vic_elec_daily %&gt;% ggplot(aes(temperature, demand, color = day_type)) + geom_point() + labs(x = &quot;Maximum daily temperature&quot;, y = &quot;Electricity demand (GW)&quot;) Electricity demand appears to be quadratically related to maximum temperature, whether the day is weekend or not can cause a shift in demand. Therefore, we fit a quadratic regression model with ARMA errors. The model also includes an indicator variable for if the day was a working day or not. elec_fit &lt;- vic_elec_daily %&gt;% model(ARIMA(demand ~ temperature + temperature ^ 2 + (day_type == &quot;weekday&quot;))) elec_fit %&gt;% report() #&gt; Series: demand #&gt; Model: LM w/ ARIMA(1,0,4)(0,1,1)[7] errors #&gt; #&gt; Coefficients: #&gt; ar1 ma1 ma2 ma3 ma4 sma1 temperature #&gt; 0.9808 -0.0913 -0.1841 -0.1560 -0.1842 -0.9325 1.5529 #&gt; s.e. 0.0174 0.0568 0.0573 0.0566 0.0634 0.0277 0.1396 #&gt; day_type == &quot;weekday&quot;TRUE #&gt; 30.6203 #&gt; s.e. 3.8442 #&gt; #&gt; sigma^2 estimated as 114.8: log likelihood=-1359.71 #&gt; AIC=2737.41 AICc=2737.93 BIC=2772.34 In fact, the estimated \\(\\hat{\\eta_t}\\) is a SARIMA error, this is due to the fact the vic_elec_daily is highly seasonal vic_elec_daily %&gt;% features(demand, feat_stl) #&gt; # A tibble: 1 x 9 #&gt; trend_strength seasonal_streng~ seasonal_peak_w~ seasonal_trough~ spikiness #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.772 0.709 0 5 0.796 #&gt; # ... with 4 more variables: linearity &lt;dbl&gt;, curvature &lt;dbl&gt;, #&gt; # stl_e_acf1 &lt;dbl&gt;, stl_e_acf10 &lt;dbl&gt; elec_fit %&gt;% gg_tsresiduals() augment(elec_fit) %&gt;% features(.resid, ljung_box, dof = 9, lag = 14) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 &quot;ARIMA(demand ~ temperature + temperature^2 + (day_type == ~ 9.77 0.0819 The model has some significant autocorrelation in the residuals, which means the prediction intervals may not provide accurate coverage. Also, the histogram of the residuals shows one positive outlier, which will also affect the coverage of the prediction intervals. augment(elec_fit) %&gt;% ggplot(aes(demand, .fitted)) + geom_point() + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) + labs(x = &quot;Demand (actual values)&quot;, y = &quot;Demand (fitted values)&quot;) To generate foreccasts, we set the temperature for the next 14 days to a constant 26 degrees. vic_elec_future &lt;- new_data(vic_elec_daily, 14) %&gt;% mutate( temperature = 26, holiday = c(TRUE, rep(FALSE, 13)), day_type = case_when( holiday ~ &quot;Holiday&quot;, wday(date) %in% 2:6 ~ &quot;Weekday&quot;, TRUE ~ &quot;Weekend&quot; ) ) elec_fit %&gt;% forecast(new_data = vic_elec_future) %&gt;% autoplot(vic_elec_daily, level = 95) The point forecasts look reasonable for the first two weeks of 2015. The slow down in electricity demand at the end of 2014 (due to many people taking summer vacations) has caused the forecasts for the next two weeks to show similarly low demand values. 11.4 Deterministic and stochastic trends There are two different ways of using a trend variable \\(t\\). A deterministic trend is obtained using the regression model \\[ y_t = \\beta_0 + \\beta_1t + \\eta_t \\] where \\(\\eta_t\\) is an ARMA process. A stochastic trend is obtained using the model \\[ y_t = \\beta_0 + \\beta_1t + \\eta_t \\] where \\(\\eta_t\\) is an ARIMA process with \\(d = 1\\). In the latter case, we can difference both sides so that \\(y&#39;_t = \\beta_1 + \\eta_t&#39;\\), where \\(\\eta_t&#39;\\) is an ARMA process. In other words, a stochastic trend can be expressed as \\[ y_t = \\beta_1 + y_{t-1} + \\eta_t&#39; \\] This is similar to a random walk with drift (introduced in Section 9.3.3), but here the error term is an ARMA process rather than simply white noise. Although these models appear quite similar (they only differ in the number of differences that need to be applied to \\(\\eta_t\\)), their forecasting characteristics are quite different. 11.4.1 Example: International visitors to Australia aus_visitors &lt;- as_tsibble(fpp2::austa) aus_visitors %&gt;% autoplot(value) + labs(x = &quot;Year&quot;, y = &quot;millions of people&quot;, title = &quot;Total annual international visitors to Australia&quot;) We will fit both a deterministic and a stochastic trend model to these data. The deterministic trend model is obtained as follows: visitors_deterministic &lt;- aus_visitors %&gt;% model(ARIMA(value ~ trend() + pdq(d = 0))) report(visitors_deterministic) #&gt; Series: value #&gt; Model: LM w/ ARIMA(2,0,0) errors #&gt; #&gt; Coefficients: #&gt; ar1 ar2 trend() intercept #&gt; 1.1127 -0.3805 0.1710 0.4156 #&gt; s.e. 0.1600 0.1585 0.0088 0.1897 #&gt; #&gt; sigma^2 estimated as 0.02979: log likelihood=13.6 #&gt; AIC=-17.2 AICc=-15.2 BIC=-9.28 The estimated growth in visitor numbers is 0.17 million people per year. Alternatively, the stochastic trend model can be estimated. visitors_stochastic &lt;- aus_visitors %&gt;% model(ARIMA(value ~ trend() + pdq(d = 1))) visitors_stochastic %&gt;% report() #&gt; Series: value #&gt; Model: LM w/ ARIMA(0,1,1) errors #&gt; #&gt; Coefficients: #&gt; ma1 trend() #&gt; 0.3006 0.1735 #&gt; s.e. 0.1647 0.0390 #&gt; #&gt; sigma^2 estimated as 0.03376: log likelihood=10.62 #&gt; AIC=-15.24 AICc=-14.46 BIC=-10.57 In this case, the estimated growth in visitor numbers is also 0.17 million people per year. Although the growth estimates are similar, the prediction intervals are not. In particular, stochastic trends have much wider prediction intervals because the errors are non-stationary. bind_cols(visitors_deterministic, visitors_stochastic) %&gt;% rename(`Deterministic trend` = 1, `Stochastic trend` = 2) %&gt;% forecast(h = 10) %&gt;% autoplot(aus_visitors, level = 95) + labs(x = &quot;Year&quot;, y = &quot;Visitors to Australia (millions)&quot;, title = &quot;Forecasts from trend models&quot;) There is an implicit assumption with deterministic trends that the slope of the trend is not going to change over time. On the other hand, stochastic trends can change, and the estimated growth is only assumed to be the average growth over the historical period, not necessarily the rate of growth that will be observed into the future. Consequently, it is safer to forecast with stochastic trends, especially for longer forecast horizons, as the prediction intervals allow for greater uncertainty in future growth. 11.5 Dynamic harmonic regression When there are long seasonal periods, a dynamic regression with Fourier terms (Section 7.4.8)is often better than other models we have considered so far. Seasonal versions of ARIMA and ETS models are designed for shorter periods such as 12 for monthly data or 4 for quarterly data. The ETS() model restricts seasonality to be a maximum period of 24 to allow hourly data but not data with a larger seasonal frequency. The problem is that there are m−1 parameters to be estimated for the initial seasonal states where \\(m\\) is the seasonal period. So for large \\(m\\), the estimation becomes almost impossible. The ARIMA() function will allow a seasonal period up to \\(m = 350\\), but in practice will usually run out of memory whenever the seasonal period is more than about 200. In any case, seasonal differencing of high order does not make a lot of sense — for daily data it involves comparing what happened today with what happened exactly a year ago and there is no constraint that the seasonal pattern is smooth. So for such time series, we prefer a harmonic regression approach where the seasonal pattern is modelled using Fourier terms with short-term time series dynamics handled by an ARMA error. The advantages of this approach are: for data with more than one seasonal period, Fourier terms of different frequencies can be included; the smoothness of the seasonal pattern can be controlled by \\(K\\), the number of Fourier sin and cos pairs – the seasonal pattern is smoother for smaller values of \\(K\\); the short-term dynamics are easily handled with a simple ARMA error. The only real disadvantage (compared to a seasonal ARIMA model) is that the seasonality is assumed to be fixed — the seasonal pattern is not allowed to change over time. But in practice, seasonality is usually remarkably constant so this is not a big disadvantage except for long time series. 11.5.1 Example: Australian eating out expenditure In this example we demonstrate combining Fourier terms for capturing seasonality with ARIMA errors capturing other dynamics in the data. For monthly data, we vary \\(K\\) from \\(1\\) to \\(6\\) aus_cafe &lt;- aus_retail %&gt;% filter( Industry == &quot;Cafes, restaurants and takeaway food services&quot;, year(Month) %in% 2004:2018 ) %&gt;% summarise(Turnover = sum(Turnover)) aus_cafe %&gt;% autoplot() To stablize variance, we instead forecast log(Turnover) cafe_fit &lt;- model( aus_cafe, `K = 1` = ARIMA(log(Turnover) ~ fourier(K = 1) + PDQ(0, 0, 0)), `K = 2` = ARIMA(log(Turnover) ~ fourier(K = 2) + PDQ(0, 0, 0)), `K = 3` = ARIMA(log(Turnover) ~ fourier(K = 3) + PDQ(0, 0, 0)), `K = 4` = ARIMA(log(Turnover) ~ fourier(K = 4) + PDQ(0, 0, 0)), `K = 5` = ARIMA(log(Turnover) ~ fourier(K = 5) + PDQ(0, 0, 0)), `K = 6` = ARIMA(log(Turnover) ~ fourier(K = 6) + PDQ(0, 0, 0)) ) cafe_fit %&gt;% glance() #&gt; # A tibble: 6 x 8 #&gt; .model sigma2 log_lik AIC AICc BIC ar_roots ma_roots #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 K = 1 0.00175 317. -616. -615. -588. &lt;cpl [2]&gt; &lt;cpl [3]&gt; #&gt; 2 K = 2 0.00107 362. -700. -698. -661. &lt;cpl [5]&gt; &lt;cpl [1]&gt; #&gt; 3 K = 3 0.000761 394. -763. -761. -725. &lt;cpl [3]&gt; &lt;cpl [1]&gt; #&gt; 4 K = 4 0.000539 427. -822. -818. -771. &lt;cpl [1]&gt; &lt;cpl [5]&gt; #&gt; 5 K = 5 0.000317 474. -919. -917. -875. &lt;cpl [2]&gt; &lt;cpl [0]&gt; #&gt; 6 K = 6 0.000316 474. -920. -918. -875. &lt;cpl [0]&gt; &lt;cpl [1]&gt; cafe_fit %&gt;% forecast(h = &quot;2 years&quot;) %&gt;% autoplot(aus_cafe) + facet_wrap(vars(.model), ncol = 2) + guides(colour = FALSE) + geom_label( aes(x = yearmonth(&quot;2007 Jan&quot;), y = 4250, label = str_c(&quot;AICc = &quot;, format(AICc))), data = glance(cafe_fit) ) + theme(legend.position = &quot;bottom&quot;) As \\(K\\) increases the Fourier terms capture and project a more “wiggly” seasonal pattern and simpler ARIMA models are required to capture other dynamics. The AICc value is minimised for \\(K = 5\\), with a significant jump going from \\(K = 4\\) to \\(K = 5\\), hence the forecasts generated from this model would be the ones used. 11.6 Lagged predictors In Section 7.4.6 we introduced distributed lags. In those situations, we need to allow for lagged effects of the predictor. Suppose that we have only one predictor in our model. Then a model which allows for lagged effects can be written as \\[ y_t + \\beta_0 + \\gamma_1x_t + \\gamma_2x_{t-2} + \\cdots + \\gamma_kx_{t-k} + \\eta_t \\] where \\(\\eta_t\\) is an ARIMA process. The value of \\(k\\) can be selected using the \\(\\text{AIC}_c\\), along with the values of \\(p\\) and \\(q\\) for the ARIMA error. Note that when comparing models with different lags, we need to set top k observations to NA, where k is the maximum lag tried, so that all models use the exact same data. 11.6.1 Example: TV advertising and insurance quotations insurance &lt;- as_tsibble(fpp2::insurance, pivot_longer = FALSE) insurance %&gt;% pivot_longer(c(Quotes, TV.advert)) %&gt;% ggplot(aes(x = index, y = value)) + geom_line() + facet_grid(vars(name), scales = &quot;free_y&quot;) + labs(x = &quot;Year&quot;, y = NULL, title = &quot;Insurance advertising and quotations&quot;) insurance_fit &lt;- insurance %&gt;% # Restrict data so models use same fitting period mutate(Quotes = c(NA, NA, NA, Quotes[4:40])) %&gt;% # Estimate models model( lag0 = ARIMA(Quotes ~ pdq(d = 0) + TV.advert), lag1= ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert)), lag2= ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert) + lag(TV.advert, 2)), lag3= ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert) + lag(TV.advert, 2) + lag(TV.advert, 3))) glance(insurance_fit) #&gt; # A tibble: 4 x 8 #&gt; .model sigma2 log_lik AIC AICc BIC ar_roots ma_roots #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 lag0 0.265 -28.3 66.6 68.3 75.0 &lt;cpl [2]&gt; &lt;cpl [0]&gt; #&gt; 2 lag1 0.209 -24.0 58.1 59.9 66.5 &lt;cpl [1]&gt; &lt;cpl [1]&gt; #&gt; 3 lag2 0.215 -24.0 60.0 62.6 70.2 &lt;cpl [1]&gt; &lt;cpl [1]&gt; #&gt; 4 lag3 0.206 -22.2 60.3 65.0 73.8 &lt;cpl [1]&gt; &lt;cpl [1]&gt; The best model (with the smallest AICc value) has two lagged predictors; that is, it includes advertising only in the current month and the previous month. So we now re-estimate that model, but using all the available data. insurance_best &lt;- insurance %&gt;% model(ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert))) report(insurance_best) #&gt; Series: Quotes #&gt; Model: LM w/ ARIMA(1,0,2) errors #&gt; #&gt; Coefficients: #&gt; ar1 ma1 ma2 TV.advert lag(TV.advert) intercept #&gt; 0.5123 0.9169 0.4591 1.2527 0.1464 2.1554 #&gt; s.e. 0.1849 0.2051 0.1895 0.0588 0.0531 0.8595 #&gt; #&gt; sigma^2 estimated as 0.2166: log likelihood=-23.94 #&gt; AIC=61.88 AICc=65.38 BIC=73.7 insurance_best %&gt;% augment() %&gt;% features(.resid, ljung_box, dof = 7, lag = 10) #&gt; # A tibble: 1 x 3 #&gt; .model lb_stat lb_pvalue #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 ARIMA(Quotes ~ pdq(d = 0) + TV.advert + lag(TV.advert)) 3.09 0.379 We can calculate forecasts using this model if we assume future values for the advertising variable. If we set the future monthly advertising to 8 units insurance_future &lt;- insurance %&gt;% new_data(20) %&gt;% mutate(TV.advert = 8) insurance_best %&gt;% forecast(insurance_future) %&gt;% autoplot(insurance) "],
["references.html", "References", " References Hyndman, R.J., &amp; Athanasopoulos, G. (2019) Forecasting: principles and practice, 3rd edition, OTexts: Melbourne, Australia. OTexts.com/fpp3. Accessed on &lt;20200208&gt;. Holmes, E. E., M. D. Scheuerell, and E. J. Ward. Applied time series analysis for fisheries and environmental data. NOAA Fisheries, Northwest Fisheries Science Center, 2725 Montlake Blvd E., Seattle, WA 98112. Contacts eli.holmes@noaa.gov, eric.ward@noaa.gov, and mark.scheuerell@noaa.gov Brown, Robert Goodell. 1959. Statistical Forecasting for Inventory Control. McGraw/Hill. Fox, John, and Sanford Weisberg. 2018. An R Companion to Applied Regression. Sage publications. Gardner Jr, Everette S, and ED McKenzie. 1985. “Forecasting Trends in Time Series.” Management Science 31 (10): 1237–46. Holt, Charles C. 1957. “Forecasting Trends and Seasonals by Exponentially Weighted Averages. Carnegie Institute of Technology.” Pittsburgh ONR memorandum. Hyndman, Rob J, and Anne B Koehler. 2006. “Another Look at Measures of Forecast Accuracy.” International Journal of Forecasting 22 (4): 679–88. Hyndman, Rob, and Yeasmin Khandakar. 2008. “Automatic Time Series Forecasting: The Forecast Package for R.” Journal of Statistical Software, Articles 27 (3): 1–22. https://doi.org/10.18637/jss.v027.i03. Hyndman, Rob, Anne B Koehler, J Keith Ord, and Ralph D Snyder. 2008. Forecasting with Exponential Smoothing: The State Space Approach. Springer Science &amp; Business Media. O’Hara-Wild, Mitchell, Rob Hyndman, and Earo Wang. 2019. Feasts: Feature Extraction and Statistics for Time Series. https://CRAN.R-project.org/package=feasts. ———. 2020. Fable: Forecasting Models for Tidy Time Series. https://CRAN.R-project.org/package=fable. Wang, Earo, Dianne Cook, and Rob J Hyndman. 2019. “A New Tidy Data Structure to Support Exploration and Modeling of Temporal Data.” Wang, Earo, Di Cook, Rob Hyndman, and Mitchell O’Hara-Wild. 2019. Tsibble: Tidy Temporal Data Frames and Tools. https://CRAN.R-project.org/package=tsibble. Wickham, Hadley. 2019. Tidyverse: Easily Install and Load the ’Tidyverse’. https://CRAN.R-project.org/package=tidyverse. Wickham, Hadley, Mara Averick, Jennifer Bryan, Winston Chang, Lucy D’Agostino McGowan, Romain François, Garrett Grolemund, et al. 2019. “Welcome to the tidyverse.” Journal of Open Source Software 4 (43): 1686. https://doi.org/10.21105/joss.01686. Winters, Peter R. 1960. “Forecasting Sales by Exponentially Weighted Moving Averages.” Management Science 6 (3): 324–42. "]
]
